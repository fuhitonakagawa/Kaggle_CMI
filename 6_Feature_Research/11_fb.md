ご修正版、丁寧に拝見しました。結論から申し上げると、**ラベル–特徴量の行ずれ問題は解消できています**（`import_features()` 後に `labels/subjects` を置き換えているため）。一方で、**実装上の設計と実行フローの乖離により「リッチな特徴」を十分に学習・推論へ反映できていない箇所**が複数あります。とりわけ \*\*ToF-PCA（および fold 内 fit→transform の流れ）\*\*が有効化されておらず、Notebook より高いスコアにならない主因になり得ます。

以下、**ブロッカー級の指摘 → スコア改善に効く修正案 → 品質/再現性改善 → 追加の改善余地**の順で、具体的な根拠とパッチ方針を整理します。

---

## A. ブロッカー級：設計と実行フローの不整合

### A-1) ToF-PCA が学習・推論に実質的に使われていない

- `train_models()` では CV 前に **全シーケンスの特徴を一括抽出**していますが、その際

  ```python
  temp_extractor = FeatureExtractor(CONFIG)
  temp_extractor.config["tof_use_pca"] = False  # 一旦PCAなしで抽出
  ...
  features = temp_extractor.extract_features(seq_df, demo_df)
  ```

  と **PCA を明示オフ**にしており、学習に使う `X_all` には **PCA 系特徴が一切入っていません**。

- 推論側も

  ```python
  X_raw = EXTRACTOR._extract_features_raw(seq_df, demo_df)  # ← PCAなし
  ```

  と **常に Raw 抽出**で、学習時と同じく **PCA なし**の特徴を使います。

- さらに `FeatureExtractor.fit()` 内のロジックも、

  1. ToF PCA を fit →
  2. 直後に `self.extract_features()` を呼ぶが、この時点では `self.is_fitted == False` のため、`extract_features()` の

     ```python
     if self.config.get("tof_use_pca", False) and self.is_fitted:
     ```

     が偽になり **PCA 変換が適用されません**（＝ fit 内で作る「最終特徴」も PCA 無し）。
     その後 `self.is_fitted=True` にするため、**fit の中でも PCA 特徴は 1 度も組み込まれていません**。

> **影響**
> 「リッチ化」の柱として書かれている ToF-PCA 改善が、現在のパイプラインでは **学習にも推論にも反映されていません**。Notebook を上回るはずという期待が実現しない最大要因です。

**最小パッチ案（抜本）**

1. **CV 各 fold で**下記の流れに変更してください（リーク無しで PCA を活かす正攻法です）。

   - `fold_extractor = FeatureExtractor(CONFIG.copy())`
     `fold_extractor.config["tof_use_pca"] = True`
   - `fold_extractor.fit(train_sequences, train_demographics)`
     （ここで ToF-PCA を学習。**fit 内で PCA を使った最終特徴を得るため、fit の途中で `self.is_fitted = True` に一時的にセット**してください。）
   - `X_train = fold_extractor.transform(train_sequences, train_demographics)`
     `X_val   = fold_extractor.transform(val_sequences,   val_demographics)`
   - `fold_artifacts.append({"feature_names": fold_extractor.feature_names, "scaler": fold_extractor.scaler, "tof_pcas": fold_extractor.tof_pcas})`
   - これで **PCA を含む最終特徴**に対して、fold 内の Scaler を fit→ 適用し、XGBoost を学習します。

2. 推論 `predict()` は、**fold ごとのアーティファクトを用いて同じ前処理**を再現します。

   - 各 fold について：

     ```python
     fe = FeatureExtractor(CONFIG)
     fe.tof_pcas = art["tof_pcas"]
     fe.feature_names = art["feature_names"]
     fe.scaler = art["scaler"]
     fe.is_fitted = True

     X = fe.extract_features(seq_df, demo_df)     # ← PCA込で抽出
     for col in art["feature_names"]:
         if col not in X.columns: X[col] = 0
     X = X[art["feature_names"]]
     Xs = fe.scaler.transform(X)
     predictions.append(model.predict_proba(Xs)[0])
     ```

   - こうすることで \*\*学習時と同じ「PCA→ 集約 → スケーリング → 分類」\*\*の経路を忠実に再現できます。

> なお、もし「前処理を CV 前に一括で済ませたい（高速化重視）」という方針であれば、**PCA は使わない**設計のままにするのが一貫性としては正しいです（現状と同じ）。その場合は、下記 B のハイパラ/特徴整理で性能改善を狙ってください。

---

### A-2) `FeatureExtractor.fit()` 内で PCA 特徴を落としてしまう実装

- 前述の通り、`fit()` の途中で `extract_features()` を呼ぶ時点で `self.is_fitted` が False のため、**PCA が効きません**。
- **パッチ**：`fit()` の「ToF-PCA を fit し終えた直後」に `self.is_fitted=True` を一時的にセットしてから `extract_features()` を呼び、最後に `scaler.fit()` → `self.is_fitted=True` で確定、という順にしてください。

---

## B. スコア改善に効く実践的チューニング（ToF-PCA を使わない前提でも効く）

> 「リッチな特徴＝常にスコア向上」ではありません。高次・大量の特徴は **学習器のバイアス/バリアンスのバランス**や **目的指標との整合**が取れて初めて威力を発揮します。以下は今の構成でも効きやすい手当です。

### B-1) 木モデル × スケーリングの影響を切り分ける（重要）

- 木ベース（XGBoost）はスケールに不変ですが、**スケーラ＋高次特徴の組合せ**で閾値の当たり方が変わり、**過剰適合や枝刈りの失敗**を招くことがあります。
- **検証**：`robust_scaler=False`（＝スケーリング無し）で 1 回回してください。多くの構造化タスクでは **スケーリング無しの方が安定**します。
  逆に「特定の外れ値に引っ張られている」場合のみ RobustScaler が効きます。

### B-2) ハイパラを「深さ抑制＋木数増」へ

- 現状：`max_depth=10, min_child_weight=3, gamma=0.1, colsample_bytree=0.8, subsample=0.8, n_estimators=1000`
- **推奨の起点**（過学習抑止＆多特徴での安定化）：

  - `max_depth=6~8`
  - `min_child_weight=5~8`
  - `gamma=0` または `≤0.05`（細かい枝を抑え過ぎない）
  - `colsample_bytree=0.6~0.8`, `subsample=0.7~0.9`
  - `n_estimators=2000`, `early_stopping_rounds=100`
  - `reg_lambda=2.0` 程度に強める

- まず上記で **IMU のみ**に限定して（`use_tof_spatial=False`, `use_thermal_trends=False`, `use_cross_modal=False`）、Notebook と同等の 0.71 近辺が出せるかを確認 → そこに ToF/Thermal/クロスモーダルを足していく、という **加法検証**が安全です。

### B-3) クラス不均衡対策（多クラス）

- BFRB と非 BFRB、BFRB 内でも頻度差が大きいはずです。
  fold ごとに `class_weight` を算出し、`sample_weight` を `model.fit(..., sample_weight=weights)` で渡すと **Macro F1 側**の底上げに効きます（`sklearn.utils.class_weight.compute_class_weight` など）。

### B-4) 特徴量の過多を抑制（相関・無効値だらけの特徴の整理）

- 「フレームごと → 時系列集約 → 差分」まで行くと**数千〜万規模**の特徴に膨らみます。
  **VarianceThreshold（ほぼ定数の列の削除）**と**高相関カラムの間引き**（|ρ|>0.98 など）は、**木の深掘りを減らし汎化を改善**します。fold 内の train のみで算出してください（リーク防止）。

---

## C. 正しさ・再現性の観点（Kaggle スコアとの整合）

### C-1) CV スコアの定義を Kaggle と一致させる

- 現在の CV は

  ```python
  binary_f1 = f1_score(y_true<=7, y_pred<=7)
  macro_f1  = f1_score(np.where(y_true<=7, y_true, 99),
                       np.where(y_pred<=7, y_pred, 99),
                       average="macro")
  score = 0.5*(binary_f1+macro_f1)
  ```

  ですが、「Macro F1 のクラス集合」「Other の扱い」が Kaggle の実装と完全一致していない可能性があります。

- **推奨**：**Kaggle の評価式と同一の関数**をローカルに実装し（もしくは Notebook で使っている実装をそのまま移植）、**CV の数値と LB のズレ**を最小化してください。これだけでモデル選好がぶれにくくなります。

### C-2) 予測クラス名の正確性

- `REVERSE_GESTURE_MAPPER` の文字列が **評価サーバの期待と 1 文字でもズレる**と失点に直結します。Notebook と同一のマッパー定義を共有化し、**単一の定義から import** する構成にしておくと安全です。

---

## D. そのほかの気になる点（優先度：中〜低）

1. **周波数特徴のゼロ埋め頻発**
   短系列だと `len(data) < 32` で 0 を多発します。
   → たとえば **短系列用の簡易 FFT バンドパワー**（`nperseg` が足りないときはハニング窓 FFT で 0.3–3/3–8/8–12Hz の矩形積分だけ計算）にフォールバックすると、無情報列を減らせます。

2. **Quality 特徴の pad/border の影響**
   ToF 同期で `padded_data` を `edge` で伸ばしていますが、統計量をやや歪めます。`reflect` や `mean` でのパディングも試す価値があります。

3. **`_extract_features_raw` と `extract_features` の二重メンテ**
   かなりの重複コードです。`_extract_features_base(..., use_pca: bool)` に集約して **分岐は引数で制御**すると、今後の修正漏れを防げます。

4. **fold ごとに保存するアーティファクトの充実**
   いまは `{"feature_names", "scaler"}` のみですが、**PCA を使うなら `tof_pcas` も保存**してください（B-1 参照）。

5. **乱数の固定**
   XGB 側と `StratifiedGroupKFold(random_state=42)` は固定済みですが、NumPy/OS レベルの seed 固定も入れると再現性がさらに安定します。

---

## まとめ（何を直すべきか）

- **必須修正（ブロッカー）**

  1. **ToF-PCA を本当に使う**（fold 内で fit→transform、一貫したアーティファクトを保存し、推論で再現）。
  2. `FeatureExtractor.fit()` 内で **PCA 特徴が消える条件分岐**を修正（`is_fitted` の切替タイミング）。

- **短期で効く改善**

  - **スケーリング OFF** の A/B、**深さ抑制＋木数増**のハイパラ、**class_weight** の導入、**低情報/高相関特徴の間引き**。

- **整合性・再現性**

  - **CV の指標関数を Kaggle と一致**させる。
  - **クラス名マッパーの単一化**。
