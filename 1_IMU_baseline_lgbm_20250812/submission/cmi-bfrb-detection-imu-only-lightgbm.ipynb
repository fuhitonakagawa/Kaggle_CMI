{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":102335,"databundleVersionId":12518947,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CMI BFRB Detection - IMU-only LightGBM Training\n\nThis notebook trains the IMU-only LightGBM baseline model for BFRB detection.","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nimport os\nimport sys\nimport warnings\nimport pickle\nimport json\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport joblib\n\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score\nfrom lightgbm import LGBMClassifier, log_evaluation, early_stopping\nfrom scipy.spatial.transform import Rotation as R\n\nwarnings.filterwarnings('ignore')\nprint('✓ All imports loaded successfully')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T14:14:04.783093Z","iopub.execute_input":"2025-08-12T14:14:04.783379Z","iopub.status.idle":"2025-08-12T14:14:14.524080Z","shell.execute_reply.started":"2025-08-12T14:14:04.783357Z","shell.execute_reply":"2025-08-12T14:14:14.523445Z"}},"outputs":[{"name":"stdout","text":"✓ All imports loaded successfully\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Configuration\nclass Config:\n    # Data paths for Kaggle environment\n    TRAIN_PATH = '/kaggle/input/cmi-detect-behavior-with-sensor-data/train.csv'\n    TRAIN_DEMOGRAPHICS_PATH = '/kaggle/input/cmi-detect-behavior-with-sensor-data/train_demographics.csv'\n    \n    # Training parameters\n    SEED = 42\n    N_FOLDS = 5\n    \n    # Feature columns\n    ACC_COLS = ['acc_x', 'acc_y', 'acc_z']\n    ROT_COLS = ['rot_w', 'rot_x', 'rot_y', 'rot_z']\n    \n    # LightGBM parameters\n    LGBM_PARAMS = {\n        'objective': 'multiclass',\n        'n_estimators': 1024,\n        'max_depth': 8,\n        'learning_rate': 0.025,\n        'colsample_bytree': 0.5,\n        'n_jobs': -1,\n        'num_leaves': 20,\n        'reg_alpha': 0.1,\n        'reg_lambda': 0.1,\n        'subsample': 0.5,\n        'verbosity': -1,\n        'random_state': 42\n    }\n    \n    # Output path\n    OUTPUT_PATH = '/kaggle/working/'\n\nnp.random.seed(Config.SEED)\nprint('✓ Configuration loaded')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T14:14:14.524983Z","iopub.execute_input":"2025-08-12T14:14:14.525483Z","iopub.status.idle":"2025-08-12T14:14:14.531034Z","shell.execute_reply.started":"2025-08-12T14:14:14.525462Z","shell.execute_reply":"2025-08-12T14:14:14.530111Z"}},"outputs":[{"name":"stdout","text":"✓ Configuration loaded\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Gesture mapping\nGESTURE_MAPPER = {\n    'Above ear - pull hair': 0,\n    'Cheek - pinch skin': 1,\n    'Eyebrow - pull hair': 2,\n    'Eyelash - pull hair': 3,\n    'Forehead - pull hairline': 4,\n    'Forehead - scratch': 5,\n    'Neck - pinch skin': 6,\n    'Neck - scratch': 7,\n    'Drink from bottle/cup': 8,\n    'Feel around in tray and pull out an object': 9,\n    'Glasses on/off': 10,\n    'Pinch knee/leg skin': 11,\n    'Pull air toward your face': 12,\n    'Scratch knee/leg skin': 13,\n    'Text on phone': 14,\n    'Wave hello': 15,\n    'Write name in air': 16,\n    'Write name on leg': 17,\n}\n\nREVERSE_GESTURE_MAPPER = {v: k for k, v in GESTURE_MAPPER.items()}\nprint(f'✓ Gesture mapping loaded ({len(GESTURE_MAPPER)} classes)')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T14:14:14.531824Z","iopub.execute_input":"2025-08-12T14:14:14.532107Z","iopub.status.idle":"2025-08-12T14:14:14.599554Z","shell.execute_reply.started":"2025-08-12T14:14:14.532088Z","shell.execute_reply":"2025-08-12T14:14:14.598738Z"}},"outputs":[{"name":"stdout","text":"✓ Gesture mapping loaded (18 classes)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Feature engineering functions\ndef handle_quaternion_missing_values(rot_data: np.ndarray) -> np.ndarray:\n    \"\"\"Handle missing values in quaternion data.\"\"\"\n    rot_cleaned = rot_data.copy()\n    \n    for i in range(len(rot_data)):\n        row = rot_data[i]\n        missing_count = np.isnan(row).sum()\n        \n        if missing_count == 0:\n            norm = np.linalg.norm(row)\n            if norm > 1e-8:\n                rot_cleaned[i] = row / norm\n            else:\n                rot_cleaned[i] = [1.0, 0.0, 0.0, 0.0]\n        elif missing_count == 1:\n            missing_idx = np.where(np.isnan(row))[0][0]\n            valid_values = row[~np.isnan(row)]\n            sum_squares = np.sum(valid_values**2)\n            if sum_squares <= 1.0:\n                missing_value = np.sqrt(max(0, 1.0 - sum_squares))\n                if i > 0 and not np.isnan(rot_cleaned[i-1, missing_idx]):\n                    if rot_cleaned[i-1, missing_idx] < 0:\n                        missing_value = -missing_value\n                rot_cleaned[i, missing_idx] = missing_value\n                rot_cleaned[i, ~np.isnan(row)] = valid_values\n            else:\n                rot_cleaned[i] = [1.0, 0.0, 0.0, 0.0]\n        else:\n            rot_cleaned[i] = [1.0, 0.0, 0.0, 0.0]\n    \n    return rot_cleaned\n\ndef compute_world_acceleration(acc: np.ndarray, rot: np.ndarray) -> np.ndarray:\n    \"\"\"Convert acceleration from device to world coordinates.\"\"\"\n    try:\n        rot_scipy = rot[:, [1, 2, 3, 0]]  # Convert to scipy format\n        norms = np.linalg.norm(rot_scipy, axis=1)\n        if np.any(norms < 1e-8):\n            mask = norms < 1e-8\n            rot_scipy[mask] = [0.0, 0.0, 0.0, 1.0]\n        r = R.from_quat(rot_scipy)\n        acc_world = r.apply(acc)\n    except Exception:\n        acc_world = acc.copy()\n    return acc_world\n\nprint('✓ Feature engineering functions defined')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T14:14:14.600691Z","iopub.execute_input":"2025-08-12T14:14:14.600994Z","iopub.status.idle":"2025-08-12T14:14:14.624336Z","shell.execute_reply.started":"2025-08-12T14:14:14.600971Z","shell.execute_reply":"2025-08-12T14:14:14.623486Z"}},"outputs":[{"name":"stdout","text":"✓ Feature engineering functions defined\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def extract_statistical_features(data: np.ndarray, prefix: str) -> dict:\n    \"\"\"Extract statistical features from 1D time series.\"\"\"\n    features = {}\n    \n    # Basic statistics\n    features[f'{prefix}_mean'] = np.mean(data)\n    features[f'{prefix}_std'] = np.std(data)\n    features[f'{prefix}_var'] = np.var(data)\n    features[f'{prefix}_min'] = np.min(data)\n    features[f'{prefix}_max'] = np.max(data)\n    features[f'{prefix}_median'] = np.median(data)\n    features[f'{prefix}_q25'] = np.percentile(data, 25)\n    features[f'{prefix}_q75'] = np.percentile(data, 75)\n    features[f'{prefix}_iqr'] = features[f'{prefix}_q75'] - features[f'{prefix}_q25']\n    features[f'{prefix}_range'] = features[f'{prefix}_max'] - features[f'{prefix}_min']\n    \n    # Boundary features\n    features[f'{prefix}_first'] = data[0] if len(data) > 0 else 0\n    features[f'{prefix}_last'] = data[-1] if len(data) > 0 else 0\n    features[f'{prefix}_delta'] = features[f'{prefix}_last'] - features[f'{prefix}_first']\n    \n    # Higher order moments\n    if len(data) > 1 and np.std(data) > 1e-8:\n        features[f'{prefix}_skew'] = pd.Series(data).skew()\n        features[f'{prefix}_kurt'] = pd.Series(data).kurtosis()\n    else:\n        features[f'{prefix}_skew'] = 0\n        features[f'{prefix}_kurt'] = 0\n    \n    # Differential features\n    if len(data) > 1:\n        diff_data = np.diff(data)\n        features[f'{prefix}_diff_mean'] = np.mean(diff_data)\n        features[f'{prefix}_diff_std'] = np.std(diff_data)\n        features[f'{prefix}_n_changes'] = np.sum(np.abs(diff_data) > np.std(data) * 0.1)\n    else:\n        features[f'{prefix}_diff_mean'] = 0\n        features[f'{prefix}_diff_std'] = 0\n        features[f'{prefix}_n_changes'] = 0\n    \n    # Segment features (3 segments)\n    seq_len = len(data)\n    if seq_len >= 9:\n        seg_size = seq_len // 3\n        for i in range(3):\n            start_idx = i * seg_size\n            end_idx = (i + 1) * seg_size if i < 2 else seq_len\n            segment = data[start_idx:end_idx]\n            features[f'{prefix}_seg{i+1}_mean'] = np.mean(segment)\n            features[f'{prefix}_seg{i+1}_std'] = np.std(segment)\n        # Segment transitions\n        features[f'{prefix}_seg1_to_seg2'] = features[f'{prefix}_seg2_mean'] - features[f'{prefix}_seg1_mean']\n        features[f'{prefix}_seg2_to_seg3'] = features[f'{prefix}_seg3_mean'] - features[f'{prefix}_seg2_mean']\n    else:\n        for i in range(3):\n            features[f'{prefix}_seg{i+1}_mean'] = features[f'{prefix}_mean']\n            features[f'{prefix}_seg{i+1}_std'] = features[f'{prefix}_std']\n        features[f'{prefix}_seg1_to_seg2'] = 0\n        features[f'{prefix}_seg2_to_seg3'] = 0\n    \n    return features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T14:14:14.625978Z","iopub.execute_input":"2025-08-12T14:14:14.626219Z","iopub.status.idle":"2025-08-12T14:14:14.645706Z","shell.execute_reply.started":"2025-08-12T14:14:14.626202Z","shell.execute_reply":"2025-08-12T14:14:14.645058Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def extract_features(sequence: pl.DataFrame, demographics: pl.DataFrame) -> pd.DataFrame:\n    \"\"\"Extract features from IMU sequence.\"\"\"\n    # Convert to pandas\n    seq_df = sequence.to_pandas()\n    demo_df = demographics.to_pandas()\n    \n    # Handle missing values\n    acc_data = seq_df[Config.ACC_COLS].copy()\n    acc_data = acc_data.ffill().bfill().fillna(0)\n    \n    rot_data = seq_df[Config.ROT_COLS].copy()\n    rot_data = rot_data.ffill().bfill()\n    \n    # Handle quaternion missing values\n    rot_data_clean = handle_quaternion_missing_values(rot_data.values)\n    \n    # Compute world acceleration\n    world_acc_data = compute_world_acceleration(acc_data.values, rot_data_clean)\n    \n    # Initialize features\n    features = {}\n    \n    # Sequence metadata\n    features['sequence_length'] = len(seq_df)\n    \n    # Demographics features\n    if len(demo_df) > 0:\n        demo_row = demo_df.iloc[0]\n        features['age'] = demo_row.get('age', 0)\n        features['adult_child'] = demo_row.get('adult_child', 0)\n        features['sex'] = demo_row.get('sex', 0)\n        features['handedness'] = demo_row.get('handedness', 0)\n        features['height_cm'] = demo_row.get('height_cm', 0)\n        features['shoulder_to_wrist_cm'] = demo_row.get('shoulder_to_wrist_cm', 0)\n        features['elbow_to_wrist_cm'] = demo_row.get('elbow_to_wrist_cm', 0)\n    \n    # Extract statistical features for each axis\n    for i, axis in enumerate(['x', 'y', 'z']):\n        # Device acceleration\n        features.update(extract_statistical_features(acc_data.values[:, i], f'acc_{axis}'))\n        # World acceleration\n        features.update(extract_statistical_features(world_acc_data[:, i], f'world_acc_{axis}'))\n    \n    # Rotation features\n    for i, comp in enumerate(['w', 'x', 'y', 'z']):\n        features.update(extract_statistical_features(rot_data_clean[:, i], f'rot_{comp}'))\n    \n    # Magnitude features\n    acc_magnitude = np.linalg.norm(acc_data.values, axis=1)\n    world_acc_magnitude = np.linalg.norm(world_acc_data, axis=1)\n    \n    features.update(extract_statistical_features(acc_magnitude, 'acc_magnitude'))\n    features.update(extract_statistical_features(world_acc_magnitude, 'world_acc_magnitude'))\n    \n    # Difference between device and world acceleration\n    acc_world_diff = acc_magnitude - world_acc_magnitude\n    features.update(extract_statistical_features(acc_world_diff, 'acc_world_diff'))\n    \n    # Convert to DataFrame\n    result_df = pd.DataFrame([features])\n    result_df = result_df.fillna(0)\n    \n    return result_df\n\nprint('✓ Feature extraction function defined')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T14:14:14.646726Z","iopub.execute_input":"2025-08-12T14:14:14.647049Z","iopub.status.idle":"2025-08-12T14:14:14.662494Z","shell.execute_reply.started":"2025-08-12T14:14:14.647020Z","shell.execute_reply":"2025-08-12T14:14:14.661582Z"}},"outputs":[{"name":"stdout","text":"✓ Feature extraction function defined\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Load data\nprint('Loading training data...')\ntrain_df = pl.read_csv(Config.TRAIN_PATH)\ntrain_demographics = pl.read_csv(Config.TRAIN_DEMOGRAPHICS_PATH)\n\nprint(f'✓ Train shape: {train_df.shape}')\nprint(f'✓ Demographics shape: {train_demographics.shape}')\n\n# Get IMU columns (common between train and test)\nimu_cols = ['sequence_id', 'subject', 'phase', 'gesture'] + Config.ACC_COLS + Config.ROT_COLS\nprint(f'✓ Using {len(imu_cols)} IMU columns')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T14:14:14.663007Z","iopub.execute_input":"2025-08-12T14:14:14.663202Z","iopub.status.idle":"2025-08-12T14:14:27.900153Z","shell.execute_reply.started":"2025-08-12T14:14:14.663188Z","shell.execute_reply":"2025-08-12T14:14:27.899210Z"}},"outputs":[{"name":"stdout","text":"Loading training data...\n✓ Train shape: (574945, 341)\n✓ Demographics shape: (81, 8)\n✓ Using 11 IMU columns\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Prepare training data\nprint('Extracting features for training sequences...')\n\ntrain_features_list = []\ntrain_labels = []\ntrain_subjects = []\n\n# Get unique sequences count\nunique_sequences = train_df['sequence_id'].unique()\nn_sequences = len(unique_sequences)\nprint(f'Total sequences to process: {n_sequences}')\n\n# Group by sequence_id\ntrain_sequences = train_df.select(pl.col(imu_cols)).group_by('sequence_id', maintain_order=True)\n\nfor i, (sequence_id, sequence_data) in enumerate(train_sequences):\n    if i % 1000 == 0:\n        print(f'Processing sequence {i+1}/{n_sequences}')\n    \n    # Get sequence ID\n    seq_id_val = sequence_id[0] if isinstance(sequence_id, tuple) else sequence_id\n    \n    # Get demographics\n    subject_id = sequence_data['subject'][0]\n    subject_demographics = train_demographics.filter(pl.col('subject') == subject_id)\n    \n    # Extract features\n    features = extract_features(sequence_data, subject_demographics)\n    train_features_list.append(features)\n    \n    # Get label\n    gesture = sequence_data['gesture'][0]\n    label = GESTURE_MAPPER[gesture]\n    train_labels.append(label)\n    train_subjects.append(subject_id)\n\n# Combine features\nX_train = pd.concat(train_features_list, ignore_index=True)\ny_train = np.array(train_labels)\nsubjects = np.array(train_subjects)\n\nprint(f'✓ Features extracted: {X_train.shape}')\nprint(f'✓ Number of classes: {len(np.unique(y_train))}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T14:22:05.561514Z","iopub.execute_input":"2025-08-12T14:22:05.561762Z","iopub.status.idle":"2025-08-12T14:24:16.784457Z","shell.execute_reply.started":"2025-08-12T14:22:05.561745Z","shell.execute_reply":"2025-08-12T14:24:16.783656Z"}},"outputs":[{"name":"stdout","text":"Extracting features for training sequences...\nTotal sequences to process: 8151\nProcessing sequence 1/8151\nProcessing sequence 1001/8151\nProcessing sequence 2001/8151\nProcessing sequence 3001/8151\nProcessing sequence 4001/8151\nProcessing sequence 5001/8151\nProcessing sequence 6001/8151\nProcessing sequence 7001/8151\nProcessing sequence 8001/8151\n✓ Features extracted: (8151, 346)\n✓ Number of classes: 18\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Train models with cross-validation\nprint('Training LightGBM models with cross-validation...')\n\ncv = StratifiedGroupKFold(n_splits=Config.N_FOLDS, shuffle=True, random_state=Config.SEED)\nmodels = []\noof_predictions = np.zeros(len(y_train))\ncv_scores = []\n\nfor fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train, subjects)):\n    print(f'\\n--- Fold {fold + 1}/{Config.N_FOLDS} ---')\n    \n    # Split data\n    X_fold_train = X_train.iloc[train_idx]\n    X_fold_val = X_train.iloc[val_idx]\n    y_fold_train = y_train[train_idx]\n    y_fold_val = y_train[val_idx]\n    \n    print(f'Train size: {len(X_fold_train)}, Val size: {len(X_fold_val)}')\n    \n    # Train model\n    model = LGBMClassifier(**Config.LGBM_PARAMS)\n    \n    model.fit(\n        X_fold_train, y_fold_train,\n        eval_set=[(X_fold_val, y_fold_val)],\n        eval_names=['valid'],\n        eval_metric='multi_logloss',\n        callbacks=[log_evaluation(period=50), early_stopping(stopping_rounds=100, verbose=True)]\n    )\n    \n    # Store model\n    models.append(model)\n    \n    # Predictions\n    val_preds = model.predict(X_fold_val)\n    oof_predictions[val_idx] = val_preds\n    \n    # Calculate score\n    binary_f1 = f1_score(\n        np.where(y_fold_val <= 7, 1, 0),\n        np.where(val_preds <= 7, 1, 0),\n        zero_division=0.0\n    )\n    \n    macro_f1 = f1_score(\n        np.where(y_fold_val <= 7, y_fold_val, 99),\n        np.where(val_preds <= 7, val_preds, 99),\n        average='macro',\n        zero_division=0.0\n    )\n    \n    score = 0.5 * (binary_f1 + macro_f1)\n    cv_scores.append(score)\n    \n    print(f'Fold {fold + 1} Score: {score:.4f} (Binary F1: {binary_f1:.4f}, Macro F1: {macro_f1:.4f})')\n\nprint(f'\\n✓ Cross-validation complete!')\nprint(f'Overall CV Score: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T14:24:16.785237Z","iopub.execute_input":"2025-08-12T14:24:16.786097Z","iopub.status.idle":"2025-08-12T14:28:37.776918Z","shell.execute_reply.started":"2025-08-12T14:24:16.786045Z","shell.execute_reply":"2025-08-12T14:28:37.776322Z"}},"outputs":[{"name":"stdout","text":"Training LightGBM models with cross-validation...\n\n--- Fold 1/5 ---\nTrain size: 6623, Val size: 1528\nTraining until validation scores don't improve for 100 rounds\n[50]\tvalid's multi_logloss: 1.48659\n[100]\tvalid's multi_logloss: 1.25091\n[150]\tvalid's multi_logloss: 1.15758\n[200]\tvalid's multi_logloss: 1.11416\n[250]\tvalid's multi_logloss: 1.08866\n[300]\tvalid's multi_logloss: 1.07365\n[350]\tvalid's multi_logloss: 1.06507\n[400]\tvalid's multi_logloss: 1.0582\n[450]\tvalid's multi_logloss: 1.05575\n[500]\tvalid's multi_logloss: 1.05507\n[550]\tvalid's multi_logloss: 1.05464\n[600]\tvalid's multi_logloss: 1.05884\nEarly stopping, best iteration is:\n[526]\tvalid's multi_logloss: 1.05376\nFold 1 Score: 0.7647 (Binary F1: 0.9819, Macro F1: 0.5475)\n\n--- Fold 2/5 ---\nTrain size: 6519, Val size: 1632\nTraining until validation scores don't improve for 100 rounds\n[50]\tvalid's multi_logloss: 1.66564\n[100]\tvalid's multi_logloss: 1.46559\n[150]\tvalid's multi_logloss: 1.39384\n[200]\tvalid's multi_logloss: 1.36616\n[250]\tvalid's multi_logloss: 1.35001\n[300]\tvalid's multi_logloss: 1.34272\n[350]\tvalid's multi_logloss: 1.3416\n[400]\tvalid's multi_logloss: 1.34099\n[450]\tvalid's multi_logloss: 1.34436\nEarly stopping, best iteration is:\n[356]\tvalid's multi_logloss: 1.34076\nFold 2 Score: 0.7174 (Binary F1: 0.9594, Macro F1: 0.4755)\n\n--- Fold 3/5 ---\nTrain size: 6526, Val size: 1625\nTraining until validation scores don't improve for 100 rounds\n[50]\tvalid's multi_logloss: 1.64098\n[100]\tvalid's multi_logloss: 1.43501\n[150]\tvalid's multi_logloss: 1.36253\n[200]\tvalid's multi_logloss: 1.33491\n[250]\tvalid's multi_logloss: 1.32469\n[300]\tvalid's multi_logloss: 1.32089\n[350]\tvalid's multi_logloss: 1.32077\n[400]\tvalid's multi_logloss: 1.32412\nEarly stopping, best iteration is:\n[335]\tvalid's multi_logloss: 1.32\nFold 3 Score: 0.7078 (Binary F1: 0.9501, Macro F1: 0.4656)\n\n--- Fold 4/5 ---\nTrain size: 6519, Val size: 1632\nTraining until validation scores don't improve for 100 rounds\n[50]\tvalid's multi_logloss: 1.6963\n[100]\tvalid's multi_logloss: 1.48519\n[150]\tvalid's multi_logloss: 1.40854\n[200]\tvalid's multi_logloss: 1.37737\n[250]\tvalid's multi_logloss: 1.36185\n[300]\tvalid's multi_logloss: 1.35411\n[350]\tvalid's multi_logloss: 1.35171\n[400]\tvalid's multi_logloss: 1.35163\n[450]\tvalid's multi_logloss: 1.35554\nEarly stopping, best iteration is:\n[383]\tvalid's multi_logloss: 1.35035\nFold 4 Score: 0.7054 (Binary F1: 0.9508, Macro F1: 0.4599)\n\n--- Fold 5/5 ---\nTrain size: 6417, Val size: 1734\nTraining until validation scores don't improve for 100 rounds\n[50]\tvalid's multi_logloss: 1.67133\n[100]\tvalid's multi_logloss: 1.47303\n[150]\tvalid's multi_logloss: 1.40222\n[200]\tvalid's multi_logloss: 1.37662\n[250]\tvalid's multi_logloss: 1.36294\n[300]\tvalid's multi_logloss: 1.36042\n[350]\tvalid's multi_logloss: 1.36228\n[400]\tvalid's multi_logloss: 1.3628\nEarly stopping, best iteration is:\n[307]\tvalid's multi_logloss: 1.36011\nFold 5 Score: 0.7122 (Binary F1: 0.9553, Macro F1: 0.4691)\n\n✓ Cross-validation complete!\nOverall CV Score: 0.7215 ± 0.0220\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Save models and metadata\nprint('Saving models...')\n\n# Prepare model data\nmodel_data = {\n    'models': models,\n    'feature_names': list(X_train.columns),\n    'gesture_mapper': GESTURE_MAPPER,\n    'reverse_gesture_mapper': REVERSE_GESTURE_MAPPER,\n    'cv_scores': cv_scores,\n    'mean_cv_score': np.mean(cv_scores),\n    'config': {\n        'n_folds': Config.N_FOLDS,\n        'seed': Config.SEED,\n        'lgbm_params': Config.LGBM_PARAMS\n    }\n}\n\n# Save to file\nmodel_path = os.path.join(Config.OUTPUT_PATH, 'imu_lgbm_model.pkl')\njoblib.dump(model_data, model_path)\n\nprint(f'✓ Models saved to {model_path}')\nprint(f'✓ File size: {os.path.getsize(model_path) / 1024 / 1024:.2f} MB')\n\n# Also save feature importance\nfeature_importance = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': np.mean([model.feature_importances_ for model in models], axis=0)\n}).sort_values('importance', ascending=False)\n\nprint('\\nTop 20 Most Important Features:')\nprint(feature_importance.head(20))\n\nfeature_importance.to_csv(os.path.join(Config.OUTPUT_PATH, 'feature_importance.csv'), index=False)\nprint('\\n✓ Training complete!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T14:28:37.777741Z","iopub.execute_input":"2025-08-12T14:28:37.777959Z","iopub.status.idle":"2025-08-12T14:28:39.026335Z","shell.execute_reply.started":"2025-08-12T14:28:37.777946Z","shell.execute_reply":"2025-08-12T14:28:39.025468Z"}},"outputs":[{"name":"stdout","text":"Saving models...\n✓ Models saved to /kaggle/working/imu_lgbm_model.pkl\n✓ File size: 70.96 MB\n\nTop 20 Most Important Features:\n                    feature  importance\n83           acc_y_seg3_std      1795.4\n31           acc_x_seg3_std      1544.4\n82          acc_y_seg3_mean      1247.0\n135          acc_z_seg3_std      1200.0\n115               acc_z_min      1053.8\n64                acc_y_max       995.6\n291  acc_magnitude_seg3_std       950.2\n181         rot_w_n_changes       946.0\n259         rot_z_n_changes       871.2\n212         rot_x_seg3_mean       861.8\n265          rot_z_seg3_std       816.0\n213          rot_x_seg3_std       809.2\n76           acc_y_diff_std       808.4\n187          rot_w_seg3_std       779.2\n85       acc_y_seg2_to_seg3       777.6\n134         acc_z_seg3_mean       765.6\n57     world_acc_x_seg3_std       735.8\n207         rot_x_n_changes       734.4\n11                acc_x_min       734.0\n9                 acc_x_std       732.8\n\n✓ Training complete!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CMI BFRB Detection - IMU-only LightGBM Inference\n","metadata":{}},{"cell_type":"code","source":"# Load trained model\nprint('Loading trained model...')\nmodel_path = '/kaggle/working/imu_lgbm_model.pkl'  # Update this path based on where your model is saved\nmodel_data = joblib.load(model_path)\n\nmodels = model_data['models']\nfeature_names = model_data['feature_names']\nreverse_gesture_mapper = model_data['reverse_gesture_mapper']\nconfig = model_data['config']\n\nprint(f'✓ Loaded {len(models)} models')\nprint(f'✓ Number of features: {len(feature_names)}')\nprint(f'✓ CV Score: {model_data[\"mean_cv_score\"]:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T14:39:13.732291Z","iopub.execute_input":"2025-08-12T14:39:13.732537Z","iopub.status.idle":"2025-08-12T14:39:14.109105Z","shell.execute_reply.started":"2025-08-12T14:39:13.732521Z","shell.execute_reply":"2025-08-12T14:39:14.108313Z"}},"outputs":[{"name":"stdout","text":"Loading trained model...\n✓ Loaded 5 models\n✓ Number of features: 346\n✓ CV Score: 0.7215\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Define feature extraction functions (same as training)\nACC_COLS = ['acc_x', 'acc_y', 'acc_z']\nROT_COLS = ['rot_w', 'rot_x', 'rot_y', 'rot_z']\n\ndef handle_quaternion_missing_values(rot_data: np.ndarray) -> np.ndarray:\n    \"\"\"Handle missing values in quaternion data.\"\"\"\n    rot_cleaned = rot_data.copy()\n    \n    for i in range(len(rot_data)):\n        row = rot_data[i]\n        missing_count = np.isnan(row).sum()\n        \n        if missing_count == 0:\n            norm = np.linalg.norm(row)\n            if norm > 1e-8:\n                rot_cleaned[i] = row / norm\n            else:\n                rot_cleaned[i] = [1.0, 0.0, 0.0, 0.0]\n        elif missing_count == 1:\n            missing_idx = np.where(np.isnan(row))[0][0]\n            valid_values = row[~np.isnan(row)]\n            sum_squares = np.sum(valid_values**2)\n            if sum_squares <= 1.0:\n                missing_value = np.sqrt(max(0, 1.0 - sum_squares))\n                if i > 0 and not np.isnan(rot_cleaned[i-1, missing_idx]):\n                    if rot_cleaned[i-1, missing_idx] < 0:\n                        missing_value = -missing_value\n                rot_cleaned[i, missing_idx] = missing_value\n                rot_cleaned[i, ~np.isnan(row)] = valid_values\n            else:\n                rot_cleaned[i] = [1.0, 0.0, 0.0, 0.0]\n        else:\n            rot_cleaned[i] = [1.0, 0.0, 0.0, 0.0]\n    \n    return rot_cleaned\n\ndef compute_world_acceleration(acc: np.ndarray, rot: np.ndarray) -> np.ndarray:\n    \"\"\"Convert acceleration from device to world coordinates.\"\"\"\n    try:\n        rot_scipy = rot[:, [1, 2, 3, 0]]  # Convert to scipy format\n        norms = np.linalg.norm(rot_scipy, axis=1)\n        if np.any(norms < 1e-8):\n            mask = norms < 1e-8\n            rot_scipy[mask] = [0.0, 0.0, 0.0, 1.0]\n        r = R.from_quat(rot_scipy)\n        acc_world = r.apply(acc)\n    except Exception:\n        acc_world = acc.copy()\n    return acc_world\n\nprint('✓ Helper functions defined')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T14:39:39.436587Z","iopub.execute_input":"2025-08-12T14:39:39.437502Z","iopub.status.idle":"2025-08-12T14:39:39.446616Z","shell.execute_reply.started":"2025-08-12T14:39:39.437466Z","shell.execute_reply":"2025-08-12T14:39:39.445656Z"}},"outputs":[{"name":"stdout","text":"✓ Helper functions defined\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"def extract_statistical_features(data: np.ndarray, prefix: str) -> dict:\n    \"\"\"Extract statistical features from 1D time series.\"\"\"\n    features = {}\n    \n    # Basic statistics\n    features[f'{prefix}_mean'] = np.mean(data)\n    features[f'{prefix}_std'] = np.std(data)\n    features[f'{prefix}_var'] = np.var(data)\n    features[f'{prefix}_min'] = np.min(data)\n    features[f'{prefix}_max'] = np.max(data)\n    features[f'{prefix}_median'] = np.median(data)\n    features[f'{prefix}_q25'] = np.percentile(data, 25)\n    features[f'{prefix}_q75'] = np.percentile(data, 75)\n    features[f'{prefix}_iqr'] = features[f'{prefix}_q75'] - features[f'{prefix}_q25']\n    features[f'{prefix}_range'] = features[f'{prefix}_max'] - features[f'{prefix}_min']\n    \n    # Boundary features\n    features[f'{prefix}_first'] = data[0] if len(data) > 0 else 0\n    features[f'{prefix}_last'] = data[-1] if len(data) > 0 else 0\n    features[f'{prefix}_delta'] = features[f'{prefix}_last'] - features[f'{prefix}_first']\n    \n    # Higher order moments\n    if len(data) > 1 and np.std(data) > 1e-8:\n        features[f'{prefix}_skew'] = pd.Series(data).skew()\n        features[f'{prefix}_kurt'] = pd.Series(data).kurtosis()\n    else:\n        features[f'{prefix}_skew'] = 0\n        features[f'{prefix}_kurt'] = 0\n    \n    # Differential features\n    if len(data) > 1:\n        diff_data = np.diff(data)\n        features[f'{prefix}_diff_mean'] = np.mean(diff_data)\n        features[f'{prefix}_diff_std'] = np.std(diff_data)\n        features[f'{prefix}_n_changes'] = np.sum(np.abs(diff_data) > np.std(data) * 0.1)\n    else:\n        features[f'{prefix}_diff_mean'] = 0\n        features[f'{prefix}_diff_std'] = 0\n        features[f'{prefix}_n_changes'] = 0\n    \n    # Segment features (3 segments)\n    seq_len = len(data)\n    if seq_len >= 9:\n        seg_size = seq_len // 3\n        for i in range(3):\n            start_idx = i * seg_size\n            end_idx = (i + 1) * seg_size if i < 2 else seq_len\n            segment = data[start_idx:end_idx]\n            features[f'{prefix}_seg{i+1}_mean'] = np.mean(segment)\n            features[f'{prefix}_seg{i+1}_std'] = np.std(segment)\n        # Segment transitions\n        features[f'{prefix}_seg1_to_seg2'] = features[f'{prefix}_seg2_mean'] - features[f'{prefix}_seg1_mean']\n        features[f'{prefix}_seg2_to_seg3'] = features[f'{prefix}_seg3_mean'] - features[f'{prefix}_seg2_mean']\n    else:\n        for i in range(3):\n            features[f'{prefix}_seg{i+1}_mean'] = features[f'{prefix}_mean']\n            features[f'{prefix}_seg{i+1}_std'] = features[f'{prefix}_std']\n        features[f'{prefix}_seg1_to_seg2'] = 0\n        features[f'{prefix}_seg2_to_seg3'] = 0\n    \n    return features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T14:39:42.193648Z","iopub.execute_input":"2025-08-12T14:39:42.193949Z","iopub.status.idle":"2025-08-12T14:39:42.203867Z","shell.execute_reply.started":"2025-08-12T14:39:42.193931Z","shell.execute_reply":"2025-08-12T14:39:42.203110Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def extract_features(sequence: pl.DataFrame, demographics: pl.DataFrame) -> pd.DataFrame:\n    \"\"\"Extract features from IMU sequence.\"\"\"\n    # Convert to pandas\n    seq_df = sequence.to_pandas()\n    demo_df = demographics.to_pandas()\n    \n    # Get available columns\n    available_acc_cols = [col for col in ACC_COLS if col in seq_df.columns]\n    available_rot_cols = [col for col in ROT_COLS if col in seq_df.columns]\n    \n    # Handle missing values\n    acc_data = seq_df[available_acc_cols].copy()\n    acc_data = acc_data.ffill().bfill().fillna(0)\n    \n    rot_data = seq_df[available_rot_cols].copy()\n    rot_data = rot_data.ffill().bfill()\n    \n    # Handle quaternion missing values\n    rot_data_clean = handle_quaternion_missing_values(rot_data.values)\n    \n    # Compute world acceleration\n    world_acc_data = compute_world_acceleration(acc_data.values, rot_data_clean)\n    \n    # Initialize features\n    features = {}\n    \n    # Sequence metadata\n    features['sequence_length'] = len(seq_df)\n    \n    # Demographics features\n    if len(demo_df) > 0:\n        demo_row = demo_df.iloc[0]\n        features['age'] = demo_row.get('age', 0)\n        features['adult_child'] = demo_row.get('adult_child', 0)\n        features['sex'] = demo_row.get('sex', 0)\n        features['handedness'] = demo_row.get('handedness', 0)\n        features['height_cm'] = demo_row.get('height_cm', 0)\n        features['shoulder_to_wrist_cm'] = demo_row.get('shoulder_to_wrist_cm', 0)\n        features['elbow_to_wrist_cm'] = demo_row.get('elbow_to_wrist_cm', 0)\n    else:\n        # Default values if demographics not available\n        features['age'] = 0\n        features['adult_child'] = 0\n        features['sex'] = 0\n        features['handedness'] = 0\n        features['height_cm'] = 0\n        features['shoulder_to_wrist_cm'] = 0\n        features['elbow_to_wrist_cm'] = 0\n    \n    # Extract statistical features for each axis\n    for i, axis in enumerate(['x', 'y', 'z']):\n        if i < acc_data.shape[1]:\n            # Device acceleration\n            features.update(extract_statistical_features(acc_data.values[:, i], f'acc_{axis}'))\n            # World acceleration\n            features.update(extract_statistical_features(world_acc_data[:, i], f'world_acc_{axis}'))\n    \n    # Rotation features\n    for i, comp in enumerate(['w', 'x', 'y', 'z']):\n        if i < rot_data_clean.shape[1]:\n            features.update(extract_statistical_features(rot_data_clean[:, i], f'rot_{comp}'))\n    \n    # Magnitude features\n    acc_magnitude = np.linalg.norm(acc_data.values, axis=1)\n    world_acc_magnitude = np.linalg.norm(world_acc_data, axis=1)\n    \n    features.update(extract_statistical_features(acc_magnitude, 'acc_magnitude'))\n    features.update(extract_statistical_features(world_acc_magnitude, 'world_acc_magnitude'))\n    \n    # Difference between device and world acceleration\n    acc_world_diff = acc_magnitude - world_acc_magnitude\n    features.update(extract_statistical_features(acc_world_diff, 'acc_world_diff'))\n    \n    # Convert to DataFrame\n    result_df = pd.DataFrame([features])\n    \n    # Ensure all expected features are present\n    for col in feature_names:\n        if col not in result_df.columns:\n            result_df[col] = 0\n    \n    # Select only the features used in training\n    result_df = result_df[feature_names]\n    result_df = result_df.fillna(0)\n    \n    return result_df\n\nprint('✓ Feature extraction function defined')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T14:39:46.710882Z","iopub.execute_input":"2025-08-12T14:39:46.711177Z","iopub.status.idle":"2025-08-12T14:39:46.721407Z","shell.execute_reply.started":"2025-08-12T14:39:46.711160Z","shell.execute_reply":"2025-08-12T14:39:46.720702Z"}},"outputs":[{"name":"stdout","text":"✓ Feature extraction function defined\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Define prediction function for CMI inference server\ndef predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n    \"\"\"\n    Prediction function for CMI inference server.\n    Takes a single sequence and returns the predicted gesture name.\n    \"\"\"\n    try:\n        # Extract features\n        features = extract_features(sequence, demographics)\n        \n        # Get predictions from all models\n        predictions = []\n        probabilities = []\n        \n        for model in models:\n            # Get prediction probabilities\n            pred_proba = model.predict_proba(features)\n            probabilities.append(pred_proba[0])\n            \n            # Get predicted class\n            pred_class = np.argmax(pred_proba, axis=1)[0]\n            predictions.append(pred_class)\n        \n        # Ensemble: average probabilities\n        avg_proba = np.mean(probabilities, axis=0)\n        final_prediction = np.argmax(avg_proba)\n        \n        # Convert to gesture name\n        gesture_name = reverse_gesture_mapper[final_prediction]\n        \n        return gesture_name\n        \n    except Exception as e:\n        print(f\"Prediction error: {e}\")\n        # Return default prediction in case of error\n        return 'Text on phone'\n\nprint('✓ Prediction function defined')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T14:39:49.411867Z","iopub.execute_input":"2025-08-12T14:39:49.412160Z","iopub.status.idle":"2025-08-12T14:39:49.418160Z","shell.execute_reply.started":"2025-08-12T14:39:49.412141Z","shell.execute_reply":"2025-08-12T14:39:49.417391Z"}},"outputs":[{"name":"stdout","text":"✓ Prediction function defined\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Test the prediction function with a small example\nprint('Testing prediction function...')\n\n# Create dummy data for testing\ntest_sequence = pl.DataFrame({\n    'acc_x': np.random.randn(100),\n    'acc_y': np.random.randn(100),\n    'acc_z': np.random.randn(100),\n    'rot_w': np.random.randn(100),\n    'rot_x': np.random.randn(100),\n    'rot_y': np.random.randn(100),\n    'rot_z': np.random.randn(100)\n})\n\ntest_demographics = pl.DataFrame({\n    'age': [25],\n    'adult_child': [1],\n    'sex': [0],\n    'handedness': [1],\n    'height_cm': [175],\n    'shoulder_to_wrist_cm': [50],\n    'elbow_to_wrist_cm': [30]\n})\n\n# Test prediction\ntest_result = predict(test_sequence, test_demographics)\nprint(f'✓ Test prediction: {test_result}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T14:39:15.927142Z","iopub.execute_input":"2025-08-12T14:39:15.927445Z","iopub.status.idle":"2025-08-12T14:39:15.984413Z","shell.execute_reply.started":"2025-08-12T14:39:15.927430Z","shell.execute_reply":"2025-08-12T14:39:15.983921Z"}},"outputs":[{"name":"stdout","text":"Testing prediction function...\n✓ Test prediction: Neck - scratch\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Import the CMI inference server\nsys.path.append('/kaggle/input/cmi-detect-behavior-with-sensor-data')\nimport kaggle_evaluation.cmi_inference_server\n\n# Initialize CMI inference server\nprint('Initializing CMI inference server...')\n\ninference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n\nprint('✓ Inference server initialized')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T14:41:01.123648Z","iopub.execute_input":"2025-08-12T14:41:01.124174Z","iopub.status.idle":"2025-08-12T14:41:01.130047Z","shell.execute_reply.started":"2025-08-12T14:41:01.124156Z","shell.execute_reply":"2025-08-12T14:41:01.129308Z"}},"outputs":[{"name":"stdout","text":"Initializing CMI inference server...\n✓ Inference server initialized\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Run inference based on environment\nprint('Starting inference...')\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    # Competition environment: serve predictions\n    print('Running in competition environment...')\n    inference_server.serve()\nelse:\n    # Local testing: run on test data\n    print('Running in local testing mode...')\n    inference_server.run_local_gateway(\n        data_paths=(\n            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv',\n            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv',\n        )\n    )\n    print('\\n✓ Inference complete!')\n    print('✓ submission.parquet has been generated')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T14:41:01.640701Z","iopub.execute_input":"2025-08-12T14:41:01.640950Z","iopub.status.idle":"2025-08-12T14:41:22.566863Z","shell.execute_reply.started":"2025-08-12T14:41:01.640935Z","shell.execute_reply":"2025-08-12T14:41:22.566187Z"}},"outputs":[{"name":"stdout","text":"Starting inference...\nRunning in local testing mode...\n\n✓ Inference complete!\n✓ submission.parquet has been generated\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}