# è©³ç´°æ¯”è¼ƒãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼šdeep.py vs ãƒˆãƒƒãƒ—ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚¿ãƒ¼ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯

## ğŸ“Š å®Ÿè£…æ¯”è¼ƒãƒãƒˆãƒªãƒƒã‚¯ã‚¹

| æ©Ÿèƒ½/æ‰‹æ³• | deep.py | LB 0.77 TF | PyTorch CNN | LightGBM World |
|----------|---------|------------|-------------|----------------|
| **åŸºæœ¬ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£** | LightGBMï¼ˆæœªå®Œæˆï¼‰ | TF BiLSTM+GRU+Attention âœ… | PyTorch 1D-CNN âœ… | LightGBM âœ… |
| **æ·±å±¤å­¦ç¿’å®Ÿè£…** | âŒ æœªå®Ÿè£… | âœ… å®Œå…¨å®Ÿè£… | âœ… å®Œå…¨å®Ÿè£… | N/A |
| **é‡åŠ›é™¤å»** | âœ… å®Ÿè£…ã‚ã‚Š | âœ… å®Ÿè£…ã‚ã‚Š | âŒ ãªã— | âœ… ä¸–ç•Œåº§æ¨™å¤‰æ› |
| **æ™‚ç³»åˆ—å‡¦ç†** | âŒ çµ±è¨ˆç‰¹å¾´é‡ã®ã¿ | âœ… ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼‹ç›´æ¥å…¥åŠ› | âœ… ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼‹ç›´æ¥å…¥åŠ› | âŒ çµ±è¨ˆç‰¹å¾´é‡ |
| **MixUp** | âŒ æœªå®Ÿè£… | âœ… å®Ÿè£…ã‚ã‚Š | âŒ ãªã— | N/A |
| **Two-Branch** | âŒ æœªå®Ÿè£… | âœ… IMU/TOFåˆ†é›¢ | âŒ ãªã— | N/A |
| **å‹•ä½œç¢ºèª** | âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ | âœ… å‹•ä½œç¢ºèªæ¸ˆ | âœ… å‹•ä½œç¢ºèªæ¸ˆ | âœ… å‹•ä½œç¢ºèªæ¸ˆ |

## ğŸš¨ deep.pyã®è‡´å‘½çš„å•é¡Œ

### 1. **ã‚³ãƒ¼ãƒ‰ã®çŸ›ç›¾**
```python
# Line 2-12: ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜
# "Deep Learning with 1D-CNN + BiLSTM + Attention"ã¨è¬³ã£ã¦ã„ã‚‹

# Line 565-566: å®Ÿéš›ã®å®Ÿè£…
binary_model = lgb.LGBMClassifier(**self.config["binary_lgbm"])
# â†’ LightGBMã‚’ä½¿ç”¨ï¼ˆæ·±å±¤å­¦ç¿’ã§ã¯ãªã„ï¼ï¼‰
```

### 2. **æœªã‚¤ãƒ³ãƒãƒ¼ãƒˆã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«**
```python
# å¿…è¦ã ãŒæœªã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼š
import lightgbm as lgb
from imblearn.over_sampling import SMOTE
```

### 3. **æœªå®šç¾©ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**
```python
CONFIG = {
    # æ·±å±¤å­¦ç¿’ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å®šç¾©ã•ã‚Œã¦ã„ã‚‹ãŒæœªä½¿ç”¨
    "cnn_filters": [64, 128, 256],  # ä½¿ã‚ã‚Œã¦ã„ãªã„
    "lstm_units": 128,               # ä½¿ã‚ã‚Œã¦ã„ãªã„
    
    # LightGBMç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæœªå®šç¾©
    # "binary_lgbm": {...},  â† ã“ã‚ŒãŒå¿…è¦ï¼
    # "bfrb_lgbm": {...},    â† ã“ã‚ŒãŒå¿…è¦ï¼
}
```

## ğŸ’¡ ãƒˆãƒƒãƒ—ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‹ã‚‰å­¦ã¶ã¹ãå®Ÿè£…

### 1. **LB 0.77 (TensorFlow) ã®å„ªã‚ŒãŸå®Ÿè£…**

#### é‡åŠ›é™¤å»ã®å®Ÿè£…
```python
def remove_gravity_from_acc(acc_data, rot_data):
    """å››å…ƒæ•°ã‚’ä½¿ã£ã¦é‡åŠ›æˆåˆ†ã‚’é™¤å»"""
    rotation = R.from_quat(quat_values)
    gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)
    linear_accel = acc_values - gravity_sensor_frame
    return linear_accel
```

#### Two-Branch Architecture
```python
def build_two_branch_model():
    # IMUå°‚ç”¨ã®æ·±ã„ãƒ–ãƒ©ãƒ³ãƒ
    x1 = residual_se_cnn_block(imu, 64, 3)
    x1 = residual_se_cnn_block(x1, 128, 5)
    
    # TOF/Thermalç”¨ã®è»½ã„ãƒ–ãƒ©ãƒ³ãƒ
    x2 = Conv1D(64, 3)(tof)
    x2 = MaxPooling1D(2)(x2)
    
    # ãƒãƒ¼ã‚¸ã—ã¦å‡¦ç†
    merged = Concatenate()([x1, x2])
    xa = Bidirectional(LSTM(128))(merged)
    xb = Bidirectional(GRU(128))(merged)
```

#### MixUpå®Ÿè£…
```python
class MixupGenerator(Sequence):
    def __getitem__(self, i):
        lam = np.random.beta(self.alpha, self.alpha)
        X_mix = lam * Xb + (1-lam) * Xb[perm]
        y_mix = lam * yb + (1-lam) * yb[perm]
        return X_mix, y_mix
```

### 2. **PyTorch CNNã®å„ªã‚ŒãŸå®Ÿè£…**

#### 1D-CNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
```python
class CNN1D(nn.Module):
    def __init__(self):
        self.block1 = nn.Sequential(
            nn.Conv1d(feature_dim, 256, kernel_size=7),
            nn.BatchNorm1d(256),
            nn.GELU(),
            nn.MaxPool1d(2),
            nn.Dropout(0.3)
        )
```

#### ç‰¹å¾´ç”Ÿæˆ
```python
def feature_gen(arr, gen_index):
    """2ã¤ã®ç‰¹å¾´ã‹ã‚‰æ–°ç‰¹å¾´ã‚’ç”Ÿæˆ"""
    # acc_x - acc_y â†’ acc_x_y_diff
    gen_arr[:, arr.shape[1]+i] = arr[:, ind[0]] - arr[:, ind[1]]
```

### 3. **LightGBM World Accã®å„ªã‚ŒãŸå®Ÿè£…**

#### ä¸–ç•Œåº§æ¨™å¤‰æ›
```python
def compute_world_acceleration(acc, rot):
    """ãƒ‡ãƒã‚¤ã‚¹åº§æ¨™ã‹ã‚‰ä¸–ç•Œåº§æ¨™ã¸å¤‰æ›"""
    rot_scipy = rot[:, [1, 2, 3, 0]]  # scipyå½¢å¼ã¸
    r = R.from_quat(rot_scipy)
    acc_world = r.apply(acc)
    return acc_world
```

## ğŸ”§ deep.pyã®æ”¹å–„ææ¡ˆ

### Phase 1: LightGBMãƒ™ãƒ¼ã‚¹ã§å‹•ä½œå¯èƒ½ã«ã™ã‚‹
```python
# 1. å¿…è¦ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ 
import lightgbm as lgb
from imblearn.over_sampling import SMOTE

# 2. LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©
CONFIG["binary_lgbm"] = {
    'objective': 'binary',
    'n_estimators': 1000,
    'max_depth': 8,
    'learning_rate': 0.025,
    'colsample_bytree': 0.5,
    'subsample': 0.5,
    'random_state': 42,
}

CONFIG["bfrb_lgbm"] = {
    'objective': 'multiclass',
    'num_class': 8,
    'n_estimators': 1000,
    'max_depth': 8,
    'learning_rate': 0.025,
    'random_state': 42,
}
```

### Phase 2: æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…
```python
def create_deep_model(sequence_length, n_features, n_classes):
    """å®Ÿéš›ã®æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…"""
    inputs = tf.keras.Input((sequence_length, n_features))
    
    # 1D-CNN layers
    x = tf.keras.layers.Conv1D(64, 3, padding='same', activation='relu')(inputs)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.MaxPooling1D(2)(x)
    
    # BiLSTM layer
    x = tf.keras.layers.Bidirectional(
        tf.keras.layers.LSTM(128, return_sequences=True)
    )(x)
    
    # Attention layer
    attention = tf.keras.layers.MultiHeadAttention(
        num_heads=4, key_dim=32
    )(x, x)
    
    # Classification head
    x = tf.keras.layers.GlobalAveragePooling1D()(attention)
    x = tf.keras.layers.Dense(256, activation='relu')(x)
    x = tf.keras.layers.Dropout(0.4)(x)
    outputs = tf.keras.layers.Dense(n_classes, activation='softmax')(x)
    
    return tf.keras.Model(inputs, outputs)
```

### Phase 3: ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®æ”¹å–„
```python
def prepare_sequences_for_dl(df, max_len=500):
    """æ·±å±¤å­¦ç¿’ç”¨ã«æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™"""
    sequences = []
    for seq_id in df['sequence_id'].unique():
        seq_data = df[df['sequence_id'] == seq_id]
        
        # IMUãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        imu_data = seq_data[['acc_x', 'acc_y', 'acc_z', 
                            'rot_w', 'rot_x', 'rot_y', 'rot_z']].values
        
        # é‡åŠ›é™¤å»
        linear_acc = remove_gravity_from_acc(
            imu_data[:, :3], imu_data[:, 3:]
        )
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°/ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ãƒˆ
        if len(imu_data) < max_len:
            padded = np.pad(imu_data, ((0, max_len - len(imu_data)), (0, 0)))
        else:
            padded = imu_data[:max_len]
        
        sequences.append(padded)
    
    return np.array(sequences)
```

## ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

| ãƒ¢ãƒ‡ãƒ« | ã‚¹ã‚³ã‚¢ | å‚™è€ƒ |
|--------|--------|------|
| LB 0.77 TF | 0.77 | BiLSTM+GRU+Attentionã€é‡åŠ›é™¤å» |
| PyTorch CNN | 0.67-0.71 | 1D-CNNã€ç‰¹å¾´ç”Ÿæˆ |
| LightGBM World | æ¨å®š0.65-0.70 | ä¸–ç•Œåº§æ¨™å¤‰æ›ã€çµ±è¨ˆç‰¹å¾´ |
| deep.py | **å‹•ä½œä¸å¯** | ã‚¨ãƒ©ãƒ¼ã§å®Ÿè¡Œä¸å¯ |

## ğŸ¯ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³

### å„ªå…ˆåº¦1ï¼šå³åº§ã«ä¿®æ­£ã™ã¹ãç‚¹
1. âœ… LightGBMã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆè¿½åŠ 
2. âœ… è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å®šç¾©
3. âœ… SMOTEã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆè¿½åŠ 

### å„ªå…ˆåº¦2ï¼šæ·±å±¤å­¦ç¿’ã®å®Ÿè£…
1. âœ… TensorFlowã¾ãŸã¯PyTorchã§ãƒ¢ãƒ‡ãƒ«å®Ÿè£…
2. âœ… æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
3. âœ… MixUpã®å®Ÿè£…

### å„ªå…ˆåº¦3ï¼šã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
1. âœ… LightGBM + æ·±å±¤å­¦ç¿’ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
2. âœ… Test Time Augmentation
3. âœ… Pseudo-labeling

## çµè«–

deep.pyã¯å„ªã‚ŒãŸç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆä¸–ç•Œåº§æ¨™å¤‰æ›ã€è§’é€Ÿåº¦è¨ˆç®—ãªã©ï¼‰ã‚’æŒã£ã¦ã„ã¾ã™ãŒã€**æ·±å±¤å­¦ç¿’ã®å®Ÿè£…ãŒå®Œå…¨ã«æ¬ å¦‚**ã—ã¦ã„ã¾ã™ã€‚ãƒˆãƒƒãƒ—ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®å®Ÿè£…ã‚’å‚è€ƒã«ã€æ®µéšçš„ã«æ”¹å–„ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šé«˜ã„ã‚¹ã‚³ã‚¢ãŒæœŸå¾…ã§ãã¾ã™ã€‚

ç‰¹ã«ã€LB 0.77ã®Two-Branch Architectureã¨MixUpã®å®Ÿè£…ã¯ã€ã™ãã«é©ç”¨å¯èƒ½ã§åŠ¹æœçš„ã§ã™ã€‚ã¾ãšã¯LightGBMãƒ™ãƒ¼ã‚¹ã§å‹•ä½œå¯èƒ½ã«ã—ã¦ã‹ã‚‰ã€æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ å®Ÿè£…ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚