- ã‚ã‚‹ã¹ããƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼ˆfeatures cacheï¼fold åˆ¥ãƒ¢ãƒ‡ãƒ«ï¼epoch ckptï¼bundleï¼‰ãŒ**å®Ÿåœ¨ã™ã‚‹ãªã‚‰å†åˆ©ç”¨ï¼å†é–‹**ã€
- **ç„¡ã‘ã‚Œã°å­¦ç¿’**ã€
- ãã—ã¦**å¿…ãšé–‹å§‹å‰ã«ã€Œä½•ã‚’ã©ã†ã™ã‚‹ã‹ã€ã‚’è¦ç´„è¡¨ç¤º**
  ã¨ã„ã†æµã‚Œã«ã™ã‚‹ã¨åˆ†ã‹ã‚Šã‚„ã™ããªã‚Šã¾ã™ã€‚

ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’ãã®ã¾ã¾æŒ¿å…¥ã™ã‚Œã°å‹•ãã¾ã™ï¼ˆ**å·®ã—è¾¼ã¿ä½ç½®**ã¯ `main()` ã®å…ˆé ­ã‚ãŸã‚Šã€æ—¢å­˜ã® `RUN_LGBM_TRAINING` ç­‰ã‚’æ±ºã‚ã¦ã„ã‚‹è¡Œã®**ä»£ã‚ã‚Š**ã¨ã—ã¦ä½¿ã£ã¦ãã ã•ã„ï¼‰ã€‚
ã¾ãŸã€features cache ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¨€åŠã—ãŸ **`train_feature.joblib`ï¼ˆå˜æ•°å½¢ï¼‰** ã¨ã€ç¾è¡Œã‚³ãƒ¼ãƒ‰ã® **`train_features.joblib`ï¼ˆè¤‡æ•°å½¢ï¼‰** ã®**ä¸¡æ–¹ã‚’è‡ªå‹•æ¤œå‡º**ã—ã€å­˜åœ¨ã™ã‚‹æ–¹ã‚’ä½¿ã„ã¾ã™ã€‚

---

## 1) è¿½åŠ ï¼šCheckPointPaths ã¨å®Ÿè¡Œãƒ—ãƒ©ãƒ³ãƒ“ãƒ«ãƒ€

```python
# ==== NEW: CheckPointPaths & Run Plan =========================================
class CheckPointPaths:
    # æ—¢å®šã¯ CheckpointConfig ã‚’è¸è¥²ã€‚ç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãå¯èƒ½ã€‚
    BASE_DIR = os.getenv("CKPT_BASE_DIR", CheckpointConfig.CKPT_DIR)

    # features cache: å…¬å¼å + æ—§åï¼ˆå˜æ•°å½¢ï¼‰ã®ä¸¡æ–¹ã‚’ã‚µãƒãƒ¼ãƒˆ
    FEATURES_CACHE_MAIN = os.getenv(
        "FEATURES_CACHE",
        os.path.join(BASE_DIR, "train_features.joblib")
    )
    FEATURES_CACHE_LEGACY = os.getenv(
        "FEATURES_CACHE_LEGACY",
        os.path.join(BASE_DIR, "train_feature.joblib")  # â† æ—§ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è‡ªå‹•æ¤œçŸ¥
    )

    # LGBM fold/çŠ¶æ…‹ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæ—¢å­˜ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å°Šé‡ï¼ä¸Šæ›¸ãå¯ï¼‰
    LGBM_FOLD_TMPL = os.getenv("LGBM_FOLD_TMPL", CheckpointConfig.LGBM_FOLD_TMPL)
    LGBM_STATE_JSON = os.getenv("LGBM_STATE_JSON", CheckpointConfig.LGBM_STATE_JSON)

    # Torch/Keras ã® epoch/çŠ¶æ…‹ãƒ•ã‚¡ã‚¤ãƒ«
    TORCH_EPOCH_TMPL = os.getenv("TORCH_EPOCH_TMPL", CheckpointConfig.TORCH_EPOCH_CKPT_TMPL)
    KERAS_STATE_JSON = os.getenv("KERAS_STATE_JSON", CheckpointConfig.KERAS_STATE_JSON)


def _first_existing(*paths):
    for p in paths:
        if p and os.path.exists(p):
            return p
    return None

def _bool_env(name, default=0):
    try:
        return bool(int(os.getenv(name, str(default))))
    except Exception:
        return bool(default)

def build_run_plan(n_folds=Config.N_FOLDS):
    """å†åˆ©ç”¨ï¼å†é–‹ï¼å­¦ç¿’ã®æ–¹é‡ã‚’ä¸€æ‹¬æ±ºå®šã—ã¦ãƒ­ã‚°ã«ä½¿ãˆã‚‹ dict ã‚’è¿”ã™ã€‚"""
    plan = {}

    # 1) features cache
    feat_path = _first_existing(CheckPointPaths.FEATURES_CACHE_MAIN,
                                CheckPointPaths.FEATURES_CACHE_LEGACY)
    plan["features"] = {
        "path": feat_path or CheckPointPaths.FEATURES_CACHE_MAIN,
        "exists": feat_path is not None,
        "action": "reuse" if feat_path is not None else "build",
        "reason": "cache found" if feat_path else "no cache"
    }

    # 2) LGBMï¼ˆpretrained bundle / foldå†é–‹ / å­¦ç¿’ï¼‰
    lgbm_bundle = Config.MODEL_PATH  # ModelPaths ã‹ã‚‰å¼•ãç¶™ãŒã‚Œã¦ã„ã‚‹æƒ³å®š
    lgbm_bundle_exists = bool(lgbm_bundle) and os.path.exists(lgbm_bundle)
    fold_paths = [
        os.path.join(CheckpointConfig.CKPT_DIR,
                     CheckpointConfig.LGBM_FOLD_TMPL.format(i))
        for i in range(n_folds)
    ]
    some_fold_exists = any(os.path.exists(p) for p in fold_paths)
    if _bool_env("FORCE_TRAIN_LGBM", 0):
        lgbm_action, reason = "train", "FORCE_TRAIN_LGBM=1"
    elif lgbm_bundle_exists:
        lgbm_action, reason = "reuse", "pretrained bundle found"
    elif some_fold_exists:
        lgbm_action, reason = "resume", "found fold checkpoints"
    else:
        lgbm_action, reason = "train", "no bundle/checkpoints"
    plan["lgbm"] = {
        "bundle_path": lgbm_bundle,
        "bundle_exists": lgbm_bundle_exists,
        "fold_paths": fold_paths,
        "has_fold_ckpt": some_fold_exists,
        "action": lgbm_action,
        "reason": reason,
    }

    # 3) Torch
    torch_bundle = EnsembleConfig.TORCH_BUNDLE_PATH
    torch_bundle_exists = bool(torch_bundle) and os.path.exists(torch_bundle)
    torch_best_paths = [
        os.path.join(DLConfig.TORCH_OUT_DIR, DLConfig.WEIGHT_TMPL.format(i))
        for i in range(DLConfig.N_FOLDS)
    ]
    torch_has_best = any(os.path.exists(p) for p in torch_best_paths)
    if (not TORCH_AVAILABLE) or (not Config.USE_TORCH):
        torch_action, reason = "skip", "torch unavailable or disabled"
    elif _bool_env("FORCE_TRAIN_TORCH", 0):
        torch_action, reason = "train", "FORCE_TRAIN_TORCH=1"
    elif torch_bundle_exists:
        torch_action, reason = "reuse", "bundle found"
    elif torch_has_best:
        torch_action, reason = "resume", "found per-fold best weights"
    else:
        torch_action, reason = "train", "no bundle/weights"
    plan["torch"] = {
        "bundle_path": torch_bundle,
        "bundle_exists": torch_bundle_exists,
        "has_best": torch_has_best,
        "action": torch_action,
        "reason": reason,
    }

    # 4) Keras
    keras_bundle = EnsembleConfig.KERAS_BUNDLE_PATH
    keras_bundle_exists = bool(keras_bundle) and os.path.exists(keras_bundle)
    keras_best_paths = [
        os.path.join(KerasConfig.OUT_DIR, KerasConfig.WEIGHT_TMPL.format(i))
        for i in range(KerasConfig.N_FOLDS)
    ]
    keras_has_best = any(os.path.exists(p) for p in keras_best_paths)
    if (not KERAS_AVAILABLE) or (not Config.USE_KERAS):
        keras_action, reason = "skip", "keras unavailable or disabled"
    elif _bool_env("FORCE_TRAIN_KERAS", 0):
        keras_action, reason = "train", "FORCE_TRAIN_KERAS=1"
    elif keras_bundle_exists:
        keras_action, reason = "reuse", "bundle found"
    elif keras_has_best:
        keras_action, reason = "resume", "found per-fold best weights"
    else:
        keras_action, reason = "train", "no bundle/weights"
    plan["keras"] = {
        "bundle_path": keras_bundle,
        "bundle_exists": keras_bundle_exists,
        "has_best": keras_has_best,
        "action": keras_action,
        "reason": reason,
    }
    return plan

def print_plan(plan):
    def _line(name, entry):
        icon = {"reuse": "âœ“", "resume": "â†»", "train": "ğŸ› ", "build": "ğŸ› ", "skip": "â­"}.get(entry["action"], "?")
        print(f"[PLAN] {name:7s}: {icon} {entry['action']:6s} â€” {entry.get('reason','')}")
        if "path" in entry:
            print(f"        path   : {entry['path']} (exists={entry.get('exists', False)})")
        if "bundle_path" in entry:
            print(f"        bundle : {entry['bundle_path']} (exists={entry.get('bundle_exists', False)})")
    print("========== RUN PLAN ==========")
    _line("features", plan["features"])
    _line("lgbm",     plan["lgbm"])
    _line("torch",    plan["torch"])
    _line("keras",    plan["keras"])
    print("==============================")
# ============================================================================

```

---

## 2) ç½®ãæ›ãˆï¼š`main()` å†’é ­ã®åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯

æ—¢å­˜ã®

```python
# Training flags for each model type
RUN_LGBM_TRAINING = Config.MODEL_PATH is None
RUN_TORCH_TRAINING = (
    TORCH_AVAILABLE
    and Config.USE_TORCH
    and not os.path.exists(EnsembleConfig.TORCH_BUNDLE_PATH)
)
RUN_KERAS_TRAINING = (
    KERAS_AVAILABLE
    and Config.USE_KERAS
    and not os.path.exists(EnsembleConfig.KERAS_BUNDLE_PATH)
)
RUN_ANY_TRAINING = RUN_LGBM_TRAINING or RUN_TORCH_TRAINING or RUN_KERAS_TRAINING
```

ã‚’ **ä¸‹è¨˜ã«ç½®ãæ›ãˆ**ï¼š

```python
    # === NEW: CheckPointPaths ã‚’ä½¿ã£ã¦å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‚’å›ºã‚ã‚‹ ===
    plan = build_run_plan(n_folds=Config.N_FOLDS)
    print_plan(plan)

    # features cache ã®æœ€çµ‚æ¡ç”¨ãƒ‘ã‚¹ã‚’å…¨ä½“è¨­å®šã¸åæ˜ ï¼ˆä»¥é™ã®å‡¦ç†ã¯ã“ã®ãƒ‘ã‚¹ã‚’è¦‹ã‚‹ï¼‰
    CheckpointConfig.FEATURES_CACHE = plan["features"]["path"]

    # ã“ã‚Œä»¥é™ã®ãƒ•ãƒ©ã‚°ã¯ plan ã«æº–æ‹ ï¼ˆresume ã‚‚å­¦ç¿’æ‰±ã„ã§OKï¼šå†…éƒ¨ã§ foldæ¯ã«ã‚¹ã‚­ãƒƒãƒ—æ¸ˆï¼‰
    RUN_LGBM_TRAINING  = plan["lgbm"]["action"]  in ("train", "resume")
    RUN_TORCH_TRAINING = plan["torch"]["action"] in ("train", "resume")
    RUN_KERAS_TRAINING = plan["keras"]["action"] in ("train", "resume")
    RUN_ANY_TRAINING = RUN_LGBM_TRAINING or RUN_TORCH_TRAINING or RUN_KERAS_TRAINING
```

> âœ… ã“ã‚Œã§ **ã€Œãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ç©ºãƒ‘ã‚¹ãªã®ã«ã€å†åˆ©ç”¨ã€ã¨è¦‹ãˆã‚‹ãƒ­ã‚°ãŒå‡ºã‚‹ã€å•é¡Œã¯ç™ºç”Ÿã—ã¾ã›ã‚“**ã€‚
> plan ã¯å­˜åœ¨ç¢ºèªæ¸ˆã¿ã®ã¨ãã ã‘ `reuse` ã‚’å‡ºã—ã€ç„¡ã‘ã‚Œã° `train` / `resume` ã‚’å‡ºã—ã¾ã™ã€‚

---

## 3) ç½®ãæ›ãˆï¼šåˆ†å²ã”ã¨ã®ãƒ­ã‚°ã‚’ã€Œãƒ—ãƒ©ãƒ³æº–æ‹ ã€ã«

`main()` å†…ã®ãã‚Œãã‚Œã®ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ã‚°ã‚‚ã€**plan ã‚’æ ¹æ‹ ã«å‡ºã™**ã‚ˆã†ã«è»½ãä¿®æ­£ã™ã‚‹ã¨ã‚ˆã‚Šæ˜ç¢ºã§ã™ã€‚

### LGBMï¼ˆå†’é ­ã®ã€Œå­¦ç¿’ã‚¹ã‚­ãƒƒãƒ—ã€ãƒ­ã‚°ï¼‰

ã“ã®ãƒ–ãƒ­ãƒƒã‚¯ï¼š

```python
else:
    print("âœ“ Skipping LGBM training. Using pretrained model at:", Config.MODEL_PATH)
    RUNTIME_MODEL_PATH = Config.MODEL_PATH
```

ã‚’æ¬¡ã®ã‚ˆã†ã«ï¼š

```python
else:
    if plan["lgbm"]["action"] == "reuse":
        print("âœ“ Skipping LGBM training. Using pretrained model at:", Config.MODEL_PATH)
        RUNTIME_MODEL_PATH = Config.MODEL_PATH
    else:
        # plan ä¸Šã“ã“ã«æ¥ãªã„æƒ³å®šã ãŒã€ä¿é™ºã§æ˜ç¤ºã‚¨ãƒ©ãƒ¼ã«ã™ã‚‹
        raise FileNotFoundError(
            f"No LGBM bundle found. Plan decided '{plan['lgbm']['action']}'. "
            "Set Config.MODEL_PATH=None to train or provide a valid bundle path."
        )
```

### Torch

```python
if RUN_TORCH_TRAINING:
    print("\nStarting Torch training..." if plan["torch"]["action"] == "train" else "\nResuming Torch training...")
    train_torch_models(train_df, train_demographics)
    print("âœ“ Torch training complete")
elif TORCH_AVAILABLE and Config.USE_TORCH:
    if plan["torch"]["action"] == "reuse":
        print(f"âœ“ Skipping Torch training. Using pretrained bundle at: {EnsembleConfig.TORCH_BUNDLE_PATH}")
    elif plan["torch"]["action"] == "skip":
        print("â­ Torch disabled/unavailable. Skipping.")
    else:
        print(f"âš ï¸ Torch bundle not found at: {EnsembleConfig.TORCH_BUNDLE_PATH}")
```

### Keras ã‚‚åŒæ§˜

```python
if RUN_KERAS_TRAINING:
    print("\nStarting Keras training..." if plan["keras"]["action"] == "train" else "\nResuming Keras training...")
    train_keras_models(train_df, train_demographics)
    print("âœ“ Keras training complete")
elif KERAS_AVAILABLE and Config.USE_KERAS:
    if plan["keras"]["action"] == "reuse":
        print(f"âœ“ Skipping Keras training. Using pretrained bundle at: {EnsembleConfig.KERAS_BUNDLE_PATH}")
    elif plan["keras"]["action"] == "skip":
        print("â­ Keras disabled/unavailable. Skipping.")
    else:
        print(f"âš ï¸ Keras bundle not found at: {EnsembleConfig.KERAS_BUNDLE_PATH}")
else:
    print("â„¹ï¸ Keras not available or disabled. Skipping Keras training.")
```

---

## 4) features cache ã®å†åˆ©ç”¨ï¼ˆ`train_feature.joblib` å¯¾å¿œï¼‰

ä¸Šã®ã€Œãƒ—ãƒ©ãƒ³ã€æŒ¿å…¥ã§ã€**`CheckpointConfig.FEATURES_CACHE` ãŒæ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã«å·®ã—æ›¿ã‚ã‚‹**ãŸã‚ã€
æ—¢å­˜ã®ã“ã®åˆ†å²ã¯ãã®ã¾ã¾ã§å‹•ãã¾ã™ï¼š

```python
features_cache_path = CheckpointConfig.FEATURES_CACHE
cached = CheckpointConfig.RESUME and os.path.exists(features_cache_path)
if cached:
    cache = joblib.load(features_cache_path)
    ...
else:
    # æŠ½å‡ºã—ã¦ä¿å­˜
    joblib.dump({...}, features_cache_path)
```

> ğŸ” ã“ã‚Œã§ **å‰å›ã® `train_feature.joblib`ï¼ˆå˜æ•°å½¢ï¼‰** ãŒã‚ã‚Œã°è‡ªå‹•ã§æ‹¾ã„ã€
> ç„¡ã‘ã‚Œã° **`train_features.joblib`ï¼ˆè¤‡æ•°å½¢ï¼‰** ã«å‡ºåŠ›ã—ã¾ã™ã€‚
> ï¼ˆå¿…è¦ãªã‚‰ `FEATURES_CACHE` / `FEATURES_CACHE_LEGACY` ç’°å¢ƒå¤‰æ•°ã§ä»»æ„ã®ãƒ‘ã‚¹ã«å›ºå®šã§ãã¾ã™ï¼‰

---

## 5) ä½¿ã„æ–¹ï¼ˆé‹ç”¨ãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆï¼‰

- **ä½•ã‚‚ã—ãªãã¦ã‚‚**èµ·å‹•æ™‚ã«æ¬¡ã®ã‚ˆã†ãª**ãƒ—ãƒ©ãƒ³è¦ç´„**ãŒå‡ºã¾ã™ï¼ˆä¾‹ï¼‰ï¼š

  ```
  ========== RUN PLAN ==========
  [PLAN] features: âœ“ reuse  â€” cache found
          path   : /kaggle/working/checkpoints/train_feature.joblib (exists=True)
  [PLAN] lgbm   : ğŸ›  train  â€” no bundle/checkpoints
          bundle : /kaggle/input/cmi-bfrb-v9-lightgbm/.../imu_lgbm_model-4.pkl (exists=False)
  [PLAN] torch  : â†» resume â€” found per-fold best weights
          bundle : /kaggle/working/torch_models/torch_bundle.pkl (exists=False)
  [PLAN] keras  : â­ skip   â€” keras unavailable or disabled
  ==============================
  ```

- **ç’°å¢ƒå¤‰æ•°ã§å¼·åˆ¶åˆ¶å¾¡**ï¼š

  - `FORCE_TRAIN_LGBM=1` / `FORCE_TRAIN_TORCH=1` / `FORCE_TRAIN_KERAS=1`
    â†’ æ—¢å­˜ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆãŒã‚ã£ã¦ã‚‚**å¿…ãšå­¦ç¿’**ã€‚
  - `FEATURES_CACHE=/kaggle/working/checkpoints/train_feature.joblib`
    â†’ cache ãƒ‘ã‚¹ã‚’**æ˜ç¤º**ã€‚
  - `CKPT_BASE_DIR=/kaggle/working/my_ckpts`
    â†’ ã™ã¹ã¦ã® checkpoint æ—¢å®šãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åˆ‡æ›¿ãˆã€‚

- **Model bundle ã®å†åˆ©ç”¨ã‚’æ˜ç¤ºã—ãŸã„**ã¨ãã¯å¾“æ¥ã©ãŠã‚Šï¼š

  - `ModelPaths.LGBM_BUNDLE_PATH` ã« Kaggle Dataset ã®**çµ¶å¯¾ãƒ‘ã‚¹**ã‚’ç½®ã
  - `TORCH_BUNDLE_PATH` / `KERAS_BUNDLE_PATH` ã‚’ **å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«**ã«è¨­å®š
    â†’ Plan ãŒ `reuse` ã¨è¡¨ç¤ºã—ã€å­¦ç¿’ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚

---

## 6) ãªãœã“ã®è¨­è¨ˆãŒåŠ¹ãã‹

- **å˜ä¸€ã®ã€ŒçœŸå®Ÿã®æºã€(CheckPointPaths + Plan)** ã§é–‹å§‹å‰ã«**å­˜åœ¨ç¢ºèª â†’ æ–¹é‡æ±ºå®š â†’ ãƒ­ã‚°æ˜ç¤º**ã‚’è¡Œã†ãŸã‚ã€
  _ã€Œãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ç©ºãƒ‘ã‚¹ãªã®ã«å†åˆ©ç”¨ã£ã½ãè¦‹ãˆã‚‹ã€_ ãƒ­ã‚°ã¯å‡ºã¾ã›ã‚“ã€‚
- `resume` ã®å ´åˆã§ã‚‚ã€æ—¢å­˜ã®

  - LGBM: fold ã”ã¨ã® checkpointï¼state jsonã€
  - Torch: fold best ãŒã‚ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—ã€epoch ckpt ã‹ã‚‰å†é–‹ã€
  - Keras: best é‡ã¿ãŒã‚ã‚Œã° fold ã‚¹ã‚­ãƒƒãƒ—
    ã¨ã„ã†**å†é–‹æŒ™å‹•ã‚’ãã®ã¾ã¾æ´»ã‹ã™**æ§‹é€ ã«ãªã£ã¦ã„ã¾ã™ã€‚

- `train_feature(s).joblib` ã®**ä¸¡å¯¾å¿œ**ã§ã€ãƒ•ã‚¡ã‚¤ãƒ«åã®æºã‚Œã«ã‚‚å¼·ã„ã§ã™ã€‚
