#!/usr/bin/env python3
# ====================================================================================================
# CMI-BFRBæ¤œå‡º: é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã¨XGBoost v2.0
# ====================================================================================================
#
# ğŸ“Š ä½¿ç”¨æ–¹æ³•:
# -----------
# Kaggleç’°å¢ƒ:
#   1. IS_KAGGLE_ENV = True ã«è¨­å®š
#   2. ã‚¹ã‚¯ãƒªãƒ—ãƒˆå…¨ä½“ã‚’Kaggleãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã«ã‚³ãƒ”ãƒ¼
#   3. å®Ÿè¡Œï¼ˆã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ¸ˆã¿ç‰¹å¾´é‡ã®ä½¿ç”¨ã‚’æ¨å¥¨ï¼‰
#
# ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ:
#   1. IS_KAGGLE_ENV = False ã«è¨­å®š
#   2. uv run 6_Feature_Research/feature_engineering_xgboost.py ã§å®Ÿè¡Œ
#
# é«˜é€Ÿå®Ÿè¡Œï¼ˆç‰¹å¾´é‡å†åˆ©ç”¨ï¼‰:
#   1. USE_EXPORTED_FEATURES = True ã«è¨­å®š
#   2. EXPORTED_FEATURES_PATH ã‚’é©åˆ‡ã«è¨­å®š
#
# ====================================================================================================
#
# ğŸ”§ å®Ÿè£…ã—ã¦ã„ã‚‹7ç¨®é¡ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°:
# ====================================================================================================
#
# â‘  IMUç‰¹å¾´é‡ï¼ˆæ…£æ€§æ¸¬å®šè£…ç½®ï¼‰
#    - 3è»¸åŠ é€Ÿåº¦è¨ˆ: acc_x, acc_y, acc_z ã®çµ±è¨ˆé‡ãƒ»å‘¨æ³¢æ•°ç‰¹æ€§
#    - ã‚¯ã‚©ãƒ¼ã‚¿ãƒ‹ã‚ªãƒ³: rot_w, rot_x, rot_y, rot_z ã‹ã‚‰å§¿å‹¢æ¨å®š
#    - ä¸–ç•Œåº§æ¨™ç³»åŠ é€Ÿåº¦: ãƒ‡ãƒã‚¤ã‚¹åº§æ¨™ã‹ã‚‰ä¸–ç•Œåº§æ¨™ã¸ã®å¤‰æ›
#    - ç·šå½¢åŠ é€Ÿåº¦: é‡åŠ›æˆåˆ†ã‚’é™¤å»ã—ãŸç´”ç²‹ãªå‹•ä½œåŠ é€Ÿåº¦
#    - è§’é€Ÿåº¦: å›è»¢ã®é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«
#    - ã‚ªã‚¤ãƒ©ãƒ¼è§’: ãƒ­ãƒ¼ãƒ«ãƒ»ãƒ”ãƒƒãƒãƒ»ãƒ¨ãƒ¼ã®ç›´æ„Ÿçš„ãªå›è»¢è¡¨ç¾
#    - ã‚¸ãƒ£ãƒ¼ã‚¯: åŠ é€Ÿåº¦ã®æ™‚é–“å¾®åˆ†ï¼ˆå‹•ãã®æ»‘ã‚‰ã‹ã•ï¼‰
#
# â‘¡ ToFç‰¹å¾´é‡ï¼ˆTime-of-Flightè·é›¢ã‚»ãƒ³ã‚µãƒ¼ï¼‰
#    - ç©ºé–“ç‰¹å¾´: 8Ã—8ç”»åƒã®é‡å¿ƒã€ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã€åˆ†æ•£
#    - é ˜åŸŸåˆ†æ: ä¸­å¿ƒ3Ã—3ã€å†…å´ãƒªãƒ³ã‚°ã€å¤–å´ãƒªãƒ³ã‚°ã®çµ±è¨ˆ
#    - è¿‘æ¥æ¤œå‡º: è·é›¢åˆ†ä½æ•°ã«ã‚ˆã‚‹ç‰©ä½“æ¥è¿‘ã®æ¤œå‡º
#    - ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°: è¿‘è·é›¢é ˜åŸŸã®é€£çµæˆåˆ†åˆ†æ
#    - PCAæ¬¡å…ƒå‰Šæ¸›: 64æ¬¡å…ƒã‚’ä¸»æˆåˆ†ã«åœ§ç¸®ï¼ˆfoldå†…ã§fitï¼‰
#    - æœ€å°è·é›¢è¿½è·¡: å„ãƒ•ãƒ¬ãƒ¼ãƒ ã®æœ€å°è·é›¢æ™‚ç³»åˆ—
#    - ç•°æ–¹æ€§: æ–¹å‘ä¾å­˜æ€§ã®æ¸¬å®š
#
# â‘¢ ã‚µãƒ¼ãƒãƒ«ç‰¹å¾´é‡ï¼ˆæ¸©åº¦ã‚»ãƒ³ã‚µãƒ¼ï¼‰
#    - æ¸©åº¦å¤‰åŒ–ç‡: 1æ¬¡å¾®åˆ†ã«ã‚ˆã‚‹å¤‰åŒ–é€Ÿåº¦
#    - ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ: ç·šå½¢å›å¸°ã«ã‚ˆã‚‹å…¨ä½“å‚¾å‘
#    - 2æ¬¡å¾®åˆ†: æ¸©åº¦å¤‰åŒ–ã®åŠ é€Ÿåº¦
#
# â‘£ çµ±è¨ˆçš„ç‰¹å¾´é‡
#    - åŸºæœ¬çµ±è¨ˆ: å¹³å‡ã€æ¨™æº–åå·®ã€æœ€å°/æœ€å¤§ã€ä¸­å¤®å€¤ã€å››åˆ†ä½æ•°
#    - å½¢çŠ¶ãƒ¡ãƒˆãƒªã‚¯ã‚¹: æ­ªåº¦ã€å°–åº¦ã€å¤‰å‹•ä¿‚æ•°
#    - ã‚»ã‚°ãƒ¡ãƒ³ãƒˆç‰¹å¾´: æ™‚ç³»åˆ—ã‚’3åˆ†å‰²ã—ãŸå„éƒ¨ã®çµ±è¨ˆé‡
#    - Hjorthãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: æ´»å‹•åº¦ã€ç§»å‹•åº¦ã€è¤‡é›‘åº¦ï¼ˆè„³æ³¢è§£æç”±æ¥ï¼‰
#    - ãƒ”ãƒ¼ã‚¯æ¤œå‡º: ãƒ”ãƒ¼ã‚¯æ•°ã€é«˜ã•ã€é–“éš”
#    - ãƒ©ã‚¤ãƒ³é•·: ä¿¡å·ã®ç·å¤‰å‹•é‡
#
# â‘¤ å‘¨æ³¢æ•°é ˜åŸŸç‰¹å¾´é‡
#    - PSD: Welchæ³•ã«ã‚ˆã‚‹ãƒ‘ãƒ¯ãƒ¼ã‚¹ãƒšã‚¯ãƒˆãƒ«å¯†åº¦ï¼ˆå‹•çš„npersegå¯¾å¿œï¼‰
#    - ãƒãƒ³ãƒ‰ãƒ‘ãƒ¯ãƒ¼: 0.3-3Hzã€3-8Hzã€8-12Hzã®çµ¶å¯¾å€¤ãƒ»ç›¸å¯¾å€¤
#    - ã‚¹ãƒšã‚¯ãƒˆãƒ«ç‰¹æ€§: é‡å¿ƒã€85%ãƒ­ãƒ¼ãƒ«ã‚ªãƒ•ã€ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
#    - æ”¯é…çš„å‘¨æ³¢æ•°: æœ€å¤§ãƒ‘ãƒ¯ãƒ¼ã‚’æŒã¤å‘¨æ³¢æ•°
#    - ã‚¼ãƒ­äº¤å·®ç‡: ä¿¡å·ã®æŒ¯å‹•é »åº¦
#
# â‘¥ ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«ç‰¹å¾´é‡
#    - ToFã‚»ãƒ³ã‚µãƒ¼é–“åŒæœŸ: è¤‡æ•°ã‚»ãƒ³ã‚µãƒ¼ã®ç›¸é–¢ãƒ»æ™‚é–“é…ã‚Œ
#    - IMU-ToFç›¸é–¢: åŠ é€Ÿåº¦ã¨ToFè·é›¢ã®é–¢ä¿‚æ€§
#    - ãƒ”ãƒ¼ã‚¯æ•´åˆ: åŠ é€Ÿåº¦ãƒ”ãƒ¼ã‚¯æ™‚ã®ToFå€¤
#
# â‘¦ ãƒãƒ«ãƒè§£åƒåº¦ç‰¹å¾´é‡
#    - æ™‚é–“çª“: micro(5)ã€short(20)ã€medium(50)ã‚µãƒ³ãƒ—ãƒ«
#    - ç§»å‹•çµ±è¨ˆ: å„çª“ã§ã®å¹³å‡ãƒ»æ¨™æº–åå·®ã®æ™‚ç³»åˆ—
#
# ğŸ” Qualityç‰¹å¾´é‡ï¼ˆãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰
#    - é€£ç¶šæ¬ æ¸¬é•·: å„ã‚»ãƒ³ã‚µãƒ¼ã®æœ€å¤§é€£ç¶šNaNé•·
#    - æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿æ¯”ç‡: ã‚»ãƒ³ã‚µãƒ¼åˆ¥ã®æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ç‡
#    - ToFå“è³ª: valid_ratioã®åˆ†ä½çµ±è¨ˆï¼ˆp5/p50/p95ï¼‰
#
# â‘¦ ãƒãƒ«ãƒè§£åƒåº¦ç‰¹å¾´é‡
#    - ç•°ãªã‚‹æ™‚é–“çª“ï¼ˆS: 1-1.5ç§’ã€M: 3-4ç§’ã€L: 10-12ç§’ï¼‰
#    - ãƒ†ãƒ³ãƒãƒ©ãƒ«ãƒ”ãƒ©ãƒŸãƒƒãƒ‰
#
# ====================================================================================================

import json
import os
import pickle
import warnings
from datetime import datetime
from pathlib import Path
from typing import List, Tuple

import numpy as np
import pandas as pd
import polars as pl
import xgboost as xgb
from scipy import stats
from scipy.signal import find_peaks, welch
from scipy.spatial.transform import Rotation as R
from sklearn.decomposition import PCA
from sklearn.metrics import f1_score
from sklearn.model_selection import StratifiedGroupKFold
from sklearn.preprocessing import RobustScaler, StandardScaler

warnings.filterwarnings("ignore")

# ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã§ã®ã¿ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¡¨ç¤ºï¼ˆãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ãƒ¯ãƒ¼ã‚«ãƒ¼ã§ã¯è¡¨ç¤ºã—ãªã„ï¼‰
if __name__ == "__main__":
    print("=" * 70)
    print("CMI-BFRBæ¤œå‡º - é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° v2.0")
    print("åŒ…æ‹¬çš„ã‚»ãƒ³ã‚µãƒ¼èåˆç‰¹å¾´é‡ã‚’ç”¨ã„ãŸXGBoost")
    print("ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ/ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ©Ÿèƒ½ã§é«˜é€Ÿåå¾©é–‹ç™ºãŒå¯èƒ½")
    print("=" * 70)

# ====================================================================================================
# ç’°å¢ƒè¨­å®š
# ====================================================================================================

# ğŸ”§ ãƒ¡ã‚¤ãƒ³ç’°å¢ƒã‚¹ã‚¤ãƒƒãƒ - Kaggleã¨ãƒ­ãƒ¼ã‚«ãƒ«Macã®åˆ‡ã‚Šæ›¿ãˆ
IS_KAGGLE_ENV = True  # True: Kaggleç’°å¢ƒã€False: ãƒ­ãƒ¼ã‚«ãƒ«MacBook

# âš™ï¸ ç‰¹å¾´é‡æŠ½å‡ºè¨­å®š
# å‹•ä½œã‚’åˆ¶å¾¡ã™ã‚‹å¤‰æ•°:
USE_EXPORTED_FEATURES = (
    True  # True: ç‰¹å¾´é‡æŠ½å‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
)
EXPORT_FEATURES = False  # True: ç‰¹å¾´é‡ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆåˆå›å®Ÿè¡Œæ™‚ï¼‰
EXPORT_NAME = None  # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆåï¼ˆNone = ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è‡ªå‹•ç”Ÿæˆï¼‰

# ğŸš€ ä¸¦åˆ—å‡¦ç†è¨­å®šï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã®ã¿ï¼‰
USE_PARALLEL = True  # True: ä¸¦åˆ—å‡¦ç†ã‚’ä½¿ç”¨ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã®ã¿æœ‰åŠ¹ï¼‰
N_JOBS = -1  # ä¸¦åˆ—å‡¦ç†ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•° (-1: å…¨ã‚³ã‚¢ä½¿ç”¨, æ­£ã®æ•´æ•°: æŒ‡å®šæ•°ã®ã‚³ã‚¢ä½¿ç”¨)

# ğŸ”§ å­¦ç¿’æ¸ˆãƒ¢ãƒ‡ãƒ«è¨­å®š
USE_PRETRAINED_MODEL = False  # True: å­¦ç¿’æ¸ˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã€False: æ–°è¦ã«å­¦ç¿’
PRETRAINED_MODEL_PATH = None  # å­¦ç¿’æ¸ˆãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆNone = è‡ªå‹•æ¤œå‡ºï¼‰
PRETRAINED_EXTRACTOR_PATH = None  # å­¦ç¿’æ¸ˆExtractorãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆNone = è‡ªå‹•æ¤œå‡ºï¼‰
PRETRAINED_ARTIFACTS_PATH = None  # fold artifactsãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆNone = è‡ªå‹•æ¤œå‡ºï¼‰
EXPORT_TRAINED_MODEL = True  # True: å­¦ç¿’å¾Œã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

# ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆè¨­å®š
USE_CHECKPOINT = False  # True: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹ã€False: æœ€åˆã‹ã‚‰å­¦ç¿’
CHECKPOINT_DIR = "checkpoints"  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
CHECKPOINT_INTERVAL = 1  # ä½•foldæ¯ã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹ã‹ï¼ˆ1=æ¯foldï¼‰
AUTO_REMOVE_CHECKPOINT = True  # å­¦ç¿’å®Œäº†æ™‚ã«è‡ªå‹•çš„ã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å‰Šé™¤

# ğŸ”§ foldæ¯ã®ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆï¼ˆã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ãªã©ï¼‰
FOLD_ARTIFACTS = None  # foldæ¯ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿

# ç’°å¢ƒã«å¿œã˜ã¦ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ¸ˆã¿ç‰¹å¾´é‡ã®ãƒ‘ã‚¹ã‚’è‡ªå‹•è¨­å®š
if IS_KAGGLE_ENV:
    # Kaggleãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹
    EXPORTED_FEATURES_PATH = "/kaggle/input/cmi-bfrb-detection-exported-feature-data/features_v1.1.0_20250813_184410"
else:
    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹
    EXPORTED_FEATURES_PATH = "exported_features/features_v1.1.0_20250813_184410"

# ä½¿ç”¨ä¾‹:
# Kaggleç’°å¢ƒ: IS_KAGGLE_ENV = True ã«è¨­å®šã™ã‚‹ã ã‘
# ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ: IS_KAGGLE_ENV = False ã«è¨­å®šã™ã‚‹ã ã‘
# ãƒ‘ã‚¹ã¯è‡ªå‹•çš„ã«è¨­å®šã•ã‚Œã¾ã™ï¼

# ç’°å¢ƒã«å¿œã˜ãŸãƒ‘ã‚¹è¨­å®š
if IS_KAGGLE_ENV:
    # Kaggleã®ãƒ‘ã‚¹
    EXPORT_DIR = Path("./exported_features")
    DATA_BASE_PATH = Path("/kaggle/input/cmi-detect-behavior-with-sensor-data")
else:
    # ãƒ­ãƒ¼ã‚«ãƒ«MacBookã®ãƒ‘ã‚¹
    EXPORT_DIR = Path("exported_features")
    DATA_BASE_PATH = Path("cmi-detect-behavior-with-sensor-data")

EXPORT_DIR.mkdir(exist_ok=True, parents=True)
FEATURE_VERSION = "v1.1.0"

# ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã§ã®ã¿è¨­å®šã‚’è¡¨ç¤º
if __name__ == "__main__":
    print(f"ğŸŒ Environment: {'KAGGLE' if IS_KAGGLE_ENV else 'LOCAL (MacBook)'}")
    print(f"ğŸ“ Export directory: {EXPORT_DIR}")
    print(f"ğŸ“Š Data directory: {DATA_BASE_PATH}")
    print(
        f"âš¡ Parallel processing: {'DISABLED (Kaggle)' if IS_KAGGLE_ENV else 'ENABLED (Local)'}"
    )
    print(
        f"ğŸ® XGBoost GPU: {'ENABLED (CUDA/T4)' if IS_KAGGLE_ENV else 'DISABLED (CPU only)'}"
    )
    print(
        f"ğŸ¤– Model mode: {'LOAD PRETRAINED' if USE_PRETRAINED_MODEL else 'TRAIN NEW'}"
    )
    if USE_PRETRAINED_MODEL:
        print(f"   Model path: {PRETRAINED_MODEL_PATH or 'Auto-detect'}")
        print(f"   Extractor path: {PRETRAINED_EXTRACTOR_PATH or 'Auto-detect'}")
        print(f"   Artifacts path: {PRETRAINED_ARTIFACTS_PATH or 'Auto-detect'}")
    if USE_CHECKPOINT:
        print("ğŸ’¾ Checkpoint: ENABLED")
        print(f"   Directory: {CHECKPOINT_DIR}")
        print(f"   Save interval: Every {CHECKPOINT_INTERVAL} fold(s)")
        print(f"   Auto-remove: {'Yes' if AUTO_REMOVE_CHECKPOINT else 'No'}")

# ====================================================================================================
# è¨­å®š
# ====================================================================================================

# ç’°å¢ƒã«å¿œã˜ãŸãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ã®è¨­å®š
if IS_KAGGLE_ENV:
    # Kaggle paths
    DATA_PATHS = {
        "train_path": str(DATA_BASE_PATH / "train.csv"),
        "train_demographics_path": str(DATA_BASE_PATH / "train_demographics.csv"),
        "test_path": str(DATA_BASE_PATH / "test.csv"),
        "test_demographics_path": str(DATA_BASE_PATH / "test_demographics.csv"),
    }
else:
    # Local MacBook paths
    DATA_PATHS = {
        "train_path": str(DATA_BASE_PATH / "train.csv"),
        "train_demographics_path": str(DATA_BASE_PATH / "train_demographics.csv"),
        "test_path": str(DATA_BASE_PATH / "test.csv"),
        "test_demographics_path": str(DATA_BASE_PATH / "test_demographics.csv"),
    }

    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ç¢ºèª
    if not DATA_BASE_PATH.exists():
        print(f"âš ï¸ è­¦å‘Š: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {DATA_BASE_PATH}")
        print("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ã„å ´æ‰€ã«ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

CONFIG = {
    # ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹
    **DATA_PATHS,
    # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°è¨­å®š
    "sampling_rate": 20,  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆï¼ˆHzï¼‰
    "gravity": 9.81,  # é‡åŠ›åŠ é€Ÿåº¦ï¼ˆm/s^2ï¼‰
    "use_world_acc": True,  # ä¸–ç•Œåº§æ¨™ç³»ã§ã®åŠ é€Ÿåº¦ã‚’ä½¿ç”¨
    "use_linear_acc": True,  # ç·šå½¢åŠ é€Ÿåº¦ï¼ˆé‡åŠ›é™¤å»ï¼‰ã‚’ä½¿ç”¨
    "use_angular_velocity": True,  # è§’é€Ÿåº¦ã‚’ä½¿ç”¨
    "use_frequency_features": True,  # å‘¨æ³¢æ•°é ˜åŸŸç‰¹å¾´é‡ã‚’ä½¿ç”¨
    "use_tof_spatial": True,  # ToFç©ºé–“ç‰¹å¾´é‡ã‚’ä½¿ç”¨
    "use_thermal_trends": True,  # æ¸©åº¦ãƒˆãƒ¬ãƒ³ãƒ‰ç‰¹å¾´é‡ã‚’ä½¿ç”¨
    "use_cross_modal": True,  # ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«ç‰¹å¾´é‡ã‚’ä½¿ç”¨
    # ãƒãƒ«ãƒè§£åƒåº¦ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆS/M/Lï¼‰
    "use_multi_resolution": True,  # ãƒãƒ«ãƒè§£åƒåº¦ç‰¹å¾´é‡ã‚’ä½¿ç”¨
    "window_sizes": {
        "S": (20, 30),  # 1.0-1.5 seconds
        "M": (60, 80),  # 3-4 seconds
        "L": (200, 256),  # 10-12.8 seconds
    },
    "use_tail_emphasis": True,  # Emphasize tail windows for TTA
    # ToF processing
    "tof_pca_components": 8,
    "tof_valid_threshold": 0.2,
    "tof_outlier_percentile": (1, 99),
    "tof_use_pca": True,
    "tof_use_handedness_mirror": True,  # Mirror ToF based on handedness
    "tof_region_analysis": True,  # Analyze different spatial regions
    # Frequency analysis
    "welch_nperseg": 128,
    "welch_noverlap": 64,
    "freq_bands": [(0.3, 3), (3, 8), (8, 12)],  # Hz
    # Normalization
    "sequence_normalize": True,
    "robust_scaler": True,
    # ğŸ”§ T1: NaNä¿æŒ & ã‚¹ã‚±ãƒ¼ãƒ©åœæ­¢
    "preserve_nan_for_missing": True,  # æ¬ æã‚’NaNã®ã¾ã¾ä¿æŒï¼ˆXGBoostã®missingåˆ†å²ã‚’æ´»ç”¨ï¼‰
    "use_scaler_for_xgb": False,  # XGBoostæ™‚ã¯ã‚¹ã‚±ãƒ¼ãƒ©ç„¡åŠ¹ï¼ˆæ¨¹æœ¨ç³»ã¯ã‚¹ã‚±ãƒ¼ãƒ«ä¸è¦ï¼‰
    # ğŸ”§ T5: éIMUã®è¨ˆç®—ã‚¹ã‚­ãƒƒãƒ—ï¼ˆä½å“è³ªæ™‚ï¼‰
    "quality_thresholds": {"tof": 0.05, "thm": 0.05},  # å“è³ªé—¾å€¤
    # ğŸ”§ T4: ã‚¹ãƒãƒ¼ãƒˆçª“
    "smart_windowing": True,  # ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å¤§çª“ã‚’ä½¿ç”¨
    "topk_windows": 1,  # Top-k windows to use
    # ğŸ”§ T7: ãƒ¢ãƒ€ãƒªãƒ†ã‚£ãƒ»ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
    "modality_dropout_prob": 0.4,  # å­¦ç¿’æ™‚ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç¢ºç‡
    # Model
    "n_folds": 5,
    "random_state": 42,
    "xgb_params": {
        "objective": "multi:softprob",
        "num_class": 18,
        "n_estimators": 1000,
        "max_depth": 10,
        "learning_rate": 0.03,
        "subsample": 0.8,
        "colsample_bytree": 0.8,
        "gamma": 0.1,
        "reg_alpha": 0.1,
        "reg_lambda": 1.0,
        "min_child_weight": 3,
        "random_state": 42,
        "n_jobs": -1,
        "early_stopping_rounds": 50,
    },
}

# ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆ18ã‚¯ãƒ©ã‚¹ï¼‰
GESTURE_MAPPER = {
    "Above ear - pull hair": 0,
    "Cheek - pinch skin": 1,
    "Eyebrow - pull hair": 2,
    "Eyelash - pull hair": 3,
    "Forehead - pull hairline": 4,
    "Forehead - scratch": 5,
    "Neck - pinch skin": 6,
    "Neck - scratch": 7,
    "Drink from bottle/cup": 8,
    "Feel around in tray and pull out an object": 9,
    "Glasses on/off": 10,
    "Pinch knee/leg skin": 11,
    "Pull air toward your face": 12,
    "Scratch knee/leg skin": 13,
    "Text on phone": 14,
    "Wave hello": 15,
    "Write name in air": 16,
    "Write name on leg": 17,
}

REVERSE_GESTURE_MAPPER = {v: k for k, v in GESTURE_MAPPER.items()}

# ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã§ã®ã¿è¨­å®šã‚’è¡¨ç¤º
if __name__ == "__main__":
    print(f"âœ“ Configuration loaded ({len(GESTURE_MAPPER)} gesture classes)")
    print(
        f"ğŸ“ Data paths configured for {'Kaggle' if IS_KAGGLE_ENV else 'Local'} environment"
    )

# ====================================================================================================
# QUATERNION AND IMU PROCESSING
# ====================================================================================================


def handle_quaternion_missing(rot_data: np.ndarray) -> np.ndarray:
    """ã‚¯ã‚©ãƒ¼ã‚¿ãƒ‹ã‚ªãƒ³ãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤ã‚’é©åˆ‡ãªæ­£è¦åŒ–ã§å‡¦ç†ã™ã‚‹ã€‚"""
    rot_cleaned = rot_data.copy()

    # Fill NaN values
    for col in range(4):
        mask = np.isnan(rot_cleaned[:, col])
        if mask.any():
            # Forward fill, then backward fill, then use default
            rot_cleaned[:, col] = (
                pd.Series(rot_cleaned[:, col])
                .fillna(method="ffill")
                .fillna(method="bfill")
                .fillna(1.0 if col == 0 else 0.0)
                .values
            )

    # Normalize quaternions
    norms = np.linalg.norm(rot_cleaned, axis=1, keepdims=True)
    norms[norms < 1e-8] = 1.0
    rot_cleaned = rot_cleaned / norms

    return rot_cleaned


def compute_world_acceleration(acc: np.ndarray, rot: np.ndarray) -> np.ndarray:
    """Convert acceleration from device to world coordinates."""
    try:
        # ã‚¯ã‚©ãƒ¼ã‚¿ãƒ‹ã‚ªãƒ³å½¢å¼(w,x,y,z)ã‚’scipyå½¢å¼(x,y,z,w)ã«å¤‰æ›
        rot_scipy = rot[:, [1, 2, 3, 0]]
        r = R.from_quat(rot_scipy)
        acc_world = r.apply(acc)
    except Exception:
        acc_world = acc.copy()
    return acc_world


def robust_normalize(x: np.ndarray) -> np.ndarray:
    """
    ğŸ”§ T3: ãƒ­ãƒã‚¹ãƒˆæ­£è¦åŒ–ï¼ˆä¸­å¤®å€¤/IQRï¼‰
    å¤–ã‚Œå€¤ã«é ‘å¥ãªæ­£è¦åŒ–ã‚’è¡Œã†ã€‚
    """
    med = np.nanmedian(x)
    iqr = np.nanpercentile(x, 75) - np.nanpercentile(x, 25)
    return (x - med) / (iqr + 1e-8)


def compute_linear_acceleration(
    acc: np.ndarray, rot: np.ndarray = None, method: str = "subtract"
) -> np.ndarray:
    """
    Remove gravity from acceleration to get linear acceleration.
    ğŸ”§ T3: ã‚¯ã‚©ãƒ¼ã‚¿ãƒ‹ã‚ªãƒ³ãªã—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¿½åŠ 
    """
    if method == "subtract" and rot is not None:
        # Method A: Subtract gravity in world coordinates
        acc_world = compute_world_acceleration(acc, rot)
        gravity_world = np.array([0, 0, CONFIG["gravity"]])
        linear_acc = acc_world - gravity_world
    else:
        # Method B: High-pass filter
        from scipy.signal import butter, filtfilt

        b, a = butter(4, 2.0, btype="high", fs=CONFIG["sampling_rate"])
        linear_acc = np.zeros_like(acc)
        for i in range(3):
            linear_acc[:, i] = filtfilt(b, a, acc[:, i])

    return linear_acc


def compute_angular_velocity(rot: np.ndarray, dt: float = None) -> np.ndarray:
    """Compute angular velocity from quaternion sequence."""
    # dtãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯CONFIGã‹ã‚‰å–å¾—
    if dt is None:
        dt = 1.0 / CONFIG.get("sampling_rate", 20)

    omega = np.zeros((len(rot) - 1, 3))

    for i in range(len(rot) - 1):
        q1 = rot[i, [1, 2, 3, 0]]  # scipyå½¢å¼ã«å¤‰æ›
        q2 = rot[i + 1, [1, 2, 3, 0]]

        try:
            r1 = R.from_quat(q1)
            r2 = R.from_quat(q2)
            r_diff = r2 * r1.inv()
            omega[i] = r_diff.as_rotvec() / dt
        except:
            omega[i] = 0

    # Pad to match original length
    omega = np.vstack([omega, omega[-1:]])
    return omega


def quaternion_to_euler(rot: np.ndarray) -> np.ndarray:
    """Convert quaternion to Euler angles (roll, pitch, yaw)."""
    rot_scipy = rot[:, [1, 2, 3, 0]]
    try:
        r = R.from_quat(rot_scipy)
        euler = r.as_euler("xyz")
    except:
        euler = np.zeros((len(rot), 3))
    return euler


# ====================================================================================================
# STATISTICAL FEATURES
# ====================================================================================================


def extract_statistical_features(data: np.ndarray, prefix: str) -> dict:
    """
    1æ¬¡å…ƒæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åŒ…æ‹¬çš„ãªçµ±è¨ˆçš„ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹ã€‚

    â‘£ çµ±è¨ˆçš„ç‰¹å¾´é‡ï¼š
    - åŸºæœ¬çµ±è¨ˆé‡ï¼ˆå¹³å‡ã€æ¨™æº–åå·®ã€æœ€å°å€¤ã€æœ€å¤§å€¤ã€ä¸­å¤®å€¤ã€å››åˆ†ä½æ•°ï¼‰
    - å½¢çŠ¶ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆæ­ªåº¦ã€å°–åº¦ã€å¤‰å‹•ä¿‚æ•°ï¼‰
    - å¢ƒç•Œç‰¹å¾´é‡ï¼ˆæœ€åˆã®å€¤ã€æœ€å¾Œã®å€¤ã€å¤‰åŒ–é‡ï¼‰
    - å·®åˆ†ç‰¹å¾´é‡ï¼ˆå·®åˆ†ã®å¹³å‡ã€æ¨™æº–åå·®ã€å¤‰åŒ–ç‚¹æ•°ï¼‰
    - ã‚»ã‚°ãƒ¡ãƒ³ãƒˆç‰¹å¾´é‡ï¼ˆ3åˆ†å‰²ã—ãŸå„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®çµ±è¨ˆé‡ã¨é·ç§»ï¼‰
    """
    features = {}

    # Pandas Seriesã‚’NumPyé…åˆ—ã«å¤‰æ›
    if hasattr(data, "values"):
        data = data.values

    if len(data) == 0 or np.all(np.isnan(data)):
        # ç©ºã®ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯ã‚¼ãƒ­ã‚’è¿”ã™
        return {
            f"{prefix}_{k}": 0
            for k in [
                "mean",
                "std",
                "min",
                "max",
                "median",
                "q25",
                "q75",
                "iqr",
                "range",
                "cv",
                "skew",
                "kurt",
                "first",
                "last",
                "delta",
                "diff_mean",
                "diff_std",
                "n_changes",
            ]
        }

    # ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    data = data[~np.isnan(data)]
    if len(data) == 0:
        return {
            f"{prefix}_{k}": 0
            for k in [
                "mean",
                "std",
                "min",
                "max",
                "median",
                "q25",
                "q75",
                "iqr",
                "range",
                "cv",
                "skew",
                "kurt",
                "first",
                "last",
                "delta",
                "diff_mean",
                "diff_std",
                "n_changes",
            ]
        }

    # åŸºæœ¬çµ±è¨ˆé‡
    features[f"{prefix}_mean"] = np.mean(data)
    features[f"{prefix}_std"] = np.std(data)
    features[f"{prefix}_min"] = np.min(data)
    features[f"{prefix}_max"] = np.max(data)
    features[f"{prefix}_median"] = np.median(data)
    features[f"{prefix}_q25"] = np.percentile(data, 25)
    features[f"{prefix}_q75"] = np.percentile(data, 75)
    features[f"{prefix}_iqr"] = features[f"{prefix}_q75"] - features[f"{prefix}_q25"]
    features[f"{prefix}_range"] = features[f"{prefix}_max"] - features[f"{prefix}_min"]
    features[f"{prefix}_cv"] = features[f"{prefix}_std"] / (
        abs(features[f"{prefix}_mean"]) + 1e-8
    )

    # å½¢çŠ¶ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    if len(data) > 1:
        features[f"{prefix}_skew"] = stats.skew(data)
        features[f"{prefix}_kurt"] = stats.kurtosis(data)
    else:
        features[f"{prefix}_skew"] = 0
        features[f"{prefix}_kurt"] = 0

    # å¢ƒç•Œç‰¹å¾´é‡
    features[f"{prefix}_first"] = data[0]
    features[f"{prefix}_last"] = data[-1]
    features[f"{prefix}_delta"] = data[-1] - data[0]

    # å·®åˆ†ç‰¹å¾´é‡
    if len(data) > 1:
        diff_data = np.diff(data)
        features[f"{prefix}_diff_mean"] = np.mean(diff_data)
        features[f"{prefix}_diff_std"] = np.std(diff_data)
        features[f"{prefix}_n_changes"] = np.sum(np.abs(diff_data) > np.std(data) * 0.1)
    else:
        features[f"{prefix}_diff_mean"] = 0
        features[f"{prefix}_diff_std"] = 0
        features[f"{prefix}_n_changes"] = 0

    # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆç‰¹å¾´é‡ï¼ˆ3åˆ†å‰²ï¼‰
    seq_len = len(data)
    if seq_len >= 9:
        seg_size = seq_len // 3
        for i in range(3):
            start_idx = i * seg_size
            end_idx = (i + 1) * seg_size if i < 2 else seq_len
            segment = data[start_idx:end_idx]
            features[f"{prefix}_seg{i + 1}_mean"] = np.mean(segment)
            features[f"{prefix}_seg{i + 1}_std"] = np.std(segment)

        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé–“ã®é·ç§»
        features[f"{prefix}_seg1_to_seg2"] = (
            features[f"{prefix}_seg2_mean"] - features[f"{prefix}_seg1_mean"]
        )
        features[f"{prefix}_seg2_to_seg3"] = (
            features[f"{prefix}_seg3_mean"] - features[f"{prefix}_seg2_mean"]
        )
    else:
        for i in range(3):
            features[f"{prefix}_seg{i + 1}_mean"] = features[f"{prefix}_mean"]
            features[f"{prefix}_seg{i + 1}_std"] = features[f"{prefix}_std"]
        features[f"{prefix}_seg1_to_seg2"] = 0
        features[f"{prefix}_seg2_to_seg3"] = 0

    return features


def extract_hjorth_parameters(data: np.ndarray, prefix: str) -> dict:
    """
    Hjorthãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæ´»å‹•åº¦ã€ç§»å‹•åº¦ã€è¤‡é›‘åº¦ï¼‰ã‚’æŠ½å‡ºã™ã‚‹ã€‚

    Hjorthãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯è„³æ³¢è§£æã§ä½¿ç”¨ã•ã‚Œã‚‹æ™‚ç³»åˆ—ç‰¹å¾´é‡ï¼š
    - æ´»å‹•åº¦(Activity): ä¿¡å·ã®åˆ†æ•£ï¼ˆãƒ‘ãƒ¯ãƒ¼ã®æŒ‡æ¨™ï¼‰
    - ç§»å‹•åº¦(Mobility): å‘¨æ³¢æ•°ã®æ¨™æº–åå·®ã®æ¨å®šå€¤
    - è¤‡é›‘åº¦(Complexity): å‘¨æ³¢æ•°å¤‰åŒ–ã®æŒ‡æ¨™
    """
    features = {}

    if len(data) < 2:
        features[f"{prefix}_hjorth_activity"] = 0
        features[f"{prefix}_hjorth_mobility"] = 0
        features[f"{prefix}_hjorth_complexity"] = 0
        return features

    # æ´»å‹•åº¦ï¼šä¿¡å·ã®åˆ†æ•£
    activity = np.var(data)
    features[f"{prefix}_hjorth_activity"] = activity

    # ç§»å‹•åº¦ï¼šsqrt(ä¸€æ¬¡å¾®åˆ†ã®åˆ†æ•£ / ä¿¡å·ã®åˆ†æ•£)
    diff1 = np.diff(data)
    if activity > 0:
        mobility = np.sqrt(np.var(diff1) / activity)
    else:
        mobility = 0
    features[f"{prefix}_hjorth_mobility"] = mobility

    # è¤‡é›‘åº¦ï¼šä¸€æ¬¡å¾®åˆ†ã®ç§»å‹•åº¦ / ä¿¡å·ã®ç§»å‹•åº¦
    if len(diff1) > 1 and mobility > 0:
        diff2 = np.diff(diff1)
        mobility2 = np.sqrt(np.var(diff2) / np.var(diff1)) if np.var(diff1) > 0 else 0
        complexity = mobility2 / mobility
    else:
        complexity = 0
    features[f"{prefix}_hjorth_complexity"] = complexity

    return features


def extract_peak_features(data: np.ndarray, prefix: str) -> dict:
    """
    ãƒ”ãƒ¼ã‚¯é–¢é€£ã®ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹ã€‚

    ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã«ã‚ˆã‚‹ç‰¹å¾´é‡ï¼š
    - ãƒ”ãƒ¼ã‚¯æ•°
    - ãƒ”ãƒ¼ã‚¯ã®å¹³å‡é«˜ã•
    - ãƒ”ãƒ¼ã‚¯é–“ã®å¹³å‡è·é›¢
    """
    features = {}

    if len(data) < 3:
        features[f"{prefix}_n_peaks"] = 0
        features[f"{prefix}_peak_mean_height"] = 0
        features[f"{prefix}_peak_mean_distance"] = 0
        return features

    # ãƒ”ãƒ¼ã‚¯ã‚’æ¤œå‡ºï¼ˆæ¨™æº–åå·®ã®0.5å€ã‚’é–¾å€¤ã¨ã—ã¦ä½¿ç”¨ï¼‰
    peaks, properties = find_peaks(data, height=np.std(data) * 0.5)

    features[f"{prefix}_n_peaks"] = len(peaks)

    if len(peaks) > 0:
        features[f"{prefix}_peak_mean_height"] = np.mean(properties["peak_heights"])
        if len(peaks) > 1:
            features[f"{prefix}_peak_mean_distance"] = np.mean(np.diff(peaks))
        else:
            features[f"{prefix}_peak_mean_distance"] = 0
    else:
        features[f"{prefix}_peak_mean_height"] = 0
        features[f"{prefix}_peak_mean_distance"] = 0

    return features


def extract_line_length(data: np.ndarray, prefix: str) -> dict:
    """
    ãƒ©ã‚¤ãƒ³é•·ï¼ˆçµ¶å¯¾å·®åˆ†ã®åˆè¨ˆï¼‰ã‚’æŠ½å‡ºã™ã‚‹ã€‚

    ä¿¡å·ã®ç·å¤‰å‹•é‡ã‚’è¡¨ã™ç‰¹å¾´é‡ã€‚
    """
    features = {}

    if len(data) < 2:
        features[f"{prefix}_line_length"] = 0
        return features

    features[f"{prefix}_line_length"] = np.sum(np.abs(np.diff(data)))

    return features


def extract_autocorrelation(
    data: np.ndarray, prefix: str, lags: list = [1, 2, 4, 8]
) -> dict:
    """
    ç•°ãªã‚‹ãƒ©ã‚°ã§ã®è‡ªå·±ç›¸é–¢ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹ã€‚

    æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å‘¨æœŸæ€§ã‚„æŒç¶šæ€§ã‚’æ¤œå‡ºã™ã‚‹ç‰¹å¾´é‡ã€‚
    """
    features = {}

    if len(data) < max(lags) + 1:
        for lag in lags:
            features[f"{prefix}_autocorr_lag{lag}"] = 0
        return features

    # Normalize data
    data_norm = (data - np.mean(data)) / (np.std(data) + 1e-8)

    for lag in lags:
        if lag < len(data):
            features[f"{prefix}_autocorr_lag{lag}"] = np.corrcoef(
                data_norm[:-lag], data_norm[lag:]
            )[0, 1]
        else:
            features[f"{prefix}_autocorr_lag{lag}"] = 0

    return features


def extract_gradient_histogram(data: np.ndarray, prefix: str, n_bins: int = 10) -> dict:
    """
    å‹¾é…ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹ã€‚

    ä¿¡å·ã®å¤‰åŒ–ç‡ã®åˆ†å¸ƒã‚’è¡¨ç¾ã™ã‚‹ç‰¹å¾´é‡ã€‚
    """
    features = {}

    if len(data) < 2:
        for i in range(n_bins):
            features[f"{prefix}_grad_hist_bin{i}"] = 0
        return features

    # å‹¾é…ã‚’è¨ˆç®—
    gradients = np.diff(data)

    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’ä½œæˆ
    hist, _ = np.histogram(gradients, bins=n_bins)
    hist = hist / (len(gradients) + 1e-8)  # Normalize

    for i, val in enumerate(hist):
        features[f"{prefix}_grad_hist_bin{i}"] = val

    return features


def extract_jerk_features(
    acc_data: np.ndarray, prefix: str, dt: float = 1.0 / 20
) -> dict:
    """
    ã‚¸ãƒ£ãƒ¼ã‚¯ç‰¹å¾´é‡ï¼ˆåŠ é€Ÿåº¦ã®ä¸€æ¬¡å¾®åˆ†ï¼‰ã‚’æŠ½å‡ºã™ã‚‹ã€‚

    å‹•ãã®æ»‘ã‚‰ã‹ã•ã‚„çªç™ºçš„ãªå¤‰åŒ–ã‚’æ‰ãˆã‚‹ç‰¹å¾´é‡ã€‚
    """
    features = {}

    if len(acc_data) < 2:
        features[f"{prefix}_jerk_mean"] = 0
        features[f"{prefix}_jerk_std"] = 0
        features[f"{prefix}_jerk_max"] = 0
        features[f"{prefix}_jerk_p90"] = 0
        features[f"{prefix}_jerk_L2"] = 0
        return features

    # ã‚¸ãƒ£ãƒ¼ã‚¯ï¼ˆåŠ é€Ÿåº¦ã®å¾®åˆ†ï¼‰ã‚’è¨ˆç®—
    jerk = np.diff(acc_data) / dt

    features[f"{prefix}_jerk_mean"] = np.mean(np.abs(jerk))
    features[f"{prefix}_jerk_std"] = np.std(jerk)
    features[f"{prefix}_jerk_max"] = np.max(np.abs(jerk))
    features[f"{prefix}_jerk_p90"] = np.percentile(np.abs(jerk), 90)
    features[f"{prefix}_jerk_L2"] = np.sqrt(np.mean(jerk**2))  # L2 norm

    return features


# ====================================================================================================
# FREQUENCY DOMAIN FEATURES
# ====================================================================================================


def extract_frequency_features(data: np.ndarray, prefix: str, fs: float = 20.0) -> dict:
    """
    Welchæ³•ã‚’ä½¿ç”¨ã—ã¦å‘¨æ³¢æ•°é ˜åŸŸç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹ã€‚

    â‘¤ å‘¨æ³¢æ•°é ˜åŸŸç‰¹å¾´é‡ï¼š
    - ãƒ‘ãƒ¯ãƒ¼ã‚¹ãƒšã‚¯ãƒˆãƒ«å¯†åº¦ï¼ˆPSDï¼‰
    - å‘¨æ³¢æ•°å¸¯åŸŸãƒ‘ãƒ¯ãƒ¼ï¼ˆçµ¶å¯¾å€¤ã¨ç›¸å¯¾å€¤ï¼‰
    - ã‚¹ãƒšã‚¯ãƒˆãƒ«é‡å¿ƒ
    - ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ­ãƒ¼ãƒ«ã‚ªãƒ•
    - ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
    - æ”¯é…çš„å‘¨æ³¢æ•°
    - ã‚¼ãƒ­äº¤å·®ç‡

    æ”¹ä¿®ï¼šçŸ­ç³»åˆ—å¯¾å¿œã®ãŸã‚å‹•çš„ã«npersegã‚’èª¿æ•´ã€‚
    """
    features = {}

    # å‹•çš„ã«npersegã‚’æ±ºå®šï¼ˆæœ€å°32ã€æœ€å¤§128ã€ãƒ‡ãƒ¼ã‚¿é•·ä»¥ä¸‹ï¼‰
    min_nperseg = 32
    max_nperseg = CONFIG.get("welch_nperseg", 128)
    nperseg = min(max(min_nperseg, len(data) // 4), max_nperseg, len(data))
    noverlap = nperseg // 2

    if len(data) < min_nperseg:
        # ãƒ‡ãƒ¼ã‚¿ãŒçŸ­ã™ãã‚‹å ´åˆã¯ã‚¼ãƒ­ã‚’è¿”ã™
        for band_idx, _ in enumerate(CONFIG["freq_bands"]):
            features[f"{prefix}_band{band_idx}_power"] = 0
            features[f"{prefix}_band{band_idx}_power_rel"] = 0  # ç›¸å¯¾ãƒ‘ãƒ¯ãƒ¼
            features[f"{prefix}_band{band_idx}_power_log"] = 0  # å¯¾æ•°ãƒ‘ãƒ¯ãƒ¼
        features[f"{prefix}_spectral_centroid"] = 0
        features[f"{prefix}_spectral_rolloff"] = 0
        features[f"{prefix}_spectral_entropy"] = 0
        features[f"{prefix}_dominant_freq"] = 0
        features[f"{prefix}_dominant_power"] = 0
        features[f"{prefix}_zcr"] = 0
        features[f"{prefix}_power_total"] = 0
        return features

    # å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦Welchæ³•ã§PSDã‚’è¨ˆç®—
    try:
        freqs, psd = welch(data, fs=fs, nperseg=nperseg, noverlap=noverlap)
    except:
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã‚¼ãƒ­ã‚’è¿”ã™
        for band_idx, _ in enumerate(CONFIG["freq_bands"]):
            features[f"{prefix}_band{band_idx}_power"] = 0
            features[f"{prefix}_band{band_idx}_power_rel"] = 0
            features[f"{prefix}_band{band_idx}_power_log"] = 0
        features[f"{prefix}_spectral_centroid"] = 0
        features[f"{prefix}_spectral_rolloff"] = 0
        features[f"{prefix}_spectral_entropy"] = 0
        features[f"{prefix}_dominant_freq"] = 0
        features[f"{prefix}_dominant_power"] = 0
        features[f"{prefix}_zcr"] = 0
        features[f"{prefix}_power_total"] = 0
        return features

    # Total power
    total_power = np.sum(psd)
    features[f"{prefix}_power_total"] = total_power

    # Band power features (absolute and relative)
    band_powers = []
    for band_idx, (low, high) in enumerate(CONFIG["freq_bands"]):
        band_mask = (freqs >= low) & (freqs <= high)
        if np.any(band_mask):
            band_power = np.sum(psd[band_mask])
            band_powers.append(band_power)
            features[f"{prefix}_band{band_idx}_power"] = band_power

            # ç›¸å¯¾ãƒ‘ãƒ¯ãƒ¼ï¼ˆãƒãƒ³ãƒ‰ãƒ‘ãƒ¯ãƒ¼ / ç·ãƒ‘ãƒ¯ãƒ¼ï¼‰
            if total_power > 0:
                features[f"{prefix}_band{band_idx}_power_rel"] = (
                    band_power / total_power
                )
            else:
                features[f"{prefix}_band{band_idx}_power_rel"] = 0

            # å¯¾æ•°ãƒ‘ãƒ¯ãƒ¼ï¼ˆlog1på¤‰æ›ã§ã‚¹ã‚±ãƒ¼ãƒ«é ‘å¥æ€§ï¼‰
            features[f"{prefix}_band{band_idx}_power_log"] = np.log1p(band_power)
        else:
            band_powers.append(0)
            features[f"{prefix}_band{band_idx}_power"] = 0
            features[f"{prefix}_band{band_idx}_power_rel"] = 0
            features[f"{prefix}_band{band_idx}_power_log"] = 0

    # ãƒ‘ãƒ¯ãƒ¼æ¯”ï¼ˆä½å‘¨æ³¢/é«˜å‘¨æ³¢ãªã©ï¼‰
    if len(band_powers) >= 2 and band_powers[1] > 0:
        features[f"{prefix}_power_ratio_lf_hf"] = band_powers[0] / band_powers[1]
    else:
        features[f"{prefix}_power_ratio_lf_hf"] = 0

    # Spectral centroid
    if np.sum(psd) > 0:
        features[f"{prefix}_spectral_centroid"] = np.sum(freqs * psd) / np.sum(psd)
    else:
        features[f"{prefix}_spectral_centroid"] = 0

    # Spectral rolloff (85%)
    cumsum = np.cumsum(psd)
    if cumsum[-1] > 0:
        rolloff_idx = np.where(cumsum >= 0.85 * cumsum[-1])[0]
        if len(rolloff_idx) > 0:
            features[f"{prefix}_spectral_rolloff"] = freqs[rolloff_idx[0]]
        else:
            features[f"{prefix}_spectral_rolloff"] = freqs[-1]
    else:
        features[f"{prefix}_spectral_rolloff"] = 0

    # Spectral entropy
    psd_norm = psd / (np.sum(psd) + 1e-8)
    psd_norm = psd_norm[psd_norm > 0]
    if len(psd_norm) > 0:
        features[f"{prefix}_spectral_entropy"] = -np.sum(
            psd_norm * np.log(psd_norm + 1e-8)
        )
    else:
        features[f"{prefix}_spectral_entropy"] = 0

    # Dominant frequency
    if len(psd) > 0:
        dominant_idx = np.argmax(psd)
        features[f"{prefix}_dominant_freq"] = freqs[dominant_idx]
        features[f"{prefix}_dominant_power"] = psd[dominant_idx]
    else:
        features[f"{prefix}_dominant_freq"] = 0
        features[f"{prefix}_dominant_power"] = 0

    # Zero crossing rate
    zero_crossings = np.sum(np.diff(np.sign(data)) != 0)
    features[f"{prefix}_zcr"] = zero_crossings / len(data)

    return features


# ====================================================================================================
# QUALITY FEATURES
# ====================================================================================================


def extract_quality_features(
    sequence_df: pd.DataFrame, prefix: str = "quality"
) -> dict:
    """
    ãƒ‡ãƒ¼ã‚¿å“è³ªã«é–¢ã™ã‚‹ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹ã€‚

    Qualityç‰¹å¾´é‡ï¼š
    - é€£ç¶šæ¬ æ¸¬é•·ã®æœ€å¤§å€¤
    - æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿æ¯”ç‡
    - ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°æ¯”ç‡
    - ã‚»ãƒ³ã‚µãƒ¼åˆ¥ã®å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹
    """
    features = {}

    # å…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿å“è³ª
    total_rows = len(sequence_df)
    features[f"{prefix}_sequence_length"] = total_rows

    # ğŸ”§ T2: å“è³ªãƒ»å¯ç”¨æ€§ãƒ•ãƒ©ã‚°ï¼ˆãƒ¢ãƒ€ãƒªãƒ†ã‚£æœ‰ç„¡ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ˜ç¤ºï¼‰
    # IMUæœ‰ç„¡ã®åˆ¤å®š
    features[f"{prefix}_has_imu"] = int(
        all(c in sequence_df.columns for c in ["acc_x", "acc_y", "acc_z"])
    )

    # ã‚¯ã‚©ãƒ¼ã‚¿ãƒ‹ã‚ªãƒ³æœ‰ç„¡ã®åˆ¤å®š
    features[f"{prefix}_has_quat"] = int(len(detect_quat_cols(sequence_df)) > 0)

    # ToFæœ‰ç„¡ã®åˆ¤å®š
    features[f"{prefix}_has_tof"] = int(
        any(c.startswith("tof_") for c in sequence_df.columns)
    )

    # ã‚µãƒ¼ãƒãƒ«æœ‰ç„¡ã®åˆ¤å®š
    tp = detect_thermal_prefix(sequence_df)
    features[f"{prefix}_has_thermal"] = int(
        any(c.startswith(tp) for c in sequence_df.columns) if tp else 0
    )

    # IMUãƒ‡ãƒ¼ã‚¿ã®å“è³ª
    for axis in ["x", "y", "z"]:
        if f"acc_{axis}" in sequence_df.columns:
            data = sequence_df[f"acc_{axis}"].values
            nan_mask = np.isnan(data)

            # æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿æ¯”ç‡
            features[f"{prefix}_acc_{axis}_valid_ratio"] = 1 - np.mean(nan_mask)

            # æœ€å¤§é€£ç¶šæ¬ æ¸¬é•·
            if np.any(nan_mask):
                # é€£ç¶šã™ã‚‹æ¬ æ¸¬ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                changes = np.diff(np.concatenate(([0], nan_mask.astype(int), [0])))
                starts = np.where(changes == 1)[0]
                ends = np.where(changes == -1)[0]
                if len(starts) > 0:
                    consecutive_nans = ends - starts
                    features[f"{prefix}_acc_{axis}_max_consecutive_nan"] = np.max(
                        consecutive_nans
                    )
                else:
                    features[f"{prefix}_acc_{axis}_max_consecutive_nan"] = 0
            else:
                features[f"{prefix}_acc_{axis}_max_consecutive_nan"] = 0

    # ğŸ”§ T2: IMUã®æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«æ¯”ï¼ˆåˆ—å˜ä½â†’å¹³å‡ï¼‰
    acc_valid = []
    for axis in ["x", "y", "z"]:
        if f"acc_{axis}" in sequence_df.columns:
            v = sequence_df[f"acc_{axis}"].values
            acc_valid.append(1 - np.mean(np.isnan(v)))
    features[f"{prefix}_imu_valid_ratio_mean"] = (
        float(np.mean(acc_valid)) if acc_valid else 0.0
    )

    # Quaternionãƒ‡ãƒ¼ã‚¿ã®å“è³ª
    quat_cols = ["quat_w", "quat_x", "quat_y", "quat_z"]
    if all(col in sequence_df.columns for col in quat_cols):
        quat_data = sequence_df[quat_cols].values
        quat_nan_ratio = np.mean(np.isnan(quat_data))
        features[f"{prefix}_quat_valid_ratio"] = 1 - quat_nan_ratio

    # ToFãƒ‡ãƒ¼ã‚¿ã®å“è³ª
    for sensor_id in range(5):
        tof_cols = [c for c in sequence_df.columns if c.startswith(f"tof_{sensor_id}_")]
        if tof_cols:
            tof_data = sequence_df[tof_cols].values

            # æœ‰åŠ¹ãƒ”ã‚¯ã‚»ãƒ«æ¯”ç‡ã®çµ±è¨ˆ
            valid_ratios = []
            for frame_idx in range(len(tof_data)):
                frame = tof_data[frame_idx]
                valid_mask = (frame >= 0) & ~np.isnan(frame)
                valid_ratios.append(np.mean(valid_mask))

            valid_ratios = np.array(valid_ratios)
            features[f"{prefix}_tof_{sensor_id}_valid_ratio_mean"] = np.mean(
                valid_ratios
            )
            features[f"{prefix}_tof_{sensor_id}_valid_ratio_min"] = np.min(valid_ratios)
            features[f"{prefix}_tof_{sensor_id}_valid_ratio_p5"] = np.percentile(
                valid_ratios, 5
            )
            features[f"{prefix}_tof_{sensor_id}_valid_ratio_p50"] = np.percentile(
                valid_ratios, 50
            )
            features[f"{prefix}_tof_{sensor_id}_valid_ratio_p95"] = np.percentile(
                valid_ratios, 95
            )

            # å®Œå…¨ã«ç„¡åŠ¹ãªãƒ•ãƒ¬ãƒ¼ãƒ ã®å‰²åˆ
            features[f"{prefix}_tof_{sensor_id}_invalid_frame_ratio"] = np.mean(
                valid_ratios == 0
            )

    # ğŸ”§ T2: ToFå…¨ä½“ã®é›†ç´„å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹
    all_tof_valid_ratios = []
    for sensor_id in range(5):
        if f"{prefix}_tof_{sensor_id}_valid_ratio_mean" in features:
            all_tof_valid_ratios.append(
                features[f"{prefix}_tof_{sensor_id}_valid_ratio_mean"]
            )

    if all_tof_valid_ratios:
        features[f"{prefix}_tof_all_valid_ratio_mean"] = np.mean(all_tof_valid_ratios)
        features[f"{prefix}_tof_all_valid_ratio_min"] = np.min(all_tof_valid_ratios)
        features[f"{prefix}_tof_all_valid_ratio_p25"] = np.percentile(
            all_tof_valid_ratios, 25
        )
        features[f"{prefix}_tof_all_valid_ratio_p75"] = np.percentile(
            all_tof_valid_ratios, 75
        )

    # Thermalãƒ‡ãƒ¼ã‚¿ã®å“è³ª
    thermal_cols = [c for c in sequence_df.columns if c.startswith("therm_")]
    for therm_col in thermal_cols:
        if therm_col in sequence_df.columns:
            therm_data = sequence_df[therm_col].values
            nan_mask = np.isnan(therm_data)
            features[f"{prefix}_{therm_col}_valid_ratio"] = 1 - np.mean(nan_mask)

            # æœ€å¤§é€£ç¶šæ¬ æ¸¬é•·
            if np.any(nan_mask):
                changes = np.diff(np.concatenate(([0], nan_mask.astype(int), [0])))
                starts = np.where(changes == 1)[0]
                ends = np.where(changes == -1)[0]
                if len(starts) > 0:
                    consecutive_nans = ends - starts
                    features[f"{prefix}_{therm_col}_max_consecutive_nan"] = np.max(
                        consecutive_nans
                    )
                else:
                    features[f"{prefix}_{therm_col}_max_consecutive_nan"] = 0
            else:
                features[f"{prefix}_{therm_col}_max_consecutive_nan"] = 0

    return features


# ====================================================================================================
# TOF SPATIAL FEATURES
# ====================================================================================================


def mirror_tof_by_handedness(tof_frame: np.ndarray, handedness: int) -> np.ndarray:
    """Mirror ToF frame based on handedness (0=left, 1=right)."""
    if handedness == 1:  # Right-handed, mirror horizontally
        if tof_frame.shape == (8, 8):
            return np.fliplr(tof_frame)
        else:
            # For flattened array, reshape then flip
            return np.fliplr(tof_frame.reshape(8, 8)).flatten()
    return tof_frame


def extract_tof_region_features(tof_frame: np.ndarray, prefix: str) -> dict:
    """
    ToFãƒ•ãƒ¬ãƒ¼ãƒ ã®ç•°ãªã‚‹ç©ºé–“é ˜åŸŸã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹ã€‚

    â‘¡ ToFç‰¹å¾´é‡ - é ˜åŸŸåˆ†æï¼š
    - 8Ã—8ç”»åƒã‚’3å±¤ã®åŒå¿ƒé ˜åŸŸã«åˆ†å‰²
    - ä¸­å¿ƒ3Ã—3é ˜åŸŸ
    - å†…å´ãƒªãƒ³ã‚°ï¼ˆ5Ã—5ã‹ã‚‰ä¸­å¿ƒ3Ã—3ã‚’é™¤ãï¼‰
    - å¤–å´ãƒªãƒ³ã‚°ï¼ˆå‘¨ç¸ï¼‰
    - å„é ˜åŸŸã®çµ±è¨ˆé‡ã¨é ˜åŸŸé–“ã®å¤‰å‹•æ€§

    ä¿®æ­£ï¼šé ˜åŸŸãƒã‚¹ã‚¯ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’æ˜ç¢ºåŒ–ã€‚
    """
    features = {}

    if tof_frame.shape != (8, 8):
        return features

    # ç„¡åŠ¹å€¤ã‚’å‡¦ç†
    valid_mask = (tof_frame >= 0) & ~np.isnan(tof_frame)
    tof_clean = np.where(valid_mask, tof_frame, np.inf)

    # æ˜ç¤ºçš„ã«ä¸‰å±¤ã‚’å®šç¾©
    # 1. Center 3x3 region (rows 2-4, cols 2-4, but Python uses 0-indexing)
    # Note: 8x8ã®ä¸­å¿ƒ3x3ã¯[2:5, 2:5]ï¼ˆPythonã®ç¯„å›²ã¯çµ‚ç«¯ã‚’å«ã¾ãªã„ï¼‰
    center_region = tof_clean[2:5, 2:5]
    valid_center = center_region[center_region < np.inf]
    if len(valid_center) > 0:
        features[f"{prefix}_center_mean"] = np.mean(valid_center)
        features[f"{prefix}_center_min"] = np.min(valid_center)
        features[f"{prefix}_center_std"] = np.std(valid_center)
    else:
        features[f"{prefix}_center_mean"] = 0
        features[f"{prefix}_center_min"] = 0
        features[f"{prefix}_center_std"] = 0

    # 2. Inner ring (5x5 excluding center 3x3)
    # 5x5é ˜åŸŸã¯[1:6, 1:6]
    inner_mask = np.zeros((8, 8), dtype=bool)
    inner_mask[1:6, 1:6] = True  # 5x5é ˜åŸŸã‚’True
    inner_mask[2:5, 2:5] = False  # ä¸­å¿ƒ3x3ã‚’False
    inner_vals = tof_clean[inner_mask]
    valid_inner = inner_vals[inner_vals < np.inf]
    if len(valid_inner) > 0:
        features[f"{prefix}_inner_mean"] = np.mean(valid_inner)
        features[f"{prefix}_inner_min"] = np.min(valid_inner)
        features[f"{prefix}_inner_std"] = np.std(valid_inner)
    else:
        features[f"{prefix}_inner_mean"] = 0
        features[f"{prefix}_inner_min"] = 0
        features[f"{prefix}_inner_std"] = 0

    # 3. Outer ring (everything outside 5x5)
    outer_mask = np.ones((8, 8), dtype=bool)
    outer_mask[1:6, 1:6] = False  # 5x5é ˜åŸŸã‚’False
    outer_vals = tof_clean[outer_mask]
    valid_outer = outer_vals[outer_vals < np.inf]
    if len(valid_outer) > 0:
        features[f"{prefix}_outer_mean"] = np.mean(valid_outer)
        features[f"{prefix}_outer_min"] = np.min(valid_outer)
        features[f"{prefix}_outer_std"] = np.std(valid_outer)
    else:
        features[f"{prefix}_outer_mean"] = 0
        features[f"{prefix}_outer_min"] = 0
        features[f"{prefix}_outer_std"] = 0

    # é ˜åŸŸé–“ã®å¤‰å‹•ï¼ˆä¸­å¿ƒã‹ã‚‰å¤–å´ã¸ã®å‹¾é…ï¼‰
    if len(valid_center) > 0 and len(valid_outer) > 0:
        features[f"{prefix}_center_to_outer_gradient"] = np.mean(valid_outer) - np.mean(
            valid_center
        )
    else:
        features[f"{prefix}_center_to_outer_gradient"] = 0

    # Four quadrants
    quadrants = [
        tof_clean[:4, :4],  # Top-left
        tof_clean[:4, 4:],  # Top-right
        tof_clean[4:, :4],  # Bottom-left
        tof_clean[4:, 4:],  # Bottom-right
    ]

    for i, quad in enumerate(quadrants):
        valid_quad = quad[quad < np.inf]
        if len(valid_quad) > 0:
            features[f"{prefix}_quad{i}_mean"] = np.mean(valid_quad)
            features[f"{prefix}_quad{i}_min"] = np.min(valid_quad)
            features[f"{prefix}_quad{i}_std"] = np.std(valid_quad)
        else:
            features[f"{prefix}_quad{i}_mean"] = 0
            features[f"{prefix}_quad{i}_min"] = 0
            features[f"{prefix}_quad{i}_std"] = 0

    # Left vs Right half
    left_half = tof_clean[:, :4]
    right_half = tof_clean[:, 4:]
    valid_left = left_half[left_half < np.inf]
    valid_right = right_half[right_half < np.inf]

    if len(valid_left) > 0 and len(valid_right) > 0:
        features[f"{prefix}_lr_asymmetry"] = np.mean(valid_left) - np.mean(valid_right)
        features[f"{prefix}_lr_variance_ratio"] = np.var(valid_left) / (
            np.var(valid_right) + 1e-8
        )
    else:
        features[f"{prefix}_lr_asymmetry"] = 0
        features[f"{prefix}_lr_variance_ratio"] = 1

    # Top vs Bottom half
    top_half = tof_clean[:4, :]
    bottom_half = tof_clean[4:, :]
    valid_top = top_half[top_half < np.inf]
    valid_bottom = bottom_half[bottom_half < np.inf]

    if len(valid_top) > 0 and len(valid_bottom) > 0:
        features[f"{prefix}_tb_asymmetry"] = np.mean(valid_top) - np.mean(valid_bottom)
        features[f"{prefix}_tb_variance_ratio"] = np.var(valid_top) / (
            np.var(valid_bottom) + 1e-8
        )
    else:
        features[f"{prefix}_tb_asymmetry"] = 0
        features[f"{prefix}_tb_variance_ratio"] = 1

    return features


def extract_tof_near_frac(
    tof_frame: np.ndarray, prefix: str, quantiles: list = [10, 20]
) -> dict:
    """
    ç‰¹å®šã®è·é›¢åˆ†ä½æ•°ä»¥ä¸‹ã®ãƒ”ã‚¯ã‚»ãƒ«ã®å‰²åˆã‚’æŠ½å‡ºã™ã‚‹ã€‚

    è¿‘è·é›¢ç‰©ä½“ã®æ¤œå‡ºã«æœ‰ç”¨ãªç‰¹å¾´é‡ã€‚
    """
    features = {}

    # ç„¡åŠ¹å€¤ã‚’å‡¦ç†
    valid_mask = (tof_frame >= 0) & ~np.isnan(tof_frame)
    valid_data = tof_frame[valid_mask]

    if len(valid_data) == 0:
        for q in quantiles:
            features[f"{prefix}_near_frac_q{q}"] = 0
        return features

    for q in quantiles:
        threshold = np.percentile(valid_data, q)
        features[f"{prefix}_near_frac_q{q}"] = np.sum(valid_data < threshold) / len(
            valid_data
        )

    return features


def extract_tof_anisotropy(tof_frame: np.ndarray, prefix: str) -> dict:
    """
    PCAå›ºæœ‰å€¤ã‚’ä½¿ç”¨ã—ã¦ç•°æ–¹æ€§ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹ã€‚

    ToFç”»åƒã®æ–¹å‘ä¾å­˜æ€§ã¨æ§‹é€ ã‚’åˆ†æã™ã‚‹ç‰¹å¾´é‡ã€‚
    """
    features = {}

    if tof_frame.shape != (8, 8):
        features[f"{prefix}_anisotropy"] = 0
        features[f"{prefix}_principal_angle"] = 0
        return features

    # ç„¡åŠ¹å€¤ã‚’å‡¦ç†
    valid_mask = (tof_frame >= 0) & ~np.isnan(tof_frame)

    if np.sum(valid_mask) < 3:
        features[f"{prefix}_anisotropy"] = 0
        features[f"{prefix}_principal_angle"] = 0
        return features

    # é€†è·é›¢é‡ã¿ä»˜ã‘ã•ã‚ŒãŸæœ‰åŠ¹ãƒ”ã‚¯ã‚»ãƒ«ã®åº§æ¨™ã‚’å–å¾—
    x, y = np.meshgrid(range(8), range(8))
    weights = np.where(valid_mask, 1.0 / (tof_frame + 1), 0)

    # ç‚¹ç¾¤ã‚’ä½œæˆ
    valid_points = []
    for i in range(8):
        for j in range(8):
            if valid_mask[i, j]:
                weight = weights[i, j]
                valid_points.append([x[i, j] * weight, y[i, j] * weight])

    if len(valid_points) < 3:
        features[f"{prefix}_anisotropy"] = 0
        features[f"{prefix}_principal_angle"] = 0
        return features

    points = np.array(valid_points)

    # å…±åˆ†æ•£è¡Œåˆ—ã‚’è¨ˆç®—
    cov = np.cov(points.T)

    # å›ºæœ‰å€¤ã‚’å–å¾—
    eigenvalues, eigenvectors = np.linalg.eig(cov)
    eigenvalues = np.sort(eigenvalues)[::-1]

    # Anisotropy (ratio of eigenvalues)
    if eigenvalues[0] > 0:
        features[f"{prefix}_anisotropy"] = 1 - eigenvalues[1] / eigenvalues[0]
    else:
        features[f"{prefix}_anisotropy"] = 0

    # Principal direction angle
    principal_vec = eigenvectors[:, 0]
    features[f"{prefix}_principal_angle"] = np.arctan2(
        principal_vec[1], principal_vec[0]
    )

    return features


def extract_tof_sensor_sync_features(
    all_min_dists: dict, prefix: str = "tof_sync"
) -> dict:
    """
    è¤‡æ•°ã®ToFã‚»ãƒ³ã‚µãƒ¼é–“ã®åŒæœŸç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹ã€‚

    â‘¥ ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«ç‰¹å¾´é‡ï¼š
    - ã‚»ãƒ³ã‚µãƒ¼é–“ã®ç›¸é–¢
    - åŒæœŸæ€§ã®æ¸¬å®š

    ä¿®æ­£ï¼špadded_dataã‚’ä½¿ç”¨ã—ã¦é•·ã•ä¸ä¸€è‡´ã‚¨ãƒ©ãƒ¼ã‚’é˜²ãã€‚
    """
    features = {}

    if len(all_min_dists) < 2:
        features[f"{prefix}_simultaneous_drop_rate"] = 0
        features[f"{prefix}_avg_time_lag"] = 0
        features[f"{prefix}_coherence"] = 0
        return features

    # é…åˆ—ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
    sensor_data = []
    max_len = 0
    for sensor_id in sorted(all_min_dists.keys()):
        data = all_min_dists[sensor_id]
        sensor_data.append(data)
        max_len = max(max_len, len(data))

    # Pad arrays to same length
    padded_data = []
    for data in sensor_data:
        if len(data) < max_len:
            padded = np.pad(data, (0, max_len - len(data)), "edge")
        else:
            padded = data[:max_len]
        padded_data.append(padded)

    sensor_array = np.array(padded_data)

    # Simultaneous drop detection
    threshold = np.percentile(sensor_array, 20, axis=1, keepdims=True)
    proximity_masks = sensor_array < threshold

    # Count frames where multiple sensors detect proximity
    simultaneous_counts = np.sum(proximity_masks, axis=0)
    features[f"{prefix}_simultaneous_drop_rate"] = np.mean(
        simultaneous_counts >= 3
    )  # At least 3 sensors

    # Time lag analysis using cross-correlationï¼ˆä¿®æ­£ï¼špadded_dataã‚’ä½¿ç”¨ï¼‰
    from scipy.signal import correlate, correlation_lags

    lags = []
    for i in range(len(padded_data)):
        for j in range(i + 1, len(padded_data)):
            # scipy.signal.correlateã‚’ä½¿ç”¨ã—ã¦ç›¸äº’ç›¸é–¢ã‚’è¨ˆç®—
            data_i = padded_data[i] - np.mean(padded_data[i])
            data_j = padded_data[j] - np.mean(padded_data[j])

            # Normalize to avoid numerical issues
            std_i = np.std(data_i)
            std_j = np.std(data_j)
            if std_i > 0 and std_j > 0:
                data_i = data_i / std_i
                data_j = data_j / std_j

                # ç›¸äº’ç›¸é–¢ã‚’è¨ˆç®—
                corr = correlate(data_i, data_j, mode="same")
                lag_values = correlation_lags(len(data_i), len(data_j), mode="same")

                # Find lag of maximum correlation
                max_corr_idx = np.argmax(np.abs(corr))
                lag = lag_values[max_corr_idx]
                lags.append(abs(lag))

    if lags:
        features[f"{prefix}_avg_time_lag"] = np.mean(lags) / 20.0  # ç§’ã«å¤‰æ›
    else:
        features[f"{prefix}_avg_time_lag"] = 0

    # Overall coherence (average correlation between sensors)ï¼ˆä¿®æ­£ï¼špadded_dataã‚’ä½¿ç”¨ï¼‰
    correlations = []
    for i in range(len(padded_data)):
        for j in range(i + 1, len(padded_data)):
            # padded_dataã¯æ—¢ã«åŒã˜é•·ã•
            try:
                corr = np.corrcoef(padded_data[i], padded_data[j])[0, 1]
                if not np.isnan(corr):
                    correlations.append(corr)
            except:
                pass  # ç›¸é–¢è¨ˆç®—ã«å¤±æ•—ã—ãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—

    if correlations:
        features[f"{prefix}_coherence"] = np.mean(correlations)
    else:
        features[f"{prefix}_coherence"] = 0

    return features


def extract_tof_arrival_event_features(
    min_dists: np.ndarray, prefix: str, threshold_percentile: int = 20
) -> dict:
    """
    ToFæœ€å°è·é›¢æ™‚ç³»åˆ—ã‹ã‚‰åˆ°ç€ã‚¤ãƒ™ãƒ³ãƒˆç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹ã€‚

    ç‰©ä½“ã®æ¥è¿‘ãƒ»é›¢è„±ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã™ã‚‹ç‰¹å¾´é‡ã€‚
    """
    features = {}

    if len(min_dists) < 2:
        features[f"{prefix}_arrival_rate"] = 0
        features[f"{prefix}_max_arrival_duration"] = 0
        features[f"{prefix}_arrival_frequency"] = 0
        return features

    # ã€Œåˆ°é”ã€ï¼ˆè¿‘è·é›¢ï¼‰ã®é–¾å€¤ã‚’è¨ˆç®—
    threshold = np.percentile(min_dists, threshold_percentile)

    # Binary mask for arrival events
    arrival_mask = min_dists < threshold

    # Arrival rate (percentage of time in arrival state)
    features[f"{prefix}_arrival_rate"] = np.mean(arrival_mask)

    # Find continuous arrival segments
    changes = np.diff(np.concatenate(([0], arrival_mask.astype(int), [0])))
    starts = np.where(changes == 1)[0]
    ends = np.where(changes == -1)[0]

    if len(starts) > 0:
        durations = ends - starts
        features[f"{prefix}_max_arrival_duration"] = (
            np.max(durations) / 20.0
        )  # ç§’ã«å¤‰æ›
        features[f"{prefix}_arrival_frequency"] = len(starts) / (
            len(min_dists) / 20.0
        )  # Events per second
    else:
        features[f"{prefix}_max_arrival_duration"] = 0
        features[f"{prefix}_arrival_frequency"] = 0

    return features


def extract_tof_clustering_features(
    tof_frame: np.ndarray, prefix: str, threshold_percentile: int = 20
) -> dict:
    """
    äºŒå€¤åŒ–ã•ã‚ŒãŸToFãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹ã€‚

    â‘¡ ToFç‰¹å¾´é‡ - ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼š
    - è¿‘è·é›¢é ˜åŸŸã®ã‚¯ãƒ©ã‚¹ã‚¿æ¤œå‡º
    - ã‚¯ãƒ©ã‚¹ã‚¿ã®ã‚µã‚¤ã‚ºã¨å½¢çŠ¶ç‰¹æ€§
    """
    features = {}

    if tof_frame.shape != (8, 8):
        for key in ["max_cluster_size", "n_clusters", "cluster_circularity"]:
            features[f"{prefix}_{key}"] = 0
        return features

    # ç„¡åŠ¹å€¤ã‚’å‡¦ç†
    valid_mask = (tof_frame >= 0) & ~np.isnan(tof_frame)
    valid_data = tof_frame[valid_mask]

    if len(valid_data) == 0:
        for key in ["max_cluster_size", "n_clusters", "cluster_circularity"]:
            features[f"{prefix}_{key}"] = 0
        return features

    # Binarize based on threshold
    threshold = np.percentile(valid_data, threshold_percentile)
    binary = (tof_frame < threshold) & valid_mask

    # Connected components analysis
    from scipy import ndimage

    labeled, n_clusters = ndimage.label(binary)

    features[f"{prefix}_n_clusters"] = n_clusters

    if n_clusters > 0:
        # Find largest cluster
        cluster_sizes = [np.sum(labeled == i) for i in range(1, n_clusters + 1)]
        max_cluster_size = max(cluster_sizes)
        features[f"{prefix}_max_cluster_size"] = max_cluster_size

        # æœ€å¤§ã‚¯ãƒ©ã‚¹ã‚¿ã®å††å½¢åº¦ã‚’è¨ˆç®—
        max_cluster_label = cluster_sizes.index(max_cluster_size) + 1
        cluster_mask = labeled == max_cluster_label

        # Perimeter approximation
        perimeter = np.sum(np.abs(np.diff(cluster_mask.astype(int), axis=0))) + np.sum(
            np.abs(np.diff(cluster_mask.astype(int), axis=1))
        )

        if perimeter > 0:
            features[f"{prefix}_cluster_circularity"] = (
                4 * np.pi * max_cluster_size / (perimeter**2)
            )
        else:
            features[f"{prefix}_cluster_circularity"] = 0
    else:
        features[f"{prefix}_max_cluster_size"] = 0
        features[f"{prefix}_cluster_circularity"] = 0

    return features


def extract_tof_spatial_features(tof_frame: np.ndarray, prefix: str) -> dict:
    """
    8Ã—8 ToFãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰ç©ºé–“ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹ã€‚

    â‘¡ ToFç‰¹å¾´é‡ - ç©ºé–“ç‰¹å¾´ï¼š
    - é‡å¿ƒä½ç½®
    - ç©ºé–“çš„åºƒãŒã‚Š
    - ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆç‰¹å¾´
    """
    features = {}

    # ç„¡åŠ¹å€¤ã‚’å‡¦ç†
    valid_mask = (tof_frame >= 0) & ~np.isnan(tof_frame)
    valid_data = tof_frame[valid_mask]

    features[f"{prefix}_valid_ratio"] = np.sum(valid_mask) / tof_frame.size

    if len(valid_data) == 0:
        # No valid data
        for key in [
            "mean",
            "std",
            "min",
            "max",
            "p10",
            "p50",
            "p90",
            "centroid_x",
            "centroid_y",
            "moment_xx",
            "moment_yy",
            "moment_xy",
            "eccentricity",
            "edge_sum",
            "gradient_sum",
        ]:
            features[f"{prefix}_{key}"] = 0
        return features

    # Basic statistics
    features[f"{prefix}_mean"] = np.mean(valid_data)
    features[f"{prefix}_std"] = np.std(valid_data)
    features[f"{prefix}_min"] = np.min(valid_data)
    features[f"{prefix}_max"] = np.max(valid_data)
    features[f"{prefix}_p10"] = np.percentile(valid_data, 10)
    features[f"{prefix}_p50"] = np.percentile(valid_data, 50)
    features[f"{prefix}_p90"] = np.percentile(valid_data, 90)

    # Spatial moments and centroid
    if tof_frame.shape == (8, 8):
        # åº§æ¨™ã‚°ãƒªãƒƒãƒ‰ã‚’ä½œæˆ
        x, y = np.meshgrid(range(8), range(8))

        # Use valid data for weights
        weights = np.where(
            valid_mask, 1.0 / (tof_frame + 1), 0
        )  # Inverse distance weighting
        total_weight = np.sum(weights)

        if total_weight > 0:
            # Centroid
            cx = np.sum(x * weights) / total_weight
            cy = np.sum(y * weights) / total_weight
            features[f"{prefix}_centroid_x"] = cx
            features[f"{prefix}_centroid_y"] = cy

            # Second moments
            features[f"{prefix}_moment_xx"] = (
                np.sum((x - cx) ** 2 * weights) / total_weight
            )
            features[f"{prefix}_moment_yy"] = (
                np.sum((y - cy) ** 2 * weights) / total_weight
            )
            features[f"{prefix}_moment_xy"] = (
                np.sum((x - cx) * (y - cy) * weights) / total_weight
            )

            # Eccentricity
            if features[f"{prefix}_moment_xx"] + features[f"{prefix}_moment_yy"] > 0:
                lambda1 = 0.5 * (
                    features[f"{prefix}_moment_xx"]
                    + features[f"{prefix}_moment_yy"]
                    + np.sqrt(
                        (
                            features[f"{prefix}_moment_xx"]
                            - features[f"{prefix}_moment_yy"]
                        )
                        ** 2
                        + 4 * features[f"{prefix}_moment_xy"] ** 2
                    )
                )
                lambda2 = 0.5 * (
                    features[f"{prefix}_moment_xx"]
                    + features[f"{prefix}_moment_yy"]
                    - np.sqrt(
                        (
                            features[f"{prefix}_moment_xx"]
                            - features[f"{prefix}_moment_yy"]
                        )
                        ** 2
                        + 4 * features[f"{prefix}_moment_xy"] ** 2
                    )
                )
                if lambda1 > 0:
                    features[f"{prefix}_eccentricity"] = np.sqrt(1 - lambda2 / lambda1)
                else:
                    features[f"{prefix}_eccentricity"] = 0
            else:
                features[f"{prefix}_eccentricity"] = 0
        else:
            features[f"{prefix}_centroid_x"] = 4
            features[f"{prefix}_centroid_y"] = 4
            features[f"{prefix}_moment_xx"] = 0
            features[f"{prefix}_moment_yy"] = 0
            features[f"{prefix}_moment_xy"] = 0
            features[f"{prefix}_eccentricity"] = 0

        # Edge detection (simplified)
        valid_frame = np.where(valid_mask, tof_frame, 0)
        dx = np.abs(np.diff(valid_frame, axis=1))
        dy = np.abs(np.diff(valid_frame, axis=0))
        features[f"{prefix}_edge_sum"] = np.sum(dx) + np.sum(dy)

        # Gradient sum (Sobel-like)
        features[f"{prefix}_gradient_sum"] = np.sqrt(np.sum(dx**2) + np.sum(dy**2))
    else:
        # Flat data
        for key in [
            "centroid_x",
            "centroid_y",
            "moment_xx",
            "moment_yy",
            "moment_xy",
            "eccentricity",
            "edge_sum",
            "gradient_sum",
        ]:
            features[f"{prefix}_{key}"] = 0

    return features


# ====================================================================================================
# THERMAL FEATURES
# ====================================================================================================


def extract_thermal_advanced_features(
    thm_data: np.ndarray, prefix: str, threshold_percentile: float = 75
) -> dict:
    """Extract advanced thermal features including second derivatives and event rates."""
    features = {}

    if len(thm_data) < 3:
        features[f"{prefix}_diff2_mean"] = 0
        features[f"{prefix}_diff2_std"] = 0
        features[f"{prefix}_diff2_max"] = 0
        features[f"{prefix}_change_event_rate"] = 0
        features[f"{prefix}_p90_p10_spread"] = 0
        return features

    # Second derivative (acceleration of temperature change)
    diff1 = np.diff(thm_data)
    diff2 = np.diff(diff1)

    features[f"{prefix}_diff2_mean"] = np.mean(diff2)
    features[f"{prefix}_diff2_std"] = np.std(diff2)
    features[f"{prefix}_diff2_max"] = np.max(np.abs(diff2))

    # Change event rate (percentage of significant changes)
    threshold = np.percentile(np.abs(diff1), threshold_percentile)
    features[f"{prefix}_change_event_rate"] = np.mean(np.abs(diff1) > threshold)

    # Quantile spread (p90 - p10)
    features[f"{prefix}_p90_p10_spread"] = np.percentile(thm_data, 90) - np.percentile(
        thm_data, 10
    )

    return features


# ====================================================================================================
# CROSS-MODAL FEATURES
# ====================================================================================================


def extract_cross_modal_sync_features(
    linear_acc_mag: np.ndarray,
    tof_min_dists: dict,
    thermal_data: dict,
    omega_mag: np.ndarray = None,
) -> dict:
    """Extract sophisticated cross-modal synchronization features."""
    features = {}

    # Find linear acceleration peaks
    if len(linear_acc_mag) < 10:
        return features

    peaks, _ = find_peaks(linear_acc_mag, height=np.std(linear_acc_mag) * 0.5)

    if len(peaks) == 0:
        features["cross_modal_sync_score"] = 0
        features["cross_modal_triplet_consistency"] = 0
        return features

    # Micro-synchronization: Check ToF min_dist around acceleration peaks
    window_size = 10  # Â±0.5 seconds at 20Hz
    min_dist_drops = []

    for sensor_id, min_dists in tof_min_dists.items():
        if len(min_dists) != len(linear_acc_mag):
            continue

        for peak_idx in peaks:
            window_start = max(0, peak_idx - window_size)
            window_end = min(len(min_dists), peak_idx + window_size)

            if window_end > window_start:
                window_data = min_dists[window_start:window_end]
                if len(window_data) > 0:
                    # ãƒ”ãƒ¼ã‚¯å‘¨è¾ºã®min_distã®ä½ä¸‹ã‚’è¨ˆç®—
                    baseline = np.mean(min_dists)
                    window_min = np.min(window_data)
                    drop = baseline - window_min
                    min_dist_drops.append(drop)

    if min_dist_drops:
        features["cross_modal_acc_tof_sync_mean"] = np.mean(min_dist_drops)
        features["cross_modal_acc_tof_sync_max"] = np.max(min_dist_drops)
    else:
        features["cross_modal_acc_tof_sync_mean"] = 0
        features["cross_modal_acc_tof_sync_max"] = 0

    # Triplet consistency: min_dist drop â†’ thermal rise â†’ acceleration peak
    triplet_scores = []

    for sensor_id in range(1, 6):
        if (
            f"tof_{sensor_id}" not in tof_min_dists
            or f"thm_{sensor_id}" not in thermal_data
        ):
            continue

        tof_data = tof_min_dists[f"tof_{sensor_id}"]
        thm_data = thermal_data[f"thm_{sensor_id}"]

        if len(tof_data) < 20 or len(thm_data) < 20:
            continue

        # Find ToF proximity events
        tof_threshold = np.percentile(tof_data, 20)
        tof_events = np.where(tof_data < tof_threshold)[0]

        for event_idx in tof_events[:10]:  # æœ€åˆã®10ã‚¤ãƒ™ãƒ³ãƒˆã‚’ãƒã‚§ãƒƒã‚¯
            if event_idx + 20 < len(thm_data) and event_idx + 20 < len(linear_acc_mag):
                # ToFè¿‘æ¥å¾Œã«æ¸©åº¦ãŒä¸Šæ˜‡ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                thm_before = np.mean(thm_data[max(0, event_idx - 5) : event_idx])
                thm_after = np.mean(
                    thm_data[event_idx : min(event_idx + 10, len(thm_data))]
                )
                thm_increase = thm_after > thm_before

                # åŠ é€Ÿåº¦ãƒ”ãƒ¼ã‚¯ãŒç¶šãã‹ãƒã‚§ãƒƒã‚¯
                acc_window = linear_acc_mag[
                    event_idx : min(event_idx + 20, len(linear_acc_mag))
                ]
                acc_peak = np.any(
                    acc_window > np.mean(linear_acc_mag) + np.std(linear_acc_mag)
                )

                if thm_increase and acc_peak:
                    triplet_scores.append(1.0)
                else:
                    triplet_scores.append(0.0)

    if triplet_scores:
        features["cross_modal_triplet_consistency"] = np.mean(triplet_scores)
    else:
        features["cross_modal_triplet_consistency"] = 0

    # Angular velocity correlation with ToF
    if omega_mag is not None and len(omega_mag) > 0:
        for sensor_id, min_dists in tof_min_dists.items():
            if len(min_dists) == len(omega_mag):
                features[f"cross_modal_omega_{sensor_id}_corr"] = np.corrcoef(
                    omega_mag, min_dists
                )[0, 1]

    return features


# ====================================================================================================
# MULTI-RESOLUTION WINDOW PROCESSING
# ====================================================================================================


def extract_multi_resolution_features(sequence_df: pd.DataFrame, config: dict) -> dict:
    """
    Extract features from multiple time windows (S/M/L) with Temporal Pyramid.
    ğŸ”§ T4: ã‚¹ãƒãƒ¼ãƒˆçª“ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å¤§çª“ï¼‰ã®å®Ÿè£…
    """
    features = {}

    if not config.get("use_multi_resolution", False):
        return features

    seq_len = len(sequence_df)
    window_sizes = config.get(
        "window_sizes", {"S": (20, 30), "M": (60, 80), "L": (200, 256)}
    )

    # ğŸ”§ T4: ã‚¹ãƒãƒ¼ãƒˆçª“ã®å®Ÿè£…ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å¤§çª“ï¼‰
    # åŠ é€Ÿåº¦ãƒã‚°ãƒ‹ãƒãƒ¥ãƒ¼ãƒ‰ã‚’ã‚¨ãƒãƒ«ã‚®ãƒ¼æŒ‡æ¨™ã¨ã—ã¦ä½¿ç”¨
    base = None
    if all(f"acc_{a}" in sequence_df.columns for a in ["x", "y", "z"]):
        base = np.sqrt(
            sequence_df["acc_x"] ** 2
            + sequence_df["acc_y"] ** 2
            + sequence_df["acc_z"] ** 2
        ).values

    # For each window size
    for window_name, (min_size, max_size) in window_sizes.items():
        # Determine actual window size based on sequence length
        if seq_len < min_size:
            continue

        window_size = min(max_size, seq_len)

        if base is not None and config.get("smart_windowing", True):
            # ğŸ”§ T4: ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å¤§çª“ã‚’è¦‹ã¤ã‘ã‚‹
            # ç§»å‹•RMSã‚’è¨ˆç®—ã—ã¦ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒæœ€å¤§ã®ä½ç½®ã‚’ç‰¹å®š
            s = pd.Series(base)
            rms = s.rolling(window_size, min_periods=max(8, window_size // 5)).apply(
                lambda v: np.sqrt(np.mean(v**2))
            )

            if not rms.isna().all():
                # RMSæœ€å¤§ä½ç½®ã‚’ä¸­å¿ƒã¨ã—ãŸçª“ã‚’å–å¾—
                center_idx = int(np.nanargmax(rms.values))
                start_idx = max(
                    0, min(center_idx - window_size // 2, seq_len - window_size)
                )
                window_df = sequence_df.iloc[start_idx : start_idx + window_size]
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæœ«å°¾çª“ã‚’ä½¿ç”¨
                start_idx = max(0, seq_len - window_size)
                window_df = sequence_df.iloc[start_idx:]
        else:
            # å¾“æ¥ã®å‡¦ç†ï¼šæœ«å°¾ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’æŠ½å‡ºï¼ˆäºˆæ¸¬ç”¨ã«å¼·èª¿ï¼‰
            if config.get("use_tail_emphasis", True):
                start_idx = max(0, seq_len - window_size)
                window_df = sequence_df.iloc[start_idx:]
            else:
                # Use middle window
                start_idx = max(0, (seq_len - window_size) // 2)
                window_df = sequence_df.iloc[start_idx : start_idx + window_size]

        # ã“ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®åŸºæœ¬çµ±è¨ˆé‡ã‚’æŠ½å‡º
        for col in ["acc_x", "acc_y", "acc_z"]:
            if col in window_df.columns:
                data = window_df[col].values
                features[f"{window_name}_{col}_mean"] = np.mean(data)
                features[f"{window_name}_{col}_std"] = np.std(data)
                features[f"{window_name}_{col}_max"] = np.max(data)
                features[f"{window_name}_{col}_min"] = np.min(data)

    # Temporal Pyramid: Additional multi-scale aggregation (0.5s, 1s, 2s windows)
    pyramid_windows = {
        "micro": 10,  # 0.5 seconds at 20Hz
        "short": 20,  # 1.0 seconds at 20Hz
        "medium": 40,  # 2.0 seconds at 20Hz
    }

    for col in ["acc_x", "acc_y", "acc_z"]:
        if col in sequence_df.columns:
            data = sequence_df[col].values

            for pyramid_name, pyramid_size in pyramid_windows.items():
                if len(data) >= pyramid_size:
                    # ã“ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ç§»å‹•å¹³å‡ã‚’é©ç”¨
                    smoothed = (
                        pd.Series(data)
                        .rolling(pyramid_size, center=True, min_periods=1)
                        .mean()
                        .values
                    )

                    # å¹³æ»‘åŒ–ã•ã‚ŒãŸä¿¡å·ã‹ã‚‰çµ±è¨ˆé‡ã‚’æŠ½å‡º
                    features[f"pyramid_{pyramid_name}_{col}_mean"] = np.mean(smoothed)
                    features[f"pyramid_{pyramid_name}_{col}_std"] = np.std(smoothed)
                    features[f"pyramid_{pyramid_name}_{col}_p10"] = np.percentile(
                        smoothed, 10
                    )
                    features[f"pyramid_{pyramid_name}_{col}_p90"] = np.percentile(
                        smoothed, 90
                    )

    return features


# ====================================================================================================
# MAIN FEATURE EXTRACTION
# ====================================================================================================


# ====================================================================================================
# åˆ—åã®è‡ªå‹•æ¤œå‡ºãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ====================================================================================================


def detect_quat_cols(df: pd.DataFrame) -> List[str]:
    """ã‚¯ã‚©ãƒ¼ã‚¿ãƒ‹ã‚ªãƒ³åˆ—åã‚’è‡ªå‹•æ¤œå‡ºã™ã‚‹ã€‚"""
    candidates = [
        ["rot_w", "rot_x", "rot_y", "rot_z"],
        ["quat_w", "quat_x", "quat_y", "quat_z"],
    ]
    for cols in candidates:
        if all(c in df.columns for c in cols):
            return cols
    return []  # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºãƒªã‚¹ãƒˆ


def detect_thermal_prefix(df: pd.DataFrame) -> str:
    """ã‚µãƒ¼ãƒãƒ«åˆ—ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è‡ªå‹•æ¤œå‡ºã™ã‚‹ã€‚"""
    if any(c.startswith("thm_") for c in df.columns):
        return "thm_"
    if any(c.startswith("therm_") for c in df.columns):
        return "therm_"
    if any(c.startswith("thermal_") for c in df.columns):
        return "thermal_"
    return "thm_"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ


def detect_tof_sensor_ids(df: pd.DataFrame) -> List[int]:
    """ToFã‚»ãƒ³ã‚µãƒ¼IDã‚’è‡ªå‹•æ¤œå‡ºã™ã‚‹ã€‚
    ä¾‹: 'tof_1_v0' â†’ ã‚»ãƒ³ã‚µãƒ¼ID=1
    """
    ids = set()
    for c in df.columns:
        if c.startswith("tof_") and "_v" in c:
            try:
                sid = int(c.split("_")[1])
                ids.add(sid)
            except:
                pass
    return sorted(ids) if ids else list(range(5))  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0-4


# ========================================
# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé–¢æ•°
# ========================================


def save_checkpoint(
    fold: int, model, feature_names: list, scaler, fold_artifacts: list
):
    """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜"""
    if not USE_CHECKPOINT:
        return

    checkpoint_dir = Path(CHECKPOINT_DIR)
    checkpoint_dir.mkdir(exist_ok=True)

    checkpoint_file = checkpoint_dir / f"checkpoint_fold_{fold}.pkl"
    checkpoint_data = {
        "fold": fold,
        "model": model,
        "feature_names": feature_names,
        "scaler": scaler,
        "fold_artifacts": fold_artifacts,
    }

    with open(checkpoint_file, "wb") as f:
        pickle.dump(checkpoint_data, f)

    print(f"  ğŸ’¾ Checkpoint saved: fold {fold}")


def load_checkpoint():
    """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹"""
    if not USE_CHECKPOINT:
        return None, None, 0

    checkpoint_dir = Path(CHECKPOINT_DIR)
    if not checkpoint_dir.exists():
        return None, None, 0

    checkpoint_files = sorted(checkpoint_dir.glob("checkpoint_fold_*.pkl"))
    if not checkpoint_files:
        return None, None, 0

    # æœ€æ–°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿
    models = []
    fold_artifacts = []
    last_fold = -1

    for cp_file in checkpoint_files:
        with open(cp_file, "rb") as f:
            cp_data = pickle.load(f)
        models.append(cp_data["model"])
        fold_artifacts.append(
            {"feature_names": cp_data["feature_names"], "scaler": cp_data["scaler"]}
        )
        last_fold = max(last_fold, cp_data["fold"])

    print(f"âœ… Checkpoint loaded: Resuming from fold {last_fold + 1}")
    return models, fold_artifacts, last_fold + 1


def remove_checkpoints():
    """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
    if not AUTO_REMOVE_CHECKPOINT:
        return

    checkpoint_dir = Path(CHECKPOINT_DIR)
    if checkpoint_dir.exists():
        import shutil

        shutil.rmtree(checkpoint_dir)
        print("ğŸ—‘ï¸ Checkpoints removed")


def fill_series_nan(x: np.ndarray) -> np.ndarray:
    """NaNå€¤ã‚’å‰æ–¹è£œå®Œâ†’å¾Œæ–¹è£œå®Œâ†’0ã§åŸ‹ã‚ã‚‹ã€‚"""
    series = pd.Series(x)
    return series.ffill().bfill().fillna(0).values


def extract_features_parallel(args):
    """Global function for parallel feature extraction (used only in local environment)."""
    extractor, seq_df, demo_df = args
    return extractor.extract_features(seq_df, demo_df)


# ========================================
# ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# ========================================


def _to01_handedness(v):
    """handednessã‚’R/Læ–‡å­—åˆ—ã‹ã‚‰1/0ã«å¤‰æ›"""
    if isinstance(v, str):
        v = v.strip().lower()
        if v.startswith("r"):
            return 1
        if v.startswith("l"):
            return 0
    try:
        return int(v)
    except:
        return 0


class FeatureExtractor:
    """
    ãƒ•ã‚£ãƒƒãƒˆã•ã‚ŒãŸå¤‰æ›å™¨ã‚’æŒã¤ãƒ¡ã‚¤ãƒ³ç‰¹å¾´é‡æŠ½å‡ºã‚¯ãƒ©ã‚¹ã€‚

    ã™ã¹ã¦ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å‡¦ç†ã‚’çµ±åˆç®¡ç†ã€‚
    æ”¹ä¿®ï¼šfoldå†…ã§Scaler/PCAã‚’fitã™ã‚‹ãŸã‚fit()ã¨transform()ã‚’åˆ†é›¢ã€‚
    """

    def __init__(self, config: dict):
        self.config = config
        self.scaler = None
        self.tof_pcas = {}  # Will store PCA transformers for each ToF sensor
        self.feature_names = None
        self.is_fitted = False
        self.percentile_thresholds = {}  # Store percentile thresholds for fold-specific fitting

    def fit(
        self, sequences: List[pd.DataFrame], demographics: List[pd.DataFrame]
    ) -> None:
        """
        è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Scalerã€PCAã€åˆ†ä½é–¾å€¤ãªã©ã‚’fitã™ã‚‹ã€‚
        foldå†…ã®trainãƒ‡ãƒ¼ã‚¿ã®ã¿ã§fitã—ã€CVãƒªãƒ¼ã‚¯ã‚’é˜²ãã€‚

        ä¿®æ­£: PCA â†’ æœ€çµ‚ç‰¹å¾´ â†’ scalerã®é †åºã§fit
        """
        print("  Fitting transformers on training data...")

        # ã‚¹ãƒ†ãƒƒãƒ—1: ToF PCAç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¦fit
        tof_data_by_sensor = {f"tof_{i}": [] for i in range(5)}

        if self.config.get("tof_use_pca", False):
            print("    Collecting ToF data for PCA...")
            for i in range(len(sequences)):
                seq_df = sequences[i]
                demo_df = demographics[i]
                for sensor_id in range(5):
                    tof_cols = [
                        c for c in seq_df.columns if c.startswith(f"tof_{sensor_id}_")
                    ]
                    if tof_cols:
                        tof_data = seq_df[tof_cols].values
                        # åˆ©ãæ‰‹å‡¦ç†ãŒæœ‰åŠ¹ãªå ´åˆã¯é©ç”¨
                        if self.config.get("tof_use_handedness_mirror", False):
                            handedness = (
                                demo_df["handedness"].iloc[0]
                                if "handedness" in demo_df.columns
                                else 0
                            )
                            for idx in range(len(tof_data)):
                                tof_data[idx] = mirror_tof_by_handedness(
                                    tof_data[idx], handedness
                                )
                        # Clean data
                        valid_mask = (tof_data >= 0) & ~np.isnan(tof_data)
                        tof_clean = np.where(valid_mask, tof_data, 0)
                        tof_data_by_sensor[f"tof_{sensor_id}"].append(tof_clean)

            # ToF PCAã‚’fit
            print("    Fitting ToF PCAs...")
            for sensor_id in range(5):
                sensor_key = f"tof_{sensor_id}"
                if tof_data_by_sensor[sensor_key]:
                    # å…¨è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
                    all_tof_data = np.vstack(tof_data_by_sensor[sensor_key])
                    n_samples, n_features = all_tof_data.shape
                    max_components = min(
                        n_samples - 1, n_features, self.config["tof_pca_components"]
                    )

                    if max_components >= 2:
                        pca = PCA(n_components=max_components)
                        try:
                            pca.fit(all_tof_data)
                            self.tof_pcas[sensor_key] = pca
                            print(
                                f"      Fitted PCA for {sensor_key}: {max_components} components"
                            )
                        except Exception as e:
                            print(f"      Failed to fit PCA for {sensor_key}: {e}")

        # ã‚¹ãƒ†ãƒƒãƒ—2: PCAã‚’å«ã‚€æœ€çµ‚å½¢ã®ç‰¹å¾´ã‚’æŠ½å‡º
        print("    Extracting final features with PCA...")
        final_features = []
        for i in range(len(sequences)):
            seq_df = sequences[i]
            demo_df = demographics[i]
            if i % 500 == 0:
                print(f"      Processing sequence {i}/{len(sequences)}...")
            # extract_featuresã‚’ä½¿ã£ã¦PCAã‚’å«ã‚€æœ€çµ‚å½¢ã®ç‰¹å¾´ã‚’å–å¾—
            features = self.extract_features(seq_df, demo_df)
            final_features.append(features)

        # ã‚¹ãƒ†ãƒƒãƒ—3: æœ€çµ‚å½¢ã®ç‰¹å¾´ã«å¯¾ã—ã¦scalerã‚’fit
        if final_features:
            X_final = pd.concat(final_features, ignore_index=True)
            self.feature_names = list(X_final.columns)

            # Scalerã‚’fit
            print("    Fitting scaler on final features...")
            if self.config["robust_scaler"]:
                self.scaler = RobustScaler()
            else:
                self.scaler = StandardScaler()
            self.scaler.fit(X_final[self.feature_names])

            # åˆ†ä½é–¾å€¤ã‚’è¨ˆç®—ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
            print("    Computing percentile thresholds...")
            # ToFè¿‘æ¥åˆ¤å®šã®é–¾å€¤ãªã©
            # ï¼ˆã“ã“ã«å¿…è¦ãªåˆ†ä½é–¾å€¤ã®è¨ˆç®—ã‚’è¿½åŠ ï¼‰

        self.is_fitted = True
        print(
            f"  âœ“ Fitted transformers on {len(sequences)} sequences with {len(self.feature_names)} features"
        )

    def transform(
        self, sequences: List[pd.DataFrame], demographics: List[pd.DataFrame]
    ) -> pd.DataFrame:
        """
        å­¦ç¿’æ¸ˆã¿ã®Scaler/PCAã§ç‰¹å¾´é‡ã‚’å¤‰æ›ã™ã‚‹ã€‚
        åˆ—ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’è¡Œã„ã€fitæ™‚ã¨åŒã˜åˆ—é †åºã‚’ä¿è¨¼ã™ã‚‹ã€‚
        """
        if not self.is_fitted:
            raise ValueError("FeatureExtractor must be fitted before transform")

        print(f"  Transforming {len(sequences)} sequences...")
        feature_dfs = []

        for i in range(len(sequences)):
            seq_df = sequences[i]
            demo_df = demographics[i]
            if i % 500 == 0:
                print(f"    Processing sequence {i}/{len(sequences)}...")
            features = self.extract_features(seq_df, demo_df)
            feature_dfs.append(features)

        X = pd.concat(feature_dfs, ignore_index=True)

        # ğŸ”§ T1: åˆ—ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆï¼ˆNaNä¿æŒï¼‰
        # ä¸è¶³åˆ—ã¯NaNã§è£œã„ã€ä½™å‰°åˆ—ã¯å‰Šé™¤ã—ã€é †åºã‚’æƒãˆã‚‹
        for col in self.feature_names:
            if col not in X.columns:
                X[col] = np.nan  # T1: æ¬ æã¯NaNã®ã¾ã¾ä¿æŒ
        X = X[self.feature_names]  # fitæ™‚ã®åˆ—é †åºã«åˆã‚ã›ã‚‹

        # ğŸ”§ T1: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®æ¡ä»¶ä»˜ãé©ç”¨
        if self.scaler is not None and self.config.get("use_scaler_for_xgb", True):
            X_scaled = self.scaler.transform(X)
            X = pd.DataFrame(X_scaled, columns=self.feature_names)

        return X

    def _extract_features_raw(
        self, seq_df: pd.DataFrame, demo_df: pd.DataFrame
    ) -> pd.DataFrame:
        """ç”Ÿã®ç‰¹å¾´é‡ã‚’æŠ½å‡ºï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰ï¼‰"""
        # å˜ä¸€ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã—ã¦ç‰¹å¾´æŠ½å‡º
        features = {}

        # ToFã®å‰å‡¦ç†
        tof_cols = [c for c in seq_df.columns if c.startswith("tof_")]
        if tof_cols and "handedness" in demo_df.columns:
            handedness = demo_df["handedness"].iloc[0] if len(demo_df) > 0 else 0
            seq_df = mirror_tof_by_handedness(seq_df, handedness)

        # IMUç‰¹å¾´
        if self.config.get("use_imu_features", True):
            # å››å…ƒæ•°åˆ—ã‚’æ¤œå‡º
            quat_cols = detect_quat_cols(seq_df)

            # åŸºæœ¬çš„ãªIMUç‰¹å¾´
            for col in ["accel_x", "accel_y", "accel_z", "gyro_x", "gyro_y", "gyro_z"]:
                if col in seq_df.columns:
                    data = seq_df[col].values
                    feat_dict = extract_statistical_features(data, prefix=f"{col}_")
                    features.update(feat_dict)

            # ä¸–ç•Œåº§æ¨™ç³»ã®åŠ é€Ÿåº¦
            if quat_cols and all(
                c in seq_df.columns for c in ["accel_x", "accel_y", "accel_z"]
            ):
                try:
                    world_accel = compute_world_acceleration(seq_df, quat_cols)
                    for i, axis in enumerate(["x", "y", "z"]):
                        feat_dict = extract_statistical_features(
                            world_accel[:, i], prefix=f"world_accel_{axis}_"
                        )
                        features.update(feat_dict)
                except:
                    pass

        # ToFç‰¹å¾´
        if self.config.get("use_tof_features", True) and tof_cols:
            # åŸºæœ¬çš„ãªToFç‰¹å¾´
            for col in tof_cols[:5]:  # æœ€åˆã®5ã‚»ãƒ³ã‚µãƒ¼
                data = seq_df[col].values
                feat_dict = extract_statistical_features(data, prefix=f"{col}_")
                features.update(feat_dict)

        # ãƒ‡ãƒ¢ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯ç‰¹å¾´
        if self.config.get("use_demographic_features", True):
            for col in demo_df.columns:
                if col != "subject":
                    features[f"demo_{col}"] = (
                        demo_df[col].iloc[0] if len(demo_df) > 0 else 0
                    )

        return pd.DataFrame([features])

    def fit_transform(
        self,
        sequences: List[pd.DataFrame],
        demographics: List[pd.DataFrame],
        labels: np.ndarray = None,
    ) -> pd.DataFrame:
        """
        fit()ã¨transform()ã‚’é€£ç¶šå®Ÿè¡Œï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰ã€‚
        """
        self.fit(sequences, demographics)
        return self.transform(sequences, demographics)

    def _extract_features_raw(
        self, seq_df: pd.DataFrame, demo_df: pd.DataFrame
    ) -> pd.DataFrame:
        """ç”Ÿã®ç‰¹å¾´é‡ã‚’æŠ½å‡ºï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰ï¼‰

        extract_featuresã¨åŒã˜ç‰¹å¾´é‡ã‚’ç”Ÿæˆã™ã‚‹ï¼ˆPCA/ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãªã—ï¼‰
        """
        # extract_featuresãƒ¡ã‚½ãƒƒãƒ‰ã‚’ãã®ã¾ã¾å‘¼ã³å‡ºã™
        # ï¼ˆPCAã¯is_fittedã§ãªã„ãŸã‚é©ç”¨ã•ã‚Œãªã„ï¼‰
        return self.extract_features(seq_df, demo_df)

    def extract_features(
        self, sequence_df: pd.DataFrame, demographics_df: pd.DataFrame
    ) -> pd.DataFrame:
        """
        ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‹ã‚‰ã™ã¹ã¦ã®ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹ã€‚

        7ç¨®é¡ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’çµ±åˆçš„ã«å®Ÿè¡Œã€‚
        æ”¹ä¿®ï¼šPCAã¯fitæ¸ˆã¿ã®ã‚‚ã®ã‚’transformã®ã¿ä½¿ç”¨ã€‚
        """
        features = {}

        # Demographic features (subjectã¯é™¤å¤–)
        # features["subject"] = demographics_df["subject"].iloc[0]  # æœªçŸ¥subjectæ±åŒ–ã®ãŸã‚é™¤å¤–
        features["age"] = demographics_df["age"].iloc[0]
        features["handedness"] = demographics_df["handedness"].iloc[0]

        # Quality featuresï¼ˆãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰
        quality_features = extract_quality_features(sequence_df)
        features.update(quality_features)

        # â‘  IMU features (Accelerometer, Quaternion, World/Linear acceleration, Angular velocity, Euler)
        for axis in ["x", "y", "z"]:
            # Raw accelerometer
            if f"acc_{axis}" in sequence_df.columns:
                # NaNå‡¦ç†: ffillâ†’bfillâ†’0
                acc_data = fill_series_nan(sequence_df[f"acc_{axis}"].values)
                features.update(extract_statistical_features(acc_data, f"acc_{axis}"))
                features.update(extract_hjorth_parameters(acc_data, f"acc_{axis}"))
                features.update(extract_peak_features(acc_data, f"acc_{axis}"))
                features.update(extract_line_length(acc_data, f"acc_{axis}"))
                features.update(extract_autocorrelation(acc_data, f"acc_{axis}"))
                features.update(extract_gradient_histogram(acc_data, f"acc_{axis}"))

                # â‘¤ Frequency features
                features.update(extract_frequency_features(acc_data, f"acc_{axis}"))

                # ğŸ”§ T3: ãƒ­ãƒã‚¹ãƒˆæ­£è¦åŒ–ç‰ˆIMUç‰¹å¾´é‡
                acc_r = robust_normalize(acc_data)
                features.update(extract_statistical_features(acc_r, f"accR_{axis}"))
                features.update(extract_frequency_features(acc_r, f"accR_{axis}"))

        # Acceleration magnitude
        if all(f"acc_{axis}" in sequence_df.columns for axis in ["x", "y", "z"]):
            acc_mag = np.sqrt(
                sequence_df["acc_x"] ** 2
                + sequence_df["acc_y"] ** 2
                + sequence_df["acc_z"] ** 2
            )
            features.update(extract_statistical_features(acc_mag, "acc_mag"))
            features.update(extract_hjorth_parameters(acc_mag, "acc_mag"))
            features.update(extract_peak_features(acc_mag, "acc_mag"))
            features.update(extract_frequency_features(acc_mag, "acc_mag"))

            # Jerk features
            features.update(extract_jerk_features(acc_mag, "acc_mag"))

        # Quaternion featuresï¼ˆè‡ªå‹•æ¤œå‡ºï¼‰
        quat_cols = detect_quat_cols(sequence_df)
        if quat_cols:
            quaternions = sequence_df[quat_cols].values
            quaternions = handle_quaternion_missing(quaternions)

            # Quaternion statistics
            for i, col in enumerate(quat_cols):
                features.update(extract_statistical_features(quaternions[:, i], col))

            # World acceleration
            if all(f"acc_{axis}" in sequence_df.columns for axis in ["x", "y", "z"]):
                acc_raw = sequence_df[["acc_x", "acc_y", "acc_z"]].values
                world_acc = compute_world_acceleration(acc_raw, quaternions)

                for i, axis in enumerate(["x", "y", "z"]):
                    features.update(
                        extract_statistical_features(
                            world_acc[:, i], f"world_acc_{axis}"
                        )
                    )
                    features.update(
                        extract_frequency_features(world_acc[:, i], f"world_acc_{axis}")
                    )

                # World acceleration magnitude
                world_acc_mag = np.linalg.norm(world_acc, axis=1)
                features.update(
                    extract_statistical_features(world_acc_mag, "world_acc_mag")
                )
                features.update(
                    extract_hjorth_parameters(world_acc_mag, "world_acc_mag")
                )
                features.update(
                    extract_frequency_features(world_acc_mag, "world_acc_mag")
                )

                # Linear acceleration
                linear_acc = compute_linear_acceleration(acc_raw, quaternions)
                for i, axis in enumerate(["x", "y", "z"]):
                    features.update(
                        extract_statistical_features(
                            linear_acc[:, i], f"linear_acc_{axis}"
                        )
                    )

                linear_acc_mag = np.linalg.norm(linear_acc, axis=1)
                features.update(
                    extract_statistical_features(linear_acc_mag, "linear_acc_mag")
                )
                features.update(
                    extract_hjorth_parameters(linear_acc_mag, "linear_acc_mag")
                )

                # â‘¦ Multi-resolution features (micro/short/medium windows)
                if self.config.get("use_multi_resolution", False):
                    for window_name, window_size in [
                        ("micro", 5),
                        ("short", 20),
                        ("medium", 50),
                    ]:
                        if len(world_acc_mag) >= window_size:
                            # Moving statistics
                            rolling_mean = (
                                pd.Series(world_acc_mag)
                                .rolling(window_size, min_periods=1)
                                .mean()
                            )
                            rolling_std = (
                                pd.Series(world_acc_mag)
                                .rolling(window_size, min_periods=1)
                                .std()
                            )

                            features[f"world_acc_mag_{window_name}_mean_mean"] = (
                                rolling_mean.mean()
                            )
                            features[f"world_acc_mag_{window_name}_mean_std"] = (
                                rolling_mean.std()
                            )
                            features[f"world_acc_mag_{window_name}_std_mean"] = (
                                rolling_std.mean()
                            )
                            features[f"world_acc_mag_{window_name}_std_max"] = (
                                rolling_std.max()
                            )

                        if len(linear_acc_mag) >= window_size:
                            rolling_mean = (
                                pd.Series(linear_acc_mag)
                                .rolling(window_size, min_periods=1)
                                .mean()
                            )
                            features[f"linear_acc_mag_{window_name}_mean_std"] = (
                                rolling_mean.std()
                            )

            # Angular velocity
            angular_vel = compute_angular_velocity(quaternions)
            for i, axis in enumerate(["x", "y", "z"]):
                features.update(
                    extract_statistical_features(
                        angular_vel[:, i], f"angular_vel_{axis}"
                    )
                )
            angular_vel_mag = np.linalg.norm(angular_vel, axis=1)
            features.update(
                extract_statistical_features(angular_vel_mag, "angular_vel_mag")
            )

            # Euler angles
            euler_angles = quaternion_to_euler(quaternions)
            for i, angle in enumerate(["roll", "pitch", "yaw"]):
                features.update(
                    extract_statistical_features(euler_angles[:, i], f"euler_{angle}")
                )

        # ğŸ”§ T5: ToFå“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆä½å“è³ªæ™‚ã¯è¨ˆç®—ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        # å“è³ªæƒ…å ±ã‹ã‚‰è¨ˆç®—ã™ã‚‹ï¼ˆæ—¢ã«extract_quality_featuresã§è¨ˆç®—æ¸ˆã¿ï¼‰
        q_tof_mean = features.get("quality_tof_all_valid_ratio_mean", 0.0)
        HAS_TOF = q_tof_mean is not None and q_tof_mean >= self.config.get(
            "quality_thresholds", {}
        ).get("tof", 0.05)

        # â‘¡ ToF features
        min_dists_all = []
        for sensor_id in range(5):
            tof_cols = [
                c for c in sequence_df.columns if c.startswith(f"tof_{sensor_id}_")
            ]
            if tof_cols and HAS_TOF:  # T5: å“è³ªãŒä½ã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                tof_data = sequence_df[tof_cols].values

                # åˆ©ãæ‰‹å‡¦ç†ã‚’é©ç”¨
                if self.config.get("tof_use_handedness_mirror", False):
                    handedness = demographics_df["handedness"].iloc[0]
                    for idx in range(len(tof_data)):
                        tof_data[idx] = mirror_tof_by_handedness(
                            tof_data[idx], handedness
                        )

                # PCA transformation if enabledï¼ˆæ”¹ä¿®ï¼štransformã®ã¿ï¼‰
                if self.config.get("tof_use_pca", False) and self.is_fitted:
                    sensor_key = f"tof_{sensor_id}"
                    if sensor_key in self.tof_pcas:
                        # ç„¡åŠ¹å€¤ã‚’å‡¦ç† for PCA
                        valid_mask = (tof_data >= 0) & ~np.isnan(tof_data)
                        tof_clean = np.where(valid_mask, tof_data, 0)

                        try:
                            pca = self.tof_pcas[sensor_key]
                            tof_pca_features = pca.transform(tof_clean)

                            # PCAç‰¹å¾´é‡ã‚’æŠ½å‡º
                            if tof_pca_features is not None:
                                for comp_idx in range(tof_pca_features.shape[1]):
                                    pca_series = tof_pca_features[:, comp_idx]
                                    features.update(
                                        extract_statistical_features(
                                            pca_series, f"tof_{sensor_id}_pca{comp_idx}"
                                        )
                                    )

                                # Reconstruction error
                                reconstructed = pca.inverse_transform(tof_pca_features)
                                recon_error = np.mean(
                                    np.abs(tof_clean - reconstructed), axis=1
                                )
                                features.update(
                                    extract_statistical_features(
                                        recon_error, f"tof_{sensor_id}_recon_error"
                                    )
                                )
                        except:
                            pass  # å¤‰æ›ã«å¤±æ•—ã—ãŸå ´åˆã¯PCAã‚’ã‚¹ã‚­ãƒƒãƒ—

                # å„ãƒ•ãƒ¬ãƒ¼ãƒ ã®ç©ºé–“ç‰¹å¾´é‡ã‚’å‡¦ç†
                frame_features = []
                for frame_idx in range(len(tof_data)):
                    frame_8x8 = tof_data[frame_idx].reshape(8, 8)

                    # Basic spatial features
                    frame_feat = extract_tof_spatial_features(
                        frame_8x8, f"tof_{sensor_id}_frame"
                    )

                    # æœ‰åŠ¹ãªå ´åˆã¯è¿½åŠ ã®ç©ºé–“ç‰¹å¾´é‡ã‚’è¨ˆç®—
                    if self.config.get("tof_region_analysis", False):
                        frame_feat.update(
                            extract_tof_region_features(
                                frame_8x8, f"tof_{sensor_id}_frame"
                            )
                        )
                        frame_feat.update(
                            extract_tof_near_frac(frame_8x8, f"tof_{sensor_id}_frame")
                        )
                        frame_feat.update(
                            extract_tof_anisotropy(frame_8x8, f"tof_{sensor_id}_frame")
                        )
                        frame_feat.update(
                            extract_tof_clustering_features(
                                frame_8x8, f"tof_{sensor_id}_frame"
                            )
                        )

                    frame_features.append(frame_feat)

                # Aggregate over time
                frame_df = pd.DataFrame(frame_features)
                for col in frame_df.columns:
                    time_series = frame_df[col].values
                    # Time statistics
                    features.update(extract_statistical_features(time_series, col))

                    # Velocity (first difference)
                    if len(time_series) > 1:
                        velocity = np.diff(time_series)
                        features.update(
                            extract_statistical_features(velocity, f"{col}_velocity")
                        )

                # Min distance time series
                min_dists = []
                for frame_idx in range(len(tof_data)):
                    frame_8x8 = tof_data[frame_idx].reshape(8, 8)
                    valid_mask = (frame_8x8 >= 0) & ~np.isnan(frame_8x8)
                    valid_data = frame_8x8[valid_mask]
                    if len(valid_data) > 0:
                        min_dists.append(np.min(valid_data))
                    else:
                        min_dists.append(np.nan)

                min_dists = np.array(min_dists)
                min_dists = min_dists[~np.isnan(min_dists)]

                if len(min_dists) > 0:
                    features.update(
                        extract_statistical_features(
                            min_dists, f"tof_{sensor_id}_min_dist"
                        )
                    )
                    features.update(
                        extract_hjorth_parameters(
                            min_dists, f"tof_{sensor_id}_min_dist"
                        )
                    )
                    features.update(
                        extract_tof_arrival_event_features(
                            min_dists, f"tof_{sensor_id}"
                        )
                    )
                    min_dists_all.append(min_dists)

                # Valid pixel ratio
                valid_ratios = []
                for frame_idx in range(len(tof_data)):
                    frame_8x8 = tof_data[frame_idx].reshape(8, 8)
                    valid_mask = (frame_8x8 >= 0) & ~np.isnan(frame_8x8)
                    valid_ratios.append(np.mean(valid_mask))
                valid_ratios = np.array(valid_ratios)
                features.update(
                    extract_statistical_features(
                        valid_ratios, f"tof_{sensor_id}_valid_ratio"
                    )
                )

        # â‘¥ Cross-modal ToF sync features
        if len(min_dists_all) > 1:
            # ã‚»ãƒ³ã‚µãƒ¼IDã‚’ã‚­ãƒ¼ã¨ã™ã‚‹è¾æ›¸ã‚’ä½œæˆ
            min_dists_dict = {
                f"tof_{i}": min_dists_all[i] for i in range(len(min_dists_all))
            }
            sync_features = extract_tof_sensor_sync_features(min_dists_dict)
            features.update(sync_features)

        # Global min across all ToF sensors
        if min_dists_all:
            # Pad to same length
            max_len = max(len(d) for d in min_dists_all)
            padded_dists = []
            for d in min_dists_all:
                if len(d) < max_len:
                    padded = np.pad(d, (0, max_len - len(d)), mode="edge")
                else:
                    padded = d
                padded_dists.append(padded)

            # Global min at each time point
            global_min = np.min(np.vstack(padded_dists), axis=0)
            features.update(
                extract_statistical_features(global_min, "tof_min_dist_global")
            )
            features.update(
                extract_hjorth_parameters(global_min, "tof_min_dist_global")
            )

        # â‘¢ Thermal features
        thermal_prefix = detect_thermal_prefix(sequence_df)
        thermal_cols = [c for c in sequence_df.columns if c.startswith(thermal_prefix)]
        for therm_col in thermal_cols:
            therm_data = sequence_df[therm_col].values
            therm_data = therm_data[~np.isnan(therm_data)]

            if len(therm_data) > 0:
                features.update(extract_statistical_features(therm_data, therm_col))

                # Rate of change
                if len(therm_data) > 1:
                    therm_diff = np.diff(therm_data)
                    features.update(
                        extract_statistical_features(therm_diff, f"{therm_col}_diff")
                    )

                # Temperature trend
                if len(therm_data) > 2:
                    time_indices = np.arange(len(therm_data))
                    slope, intercept = np.polyfit(time_indices, therm_data, 1)
                    features[f"{therm_col}_trend_slope"] = slope
                    features[f"{therm_col}_trend_intercept"] = intercept

                # Second derivative (acceleration of temperature change)
                if len(therm_data) > 2:
                    therm_diff2 = np.diff(therm_data, n=2)
                    features.update(
                        extract_statistical_features(therm_diff2, f"{therm_col}_diff2")
                    )

        # â‘¥ Cross-modal: IMU-ToF correlations
        # å¸¸ã«5ã¤ã®ã‚»ãƒ³ã‚µãƒ¼åˆ†ã®ç‰¹å¾´é‡ã‚’ç”Ÿæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯0ï¼‰
        for sensor_idx in range(5):
            features[f"cross_acc_tof{sensor_idx}_corr"] = 0
            features[f"cross_acc_peak_tof{sensor_idx}_mean"] = 0
            features[f"cross_acc_peak_tof{sensor_idx}_min"] = 0

        if "acc_mag" in locals() and len(acc_mag) > 0 and min_dists_all:
            # Correlate acceleration peaks with ToF proximity
            acc_peaks, _ = find_peaks(acc_mag, height=np.std(acc_mag) * 0.5)

            for i, min_dists in enumerate(min_dists_all):
                if i >= 5:  # æœ€å¤§5ã‚»ãƒ³ã‚µãƒ¼ã¾ã§
                    break
                if len(min_dists) > 0:
                    # Resample to match lengths
                    if len(acc_mag) != len(min_dists):
                        min_dists_resampled = np.interp(
                            np.linspace(0, 1, len(acc_mag)),
                            np.linspace(0, 1, len(min_dists)),
                            min_dists,
                        )
                    else:
                        min_dists_resampled = min_dists

                    # Correlation
                    if len(min_dists_resampled) > 1:
                        correlation = np.corrcoef(acc_mag, min_dists_resampled)[0, 1]
                        if not np.isnan(correlation):
                            features[f"cross_acc_tof{i}_corr"] = correlation

                    # Peak alignment
                    if len(acc_peaks) > 0:
                        # åŠ é€Ÿåº¦ãƒ”ãƒ¼ã‚¯æ™‚ã®ToFå€¤ã‚’ãƒã‚§ãƒƒã‚¯
                        peak_tof_values = []
                        for peak_idx in acc_peaks:
                            if peak_idx < len(min_dists_resampled):
                                peak_tof_values.append(min_dists_resampled[peak_idx])
                        if peak_tof_values:
                            features[f"cross_acc_peak_tof{i}_mean"] = np.mean(
                                peak_tof_values
                            )
                            features[f"cross_acc_peak_tof{i}_min"] = np.min(
                                peak_tof_values
                            )

        # ğŸ”§ T1: NaN/infå€¤ã®å‡¦ç†ï¼ˆpreserve_nan_for_missingã«å¿œã˜ã¦ï¼‰
        if not self.config.get("preserve_nan_for_missing", False):
            # å¾“æ¥ã®å‡¦ç†: NaN/infã‚’0ã«ç½®æ›
            for key in features:
                if isinstance(features[key], (float, np.floating)):
                    if np.isnan(features[key]) or np.isinf(features[key]):
                        features[key] = 0
        # else: NaNã‚’ä¿æŒï¼ˆXGBoostãŒå‡¦ç†ï¼‰

        return pd.DataFrame([features])

    # transform()ãƒ¡ã‚½ãƒƒãƒ‰ã¯æ—¢ã«ä¸Šã§å®šç¾©æ¸ˆã¿


# ====================================================================================================
# MODALITY DROPOUT (T7)
# ====================================================================================================


def apply_modality_dropout(X: pd.DataFrame, p: float, seed: int = 42) -> pd.DataFrame:
    """
    ğŸ”§ T7: ãƒ¢ãƒ€ãƒªãƒ†ã‚£ãƒ»ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
    å­¦ç¿’æ™‚ã«ToF/ã‚µãƒ¼ãƒãƒ«ç‰¹å¾´ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«NaNåŒ–ã—ã¦ã€
    IMU-onlyãƒ‡ãƒ¼ã‚¿ã¸ã®é †å¿œæ€§ã‚’é«˜ã‚ã‚‹ã€‚

    Args:
        X: ç‰¹å¾´é‡DataFrame
        p: ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç¢ºç‡ï¼ˆ0-1ï¼‰
        seed: ä¹±æ•°ã‚·ãƒ¼ãƒ‰

    Returns:
        ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’é©ç”¨ã—ãŸDataFrame
    """
    if p <= 0:
        return X

    X_dropout = X.copy()
    rng = np.random.RandomState(seed)
    n_samples = len(X)

    # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã™ã‚‹è¡Œã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ
    dropout_mask = rng.rand(n_samples) < p

    # ToFã¨ã‚µãƒ¼ãƒãƒ«é–¢é€£ã®åˆ—ã‚’è¦‹ã¤ã‘ã‚‹
    drop_cols = []
    for col in X.columns:
        if (
            col.startswith("tof_")
            or col.startswith("thm_")
            or col.startswith("therm_")
            or col.startswith("thermal_")
        ):
            drop_cols.append(col)

    # é¸æŠã•ã‚ŒãŸè¡Œã®è©²å½“åˆ—ã‚’NaNåŒ–
    if len(drop_cols) > 0:
        X_dropout.loc[dropout_mask, drop_cols] = np.nan
        print(
            f"  Applied modality dropout: {dropout_mask.sum()}/{n_samples} samples, {len(drop_cols)} columns"
        )

    return X_dropout


# ====================================================================================================
# FEATURE EXPORT/IMPORT UTILITIES
# ====================================================================================================


class FeatureExporter:
    """Handle export and import of features for cross-environment usage."""

    @staticmethod
    def export_features(
        features_df: pd.DataFrame,
        extractor,
        labels: np.ndarray,
        subjects: np.ndarray,
        export_name: str = None,
        compress: bool = True,
    ) -> Path:
        """Export features to portable format."""
        if export_name is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            export_name = f"features_{FEATURE_VERSION}_{timestamp}"

        export_path = EXPORT_DIR / export_name
        export_path.mkdir(exist_ok=True, parents=True)

        print(f"  Saving to: {export_path}")

        # ç‰¹å¾´é‡ã‚’Parquetå½¢å¼ã§ä¿å­˜
        features_file = export_path / "features.parquet"
        features_df.to_parquet(
            features_file, compression="snappy" if compress else None, index=False
        )
        print(
            f"  âœ“ Features saved ({len(features_df)} samples, {len(features_df.columns)} features)"
        )

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        metadata = {
            "labels": labels.tolist(),
            "subjects": subjects.tolist(),
            "n_samples": len(labels),
            "n_features": len(features_df.columns),
            "feature_names": list(features_df.columns),
            "feature_version": FEATURE_VERSION,
            "export_date": datetime.now().isoformat(),
        }

        with open(export_path / "metadata.json", "w") as f:
            json.dump(metadata, f, indent=2)

        # ã‚¨ã‚¯ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã®çŠ¶æ…‹ã‚’ä¿å­˜
        extractor_state = {
            "scaler": extractor.scaler,
            "tof_pcas": extractor.tof_pcas,
            "feature_names": extractor.feature_names,
            "config": extractor.config,
            "is_fitted": extractor.is_fitted,
        }
        with open(export_path / "extractor.pkl", "wb") as f:
            pickle.dump(extractor_state, f)

        print(f"  âœ“ Export complete: {export_path}")
        return export_path

    @staticmethod
    def import_features(
        import_path: str,
    ) -> Tuple[pd.DataFrame, np.ndarray, np.ndarray, dict]:
        """Import features from exported files."""
        import_path = Path(import_path)

        print(f"\nImporting features from: {import_path}")

        # ç‰¹å¾´é‡ã‚’ãƒ­ãƒ¼ãƒ‰
        features_df = pd.read_parquet(import_path / "features.parquet")

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰
        with open(import_path / "metadata.json", "r") as f:
            metadata = json.load(f)

        labels = np.array(metadata["labels"])
        subjects = np.array(metadata["subjects"])

        # ã‚¨ã‚¯ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã®çŠ¶æ…‹ã‚’ãƒ­ãƒ¼ãƒ‰
        with open(import_path / "extractor.pkl", "rb") as f:
            extractor_state = pickle.load(f)

        print(
            f"  âœ“ Imported {features_df.shape[0]} samples, {features_df.shape[1]} features"
        )
        return features_df, labels, subjects, extractor_state


# ====================================================================================================
# DATA VARIANT BUILDING (T6)
# ====================================================================================================


def build_dataset_variant(
    features_df: pd.DataFrame, variant: str = "full"
) -> pd.DataFrame:
    """
    ğŸ”§ T6: ãƒ‡ãƒ¼ã‚¿ãƒãƒªã‚¢ãƒ³ãƒˆã®æ’å¸¸åŒ–
    Fullç‰ˆã¨IMU-onlyç‰ˆã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆã€‚

    Args:
        features_df: ç‰¹å¾´é‡DataFrame
        variant: "full" ã¾ãŸã¯ "imu_only"

    Returns:
        æŒ‡å®šã•ã‚ŒãŸãƒãƒªã‚¢ãƒ³ãƒˆã®DataFrame
    """
    if variant == "full":
        # Fullç‰ˆã¯ãã®ã¾ã¾è¿”ã™
        return features_df
    elif variant == "imu_only":
        # IMU-onlyç‰ˆï¼šToF/ã‚µãƒ¼ãƒãƒ«ç‰¹å¾´ã‚’NaNåŒ–
        features_variant = features_df.copy()

        # ToFã¨ã‚µãƒ¼ãƒãƒ«é–¢é€£ã®åˆ—ã‚’è¦‹ã¤ã‘ã¦NaNåŒ–
        drop_cols = []
        for col in features_variant.columns:
            if (
                col.startswith("tof_")
                or col.startswith("thm_")
                or col.startswith("therm_")
                or col.startswith("thermal_")
            ):
                drop_cols.append(col)

        if len(drop_cols) > 0:
            features_variant[drop_cols] = np.nan
            print(f"  Created IMU-only variant: {len(drop_cols)} columns set to NaN")

        return features_variant
    else:
        raise ValueError(f"Unknown variant: {variant}")


# ====================================================================================================
# MODEL TRAINING
# ====================================================================================================


def train_models():
    """Train XGBoost models with cross-validation, with feature import/export.

    æ”¹ä¿®ï¼šCVãƒªãƒ¼ã‚¯é˜²æ­¢ã®ãŸã‚ã€foldå†…ã§Scaler/PCAã‚’fitã€‚
    ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã€å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã€fold_artifactså¯¾å¿œã‚’è¿½åŠ ã€‚
    """
    # Access global variables
    global USE_EXPORTED_FEATURES, EXPORTED_FEATURES_PATH, EXPORT_FEATURES, EXPORT_NAME
    global MODELS, EXTRACTOR, FOLD_ARTIFACTS
    global USE_PRETRAINED_MODEL, PRETRAINED_MODEL_PATH, PRETRAINED_EXTRACTOR_PATH
    global EXPORT_TRAINED_MODEL, USE_CHECKPOINT

    print("\n" + "=" * 70)
    print("TRAINING PHASE")
    print("=" * 70)

    # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
    if USE_PRETRAINED_MODEL:
        print("\nğŸ“¦ Loading pretrained model...")
        if PRETRAINED_MODEL_PATH and Path(PRETRAINED_MODEL_PATH).exists():
            MODELS, EXTRACTOR = load_models(
                PRETRAINED_MODEL_PATH, PRETRAINED_EXTRACTOR_PATH
            )
            print("âœ“ Pretrained model loaded")
            return MODELS, EXTRACTOR, {}
        else:
            print("âš ï¸ Pretrained model not found, proceeding with training...")
            USE_PRETRAINED_MODEL = False

    # Display current settings
    if USE_EXPORTED_FEATURES:
        print("\nğŸ“¥ Mode: IMPORT (using exported features)")
        print(f"   Path: {EXPORTED_FEATURES_PATH}")
    else:
        print("\nğŸ”„ Mode: EXTRACT (computing features from raw data)")
        if EXPORT_FEATURES:
            print(f"   Export: Enabled (name: {EXPORT_NAME or 'auto-generated'})")
        else:
            print("   Export: Disabled")

    # CONFIGãƒ‘ã‚¹ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰
    print("\nLoading data...")
    print(f"  Train data: {CONFIG['train_path']}")
    print(f"  Demographics: {CONFIG['train_demographics_path']}")

    try:
        train_df = pd.read_csv(CONFIG["train_path"])
        demo_df = pd.read_csv(CONFIG["train_demographics_path"])
    except FileNotFoundError as e:
        print(f"\nâš ï¸ Error: {e}")
        print("\nPlease check your data paths in CONFIG:")
        print("  - For Kaggle: Use /kaggle/input/... paths")
        print("  - For Local: Update paths to your data directory")
        raise

    # ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    train_df = train_df[train_df["behavior"] == "Performs gesture"].copy()

    print(
        f"Loaded {len(train_df)} samples from {train_df['sequence_id'].nunique()} sequences"
    )

    # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    sequences = []
    demographics = []
    labels = []
    subjects = []

    for seq_id in train_df["sequence_id"].unique():
        seq_data = train_df[train_df["sequence_id"] == seq_id]
        subject_id = seq_data["subject"].iloc[0]

        sequences.append(seq_data)
        demographics.append(demo_df[demo_df["subject"] == subject_id])
        labels.append(GESTURE_MAPPER[seq_data["gesture"].iloc[0]])
        subjects.append(subject_id)

    labels = np.array(labels)
    subjects = np.array(subjects)

    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ¸ˆã¿ç‰¹å¾´é‡ã®ãƒã‚§ãƒƒã‚¯ã¨èª­ã¿è¾¼ã¿
    import_path = EXPORTED_FEATURES_PATH
    use_precomputed = False
    X_all = None
    temp_extractor = None

    if USE_EXPORTED_FEATURES and EXPORTED_FEATURES_PATH:
        # Check if path exists (both Kaggle and local)
        import_path_obj = Path(import_path)
        path_exists = False

        # For Kaggle: just check if the path exists as-is
        if IS_KAGGLE_ENV:
            if import_path_obj.exists():
                path_exists = True
            else:
                print(f"âš ï¸ Warning: Export not found at {EXPORTED_FEATURES_PATH}")
                print("  Will extract features from raw data instead.")
        else:
            # For local: try adjusting the path if needed
            if not import_path_obj.exists():
                export_name = Path(EXPORTED_FEATURES_PATH).name
                local_path = EXPORT_DIR / export_name
                if local_path.exists():
                    import_path = str(local_path)
                    path_exists = True
                    print(
                        f"ğŸ“‚ Adjusted import path for local environment: {import_path}"
                    )
                else:
                    print(f"âš ï¸ Warning: Export not found at {EXPORTED_FEATURES_PATH}")
            else:
                path_exists = True

        if path_exists and import_path and Path(import_path).exists():
            print("ğŸ“¥ Loading exported raw features...")
            # Note: We load features but will re-fit scalers/PCA per fold
            X_all, loaded_labels, loaded_subjects, extractor_state = (
                FeatureExporter.import_features(import_path)
            )
            # extractor_stateã‹ã‚‰å®Ÿéš›ã®FeatureExtractorã‚’å¾©å…ƒ
            temp_extractor = FeatureExtractor(CONFIG)
            if isinstance(extractor_state, dict):
                temp_extractor.feature_names = extractor_state.get("feature_names", [])
                temp_extractor.is_fitted = True
            else:
                # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚
                temp_extractor = extractor_state
            use_precomputed = True
            print(f"  Raw features loaded! Shape: {X_all.shape}")
            # Verify the loaded data matches
            if len(loaded_labels) != len(labels):
                print(
                    f"âš ï¸ Warning: Loaded labels count ({len(loaded_labels)}) doesn't match current ({len(labels)})"
                )
                use_precomputed = False
                X_all = None
                temp_extractor = None

    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ¸ˆã¿ç‰¹å¾´é‡ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ã¿ã€æ–°è¦ã«æŠ½å‡º
    if not use_precomputed:
        # ç‰¹å¾´é‡ã‚’ä¸€æ‹¬ã§æŠ½å‡ºï¼ˆCVã®å‰ã«å®Ÿè¡Œï¼‰
        print("ğŸ“Š Extracting features for all sequences...")
        print(f"  Total sequences: {len(sequences)}")

        # ä¸€æ™‚çš„ãªextractorã‚’ä½œæˆã—ã¦ç‰¹å¾´é‡ã‚’æŠ½å‡ºï¼ˆPCAãªã—ã€Scalerãªã—ï¼‰
        temp_extractor = FeatureExtractor(CONFIG)
        temp_extractor.config["tof_use_pca"] = False  # ä¸€æ—¦PCAãªã—ã§æŠ½å‡º

        # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã¯ä¸¦åˆ—å‡¦ç†ã‚’ä½¿ç”¨
        if not IS_KAGGLE_ENV and USE_PARALLEL:
            from multiprocessing import Pool, cpu_count

            # N_JOBSãŒ-1ã®å ´åˆã¯å…¨ã‚³ã‚¢ä½¿ç”¨
            n_workers = cpu_count() if N_JOBS == -1 else N_JOBS
            print(f"  Using parallel processing with {n_workers} workers...")
            print(f"  Available CPU cores: {cpu_count()}")

            # ä¸¦åˆ—å‡¦ç†ç”¨ã®å¼•æ•°ã‚’æº–å‚™
            parallel_args = [
                (temp_extractor, seq_df, demo_df)
                for seq_df, demo_df in zip(sequences, demographics)
            ]

            # ä¸¦åˆ—å‡¦ç†ã‚’å®Ÿè¡Œ
            with Pool(processes=n_workers) as pool:
                all_features = []
                for i, features in enumerate(
                    pool.imap(extract_features_parallel, parallel_args, chunksize=10)
                ):
                    if i % 500 == 0:
                        print(f"  Processing sequence {i}/{len(sequences)}...")
                    all_features.append(features)
        else:
            # Kaggleç’°å¢ƒã¾ãŸã¯ä¸¦åˆ—å‡¦ç†ç„¡åŠ¹æ™‚ã¯é€æ¬¡å‡¦ç†
            if IS_KAGGLE_ENV:
                print("  Using sequential processing (Kaggle environment)...")
            else:
                print("  Using sequential processing (parallel disabled)...")

            all_features = []
            for i, (seq_df, demo_df) in enumerate(zip(sequences, demographics)):
                if i % 500 == 0:
                    print(f"  Processing sequence {i}/{len(sequences)}...")
                features = temp_extractor.extract_features(seq_df, demo_df)
                all_features.append(features)

        # å…¨ç‰¹å¾´é‡ã‚’çµåˆ
        X_all = pd.concat(all_features, ignore_index=True)
        print(f"âœ“ Features extracted: {X_all.shape}")

    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹å ´åˆã¯ã“ã“ã§ä¿å­˜
    if EXPORT_FEATURES and not USE_EXPORTED_FEATURES:
        print("ğŸ’¾ Exporting raw features for future use...")
        export_path = FeatureExporter.export_features(
            X_all, temp_extractor, labels, subjects, EXPORT_NAME
        )
        print(f"âœ“ Features exported to: {export_path}")
        print("ğŸ“ To use these features in the future, set:")
        print("   USE_EXPORTED_FEATURES = True")
        if IS_KAGGLE_ENV:
            print(f'   EXPORTED_FEATURES_PATH = "./{export_path.name}"')
        else:
            print(f'   EXPORTED_FEATURES_PATH = "{export_path}"')

    # Cross-validation setup
    print("Starting cross-validation...")
    cv = StratifiedGroupKFold(
        n_splits=CONFIG["n_folds"], shuffle=True, random_state=CONFIG["random_state"]
    )

    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹
    models, fold_artifacts, start_fold = load_checkpoint()
    if models is None:
        models = []
        fold_artifacts = []
        start_fold = 0

    oof_predictions = np.zeros(len(labels))
    cv_scores = []
    binary_f1_scores = []
    macro_f1_scores = []

    # Store extractor from first fold for later use
    final_extractor = None

    # Keep the extractor for later use
    # temp_extractorãŒNoneã®å ´åˆã¯æ–°ã—ãä½œæˆ
    if temp_extractor is None:
        temp_extractor = FeatureExtractor(CONFIG)
    extractor = temp_extractor

    # Cross-validation loop
    for fold, (train_idx, val_idx) in enumerate(cv.split(labels, labels, subjects)):
        # æ—¢ã«å‡¦ç†æ¸ˆã¿ã®foldã¯ã‚¹ã‚­ãƒƒãƒ—
        if fold < start_fold:
            continue

        print(f"--- Fold {fold + 1}/{CONFIG['n_folds']} ---")

        # ã“ã®foldã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ï¼ˆäº‹å‰ã«æŠ½å‡ºã—ãŸç‰¹å¾´é‡ã‚’ä½¿ç”¨ï¼‰
        y_train = labels[train_idx]
        y_val = labels[val_idx]

        print(f"Train: {len(train_idx)} samples, Val: {len(val_idx)} samples")

        # Scalerã‚’foldå†…ã§fitï¼ˆCVãƒªãƒ¼ã‚¯ã‚’é˜²ããŸã‚ï¼‰
        if use_precomputed and X_all is not None:
            # If using precomputed features, we still need to fit scaler per fold
            print("  Using precomputed raw features, fitting scaler for this fold...")
            X_train_raw = X_all.iloc[train_idx]
            X_val_raw = X_all.iloc[val_idx]

            # Fit scaler on train data only
            if CONFIG["robust_scaler"]:
                scaler = RobustScaler()
            else:
                scaler = StandardScaler()

            X_train = pd.DataFrame(
                scaler.fit_transform(X_train_raw),
                columns=X_train_raw.columns,
                index=X_train_raw.index,
            )
            X_val = pd.DataFrame(
                scaler.transform(X_val_raw),
                columns=X_val_raw.columns,
                index=X_val_raw.index,
            )

            if hasattr(extractor, "scaler"):
                extractor.scaler = scaler
            if hasattr(extractor, "feature_names"):
                extractor.feature_names = list(X_train.columns)
            if hasattr(extractor, "is_fitted"):
                extractor.is_fitted = True
        else:
            # æ–°è¦ã«æŠ½å‡ºã—ãŸç‰¹å¾´é‡ã‚’ä½¿ç”¨
            print("  Using newly extracted features, fitting scaler for this fold...")
            X_train_raw = X_all.iloc[train_idx]
            X_val_raw = X_all.iloc[val_idx]

            # Fit scaler on train data only
            if CONFIG["robust_scaler"]:
                scaler = RobustScaler()
            else:
                scaler = StandardScaler()

            X_train = pd.DataFrame(
                scaler.fit_transform(X_train_raw),
                columns=X_train_raw.columns,
                index=X_train_raw.index,
            )
            X_val = pd.DataFrame(
                scaler.transform(X_val_raw),
                columns=X_val_raw.columns,
                index=X_val_raw.index,
            )

        # Store first fold's extractor for later use
        if fold == 0:
            final_extractor = extractor

            # Export features if requested (only on first fold)
            if EXPORT_FEATURES and not use_precomputed:
                print("\nğŸ’¾ Exporting features for future use...")
                # Combine train and val for export
                X_all = pd.concat([X_train, X_val])
                all_labels = np.concatenate([y_train, y_val])
                all_subjects = np.concatenate([subjects[train_idx], subjects[val_idx]])

                export_path = FeatureExporter.export_features(
                    X_all, extractor, all_labels, all_subjects, EXPORT_NAME
                )
                print(f"âœ“ Features exported to: {export_path}")
                print("\nğŸ“ To use these features in the future, set:")
                print("   USE_EXPORTED_FEATURES = True")
                if IS_KAGGLE_ENV:
                    print(f'   EXPORTED_FEATURES_PATH = "./{export_path.name}"')
                else:
                    print(f'   EXPORTED_FEATURES_PATH = "{export_path}"')

        print(f"  Train features shape: {X_train.shape}")
        print(f"  Val features shape: {X_val.shape}")

        # Configure XGBoost parameters based on environment
        xgb_params = CONFIG["xgb_params"].copy()

        # GPU acceleration settings - è‡ªå‹•æ¤œå‡º
        try:
            import torch

            if torch.cuda.is_available():
                xgb_params["tree_method"] = "gpu_hist"
                xgb_params["device"] = "cuda:0"
                xgb_params.pop("gpu_id", None)
                print("  Using GPU acceleration (CUDA)")
            else:
                xgb_params["tree_method"] = "hist"
                xgb_params["device"] = "cpu"
                xgb_params.pop("gpu_id", None)
                print("  Using CPU")
        except ImportError:
            # torchãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆã¯CPUã‚’ä½¿ç”¨
            xgb_params["tree_method"] = "hist"
            xgb_params["device"] = "cpu"
            xgb_params.pop("gpu_id", None)
            print("  Using CPU (torch not installed)")

        # XGBoostã‚’è¨“ç·´
        model = xgb.XGBClassifier(**xgb_params)

        model.fit(
            X_train,
            y_train,
            eval_set=[(X_val, y_val)],
            verbose=100,
        )

        models.append(model)

        # fold artifactsã‚’ä¿å­˜
        fold_artifacts.append(
            {"feature_names": list(X_train_raw.columns), "scaler": scaler}
        )

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜
        if USE_CHECKPOINT and (fold + 1) % CHECKPOINT_INTERVAL == 0:
            save_checkpoint(
                fold, model, list(X_train_raw.columns), scaler, fold_artifacts
            )

        # Predictions
        val_preds = model.predict(X_val)
        oof_predictions[val_idx] = val_preds

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—
        binary_f1 = f1_score(
            np.where(y_val <= 7, 1, 0),
            np.where(val_preds <= 7, 1, 0),
            zero_division=0.0,
        )

        macro_f1 = f1_score(
            np.where(y_val <= 7, y_val, 99),
            np.where(val_preds <= 7, val_preds, 99),
            average="macro",
            zero_division=0.0,
        )

        score = 0.5 * (binary_f1 + macro_f1)
        cv_scores.append(score)
        binary_f1_scores.append(binary_f1)
        macro_f1_scores.append(macro_f1)

        print(
            f"Fold {fold + 1} - Binary F1: {binary_f1:.4f}, Macro F1: {macro_f1:.4f}, Score: {score:.4f}"
        )

    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å‰Šé™¤
    if USE_CHECKPOINT:
        remove_checkpoints()

    # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«ä¿å­˜
    MODELS = models
    EXTRACTOR = final_extractor if final_extractor else extractor
    FOLD_ARTIFACTS = fold_artifacts

    print("" + "=" * 70)
    print("CROSS-VALIDATION RESULTS")
    print("=" * 70)

    # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å¹³å‡ã¨æ¨™æº–åå·®ã‚’è¨ˆç®—
    mean_score = np.mean(cv_scores)
    std_score = np.std(cv_scores)
    mean_binary_f1 = np.mean(binary_f1_scores)
    std_binary_f1 = np.std(binary_f1_scores)
    mean_macro_f1 = np.mean(macro_f1_scores)
    std_macro_f1 = np.std(macro_f1_scores)

    print(f"Binary F1: {mean_binary_f1:.4f} Â± {std_binary_f1:.4f}")
    print(f"Macro F1:  {mean_macro_f1:.4f} Â± {std_macro_f1:.4f}")
    print(f"CV Score:  {mean_score:.4f} Â± {std_score:.4f}")
    print(f"Fold scores: {cv_scores}")

    # Feature importance (average across folds)
    if final_extractor and final_extractor.feature_names:
        feature_importance = pd.DataFrame(
            {
                "feature": final_extractor.feature_names,
                "importance": np.mean([m.feature_importances_ for m in models], axis=0),
            }
        ).sort_values("importance", ascending=False)

        print("\nTop 20 Most Important Features:")
        print(feature_importance.head(20))

    # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    if EXPORT_TRAINED_MODEL:
        save_models(models, final_extractor, fold_artifacts)

    # çµæœã‚’ã¾ã¨ã‚ã¦è¿”ã™
    metrics = {
        "mean_score": mean_score,
        "std_score": std_score,
        "mean_binary_f1": mean_binary_f1,
        "std_binary_f1": std_binary_f1,
        "mean_macro_f1": mean_macro_f1,
        "std_macro_f1": std_macro_f1,
        "cv_scores": cv_scores,
        "binary_f1_scores": binary_f1_scores,
        "macro_f1_scores": macro_f1_scores,
    }

    return models, final_extractor, metrics


# ========================================
# ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨èª­ã¿è¾¼ã¿
# ========================================


def save_models(models, extractor, fold_artifacts):
    """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨ã‚¨ã‚¯ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã‚’ä¿å­˜"""
    import pickle
    from pathlib import Path

    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    model_export_dir = Path("trained_models")
    model_export_dir.mkdir(exist_ok=True)

    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    model_file = model_export_dir / "models.pkl"
    with open(model_file, "wb") as f:
        pickle.dump(models, f)
    print(f"âœ“ Models saved to: {model_file}")

    # ã‚¨ã‚¯ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã‚’ä¿å­˜
    extractor_file = model_export_dir / "extractor.pkl"
    with open(extractor_file, "wb") as f:
        pickle.dump(extractor, f)
    print(f"âœ“ Extractor saved to: {extractor_file}")

    # fold artifactsã‚’ä¿å­˜
    artifacts_file = model_export_dir / "fold_artifacts.pkl"
    with open(artifacts_file, "wb") as f:
        pickle.dump(fold_artifacts, f)
    print(f"âœ“ Fold artifacts saved to: {artifacts_file}")

    print(f"\nğŸ“¦ All models exported to: {model_export_dir}/")
    print("To use these models for inference, set:")
    print("  USE_PRETRAINED_MODEL = True")
    print(f'  PRETRAINED_MODEL_PATH = "{model_file}"')
    print(f'  PRETRAINED_EXTRACTOR_PATH = "{extractor_file}"')


# ====================================================================================================
# INFERENCE
# ====================================================================================================

# Global variables for models
MODELS = None
EXTRACTOR = None


def load_models(
    model_path: str = None, extractor_path: str = None, artifacts_path: str = None
):
    """
    äº‹å‰ã«ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¨extractorã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚
    è©•ä¾¡ã‚µãƒ¼ãƒãƒ¼ã§ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’é˜²ããŸã‚ã€å­¦ç¿’ã¯è¡Œã‚ãªã„ã€‚
    """
    global MODELS, EXTRACTOR, FOLD_ARTIFACTS
    global PRETRAINED_MODEL_PATH, PRETRAINED_EXTRACTOR_PATH, PRETRAINED_ARTIFACTS_PATH
    import os

    # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
    if model_path is None and PRETRAINED_MODEL_PATH is not None:
        model_path = PRETRAINED_MODEL_PATH
    if extractor_path is None and PRETRAINED_EXTRACTOR_PATH is not None:
        extractor_path = PRETRAINED_EXTRACTOR_PATH
    if artifacts_path is None and PRETRAINED_ARTIFACTS_PATH is not None:
        artifacts_path = PRETRAINED_ARTIFACTS_PATH

    # ã¾ãšã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å„ªå…ˆ
    if model_path is None and os.path.exists("models.pkl"):
        model_path = "models.pkl"
    if extractor_path is None and os.path.exists("extractor.pkl"):
        extractor_path = "extractor.pkl"
    if artifacts_path is None and os.path.exists("fold_artifacts.pkl"):
        artifacts_path = "fold_artifacts.pkl"

    # trained_modelsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚ãƒã‚§ãƒƒã‚¯
    if model_path is None and os.path.exists("trained_models/models.pkl"):
        model_path = "trained_models/models.pkl"
    if extractor_path is None and os.path.exists("trained_models/extractor.pkl"):
        extractor_path = "trained_models/extractor.pkl"
    if artifacts_path is None and os.path.exists("trained_models/fold_artifacts.pkl"):
        artifacts_path = "trained_models/fold_artifacts.pkl"

    # ãã‚Œã§ã‚‚ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ã‚’æ¢ã™
    if model_path is None:
        if IS_KAGGLE_ENV:
            # Kaggleç’°å¢ƒã§ã¯/kaggle/input/ã‹ã‚‰èª­ã¿è¾¼ã‚€
            model_path = "/kaggle/input/cmi-models/models.pkl"
            extractor_path = "/kaggle/input/cmi-models/extractor.pkl"
            artifacts_path = "/kaggle/input/cmi-models/fold_artifacts.pkl"
        else:
            # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã¯exported_featuresã‹ã‚‰æœ€æ–°ã®ã‚‚ã®ã‚’æ¢ã™
            exports = sorted(EXPORT_DIR.glob("features_*"))
            if exports:
                latest_export = exports[-1]
                model_path = latest_export / "models_5fold.pkl"
                extractor_path = latest_export / "extractor.pkl"
                artifacts_path = latest_export / "fold_artifacts.pkl"
            else:
                raise FileNotFoundError(
                    "No saved models found. Please train models first."
                )

    # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    print(f"Loading models from: {model_path}")
    with open(model_path, "rb") as f:
        MODELS = pickle.load(f)

    # Extractorã‚’ãƒ­ãƒ¼ãƒ‰
    if extractor_path and Path(extractor_path).exists():
        print(f"Loading extractor from: {extractor_path}")
        with open(extractor_path, "rb") as f:
            EXTRACTOR = pickle.load(f)
    else:
        # ExtractorãŒä¿å­˜ã•ã‚Œã¦ã„ãªã„å ´åˆã¯æ–°è¦ä½œæˆï¼ˆç‰¹å¾´é‡æŠ½å‡ºç”¨ï¼‰
        print("Creating new extractor...")
        EXTRACTOR = FeatureExtractor(CONFIG)
        # Note: ã“ã®å ´åˆã€fitæ¸ˆã¿ã§ãªã„ã®ã§äº‹å‰ã«å­¦ç¿’ãŒå¿…è¦

    # fold artifactsã®ãƒ­ãƒ¼ãƒ‰
    FOLD_ARTIFACTS = None
    if artifacts_path and Path(artifacts_path).exists():
        with open(artifacts_path, "rb") as f:
            FOLD_ARTIFACTS = pickle.load(f)
        print(f"âœ“ Loaded fold artifacts: {len(FOLD_ARTIFACTS)} folds")
    else:
        print("âš ï¸ Fold artifacts not found â€” per-fold scaling will be inconsistent")

    print(f"âœ“ Loaded {len(MODELS)} models")
    return MODELS, EXTRACTOR


def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:
    """
    Prediction function for Kaggle inference server.
    æ”¹ä¿®ï¼šfoldæ¯ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ä½¿ç”¨ã—ã¦æ­£ã—ãã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€‚
    """
    global MODELS, EXTRACTOR, FOLD_ARTIFACTS

    # å¿…è¦ã«å¿œã˜ã¦ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ï¼ˆãƒ­ãƒ¼ãƒ‰ã®ã¿ï¼‰
    if MODELS is None or EXTRACTOR is None:
        print("Loading pre-trained models...")
        try:
            MODELS, EXTRACTOR = load_models()
        except FileNotFoundError as e:
            print(f"Error: {e}")
            print(
                "Falling back to training models (this may timeout on evaluation server)..."
            )
            MODELS, EXTRACTOR, _ = train_models()

    # pandasã«å¤‰æ›
    seq_df = sequence.to_pandas()
    demo_df = demographics.to_pandas()

    # handednessã®å¤‰æ›ï¼ˆR/Læ–‡å­—åˆ—ã‚’1/0ã«ï¼‰
    if "handedness" in demo_df.columns:
        demo_df = demo_df.copy()
        demo_df["handedness"] = demo_df["handedness"].apply(_to01_handedness)

    # ç”Ÿã®ç‰¹å¾´é‡ã‚’æŠ½å‡ºï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰ï¼‰
    if hasattr(EXTRACTOR, "_extract_features_raw"):
        X_raw = EXTRACTOR._extract_features_raw(seq_df, demo_df)
    else:
        # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚
        features = EXTRACTOR.extract_features(seq_df, demo_df)
        X_raw = features

    # ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰äºˆæ¸¬ã‚’å–å¾—
    predictions = []

    if FOLD_ARTIFACTS is not None and len(FOLD_ARTIFACTS) == len(MODELS):
        # foldæ¯ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ä½¿ç”¨ï¼ˆæ­£ã—ã„æ–¹æ³•ï¼‰
        for model, art in zip(MODELS, FOLD_ARTIFACTS):
            # è¨“ç·´æ™‚ã®ç‰¹å¾´é‡åã«åˆã‚ã›ã‚‹
            feature_names = art["feature_names"]

            # X_rawã‹ã‚‰å¿…è¦ãªç‰¹å¾´é‡ã®ã¿ã‚’é¸æŠï¼ˆå­˜åœ¨ã—ãªã„ç‰¹å¾´é‡ã¯0ã§åŸ‹ã‚ã‚‹ï¼‰
            X_selected = pd.DataFrame()
            for col in feature_names:
                if col in X_raw.columns:
                    X_selected[col] = X_raw[col]
                else:
                    # è¨“ç·´æ™‚ã«ã‚ã£ãŸãŒæ¨è«–æ™‚ã«ãªã„ç‰¹å¾´é‡ã¯0ã§åŸ‹ã‚ã‚‹
                    X_selected[col] = 0

            # ã“ã®foldã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’é©ç”¨
            X_scaled = pd.DataFrame(
                art["scaler"].transform(X_selected),
                columns=feature_names,
                index=X_raw.index,
            )

            # äºˆæ¸¬
            pred = model.predict_proba(X_scaled)[0]
            predictions.append(pred)
    else:
        # fold artifactsãŒãªã„å ´åˆã¯å¾“æ¥ã®æ–¹æ³•ï¼ˆéæ¨å¥¨ï¼‰
        print("âš ï¸ Warning: Using fallback prediction without fold-specific scalers")

        # extractorã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ä½¿ç”¨ï¼ˆfold 0ã®ã¿ï¼‰
        if hasattr(EXTRACTOR, "scaler") and EXTRACTOR.scaler is not None:
            X_scaled = pd.DataFrame(
                EXTRACTOR.scaler.transform(X_raw),
                columns=X_raw.columns,
                index=X_raw.index,
            )
        else:
            # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ãŒãªã„å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
            X_scaled = X_raw

        for model in MODELS:
            pred = model.predict_proba(X_scaled)[0]
            predictions.append(pred)

    # Average predictions
    avg_pred = np.mean(predictions, axis=0)
    final_class = np.argmax(avg_pred)

    # ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼åã«å¤‰æ›
    gesture_name = REVERSE_GESTURE_MAPPER[final_class]

    return gesture_name


# ====================================================================================================
# MAIN EXECUTION
# ====================================================================================================

if __name__ == "__main__":
    # åˆ©ç”¨å¯èƒ½ãªã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦è¡¨ç¤º
    print("\n" + "=" * 70)
    print("AVAILABLE FEATURE EXPORTS")
    print("=" * 70)

    # Kaggleç’°å¢ƒã§ã¯ã€inputãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚ãƒã‚§ãƒƒã‚¯
    if IS_KAGGLE_ENV and USE_EXPORTED_FEATURES and EXPORTED_FEATURES_PATH:
        kaggle_path = Path(EXPORTED_FEATURES_PATH)
        if kaggle_path.exists():
            print(f"âœ“ Found Kaggle dataset at: {EXPORTED_FEATURES_PATH}")
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
            metadata_file = kaggle_path / "metadata.json"
            if metadata_file.exists():
                with open(metadata_file, "r") as f:
                    meta = json.load(f)
                print(
                    f"  Samples: {meta.get('n_samples', '?')}, Features: {meta.get('n_features', '?')}"
                )
                print("ğŸ“Š Will use these exported features for training.")
            else:
                print("  âš ï¸ Warning: metadata.json not found in dataset")
        else:
            print(f"âš ï¸ Dataset not found at: {EXPORTED_FEATURES_PATH}")
            print("  Will extract features from raw data instead.")
    elif EXPORT_DIR.exists():
        exports = sorted(EXPORT_DIR.glob("features_*"))
        if exports:
            print("\nFound exported features:")
            for exp in exports[-3:]:  # Show last 3 exports
                if exp.is_dir():
                    metadata_file = exp / "metadata.json"
                    if metadata_file.exists():
                        with open(metadata_file, "r") as f:
                            meta = json.load(f)
                        print(f"  ğŸ“ {exp.name}")
                        print(
                            f"     Samples: {meta.get('n_samples', '?')}, Features: {meta.get('n_features', '?')}"
                        )
            print("\nğŸ’¡ To use exported features, set:")
            print("   USE_EXPORTED_FEATURES = True")
            if IS_KAGGLE_ENV:
                print(f'   EXPORTED_FEATURES_PATH = "./{exports[-1].name}"')
            else:
                print(f'   EXPORTED_FEATURES_PATH = "{exports[-1]}"')
        else:
            print("No exported features found. First run will extract and export.")

    # ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
    MODELS, EXTRACTOR, metrics = train_models()
    print("âœ“ Models trained successfully")
    print(
        f"   Binary F1: {metrics['mean_binary_f1']:.4f} Â± {metrics['std_binary_f1']:.4f}"
    )
    print(
        f"   Macro F1:  {metrics['mean_macro_f1']:.4f} Â± {metrics['std_macro_f1']:.4f}"
    )
    print(f"   CV Score:  {metrics['mean_score']:.4f} Â± {metrics['std_score']:.4f}")

    # Show performance summary
    print("\n" + "=" * 70)
    print("PERFORMANCE SUMMARY")
    print("=" * 70)
    if USE_EXPORTED_FEATURES:
        print("âœ… Used exported features - execution time: ~30 seconds")
    else:
        print("âœ… Extracted features from raw data - execution time: ~400 seconds")
        if EXPORT_FEATURES:
            print("   Features have been exported for future use.")
            print("   Next run will be 10x faster with exported features!")

    # Environment-specific completion message
    print("\n" + "=" * 70)
    if IS_KAGGLE_ENV:
        print("KAGGLE SUBMISSION READY")
    else:
        print("LOCAL EXECUTION COMPLETE")
        print(
            "To use in Kaggle: Copy exported features to Kaggle and set IS_KAGGLE_ENV = True"
        )
    print("=" * 70)

    # Kaggleæ¨è«–ã‚µãƒ¼ãƒãƒ¼ã‚’åˆæœŸåŒ–
    if IS_KAGGLE_ENV:
        print("Initializing Kaggle inference server...")

        try:
            from kaggle_evaluation.cmi_inference_server import CMIInferenceServer

            inference_server = CMIInferenceServer(predict)
            print("âœ“ Inference server created")

            # ç’°å¢ƒã«å¿œã˜ã¦é©åˆ‡ãªãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã™
            if os.getenv("KAGGLE_IS_COMPETITION_RERUN"):
                # ç«¶æŠ€ç’°å¢ƒ: serve()ã‚’ä½¿ç”¨
                print("Running in competition environment...")
                inference_server.serve()
                print("âœ“ Submission complete!")
            else:
                # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆç’°å¢ƒ: run_local_gateway()ã‚’ä½¿ç”¨
                print("Running in local testing mode...")
                print("Generating submission.parquet from test data...")

                # test.csvãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯å‡¦ç†
                test_path = (
                    "/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv"
                )
                test_demo_path = "/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv"

                if os.path.exists(test_path) and os.path.exists(test_demo_path):
                    inference_server.run_local_gateway(
                        data_paths=(test_path, test_demo_path)
                    )
                    print("âœ“ Inference complete!")
                    print("âœ“ submission.parquet has been generated")
                else:
                    print("âš ï¸ Test data not found, generating empty submission...")
                    # ç©ºã®submissionã‚’ç”Ÿæˆ
                    submission_df = pd.DataFrame({"sequence_id": [], "prediction": []})
                    submission_df.to_parquet("submission.parquet", index=False)
                    print("âœ“ Empty submission.parquet generated")

        except ImportError as e:
            print(f"âš ï¸ Kaggle evaluation module not available: {e}")
            print("Generating submission manually...")

            # Manual submission generation as fallback
            test_path = "/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv"
            test_demo_path = "/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv"

            if os.path.exists(test_path):
                test_df = pd.read_csv(test_path)
                test_demo_df = pd.read_csv(test_demo_path)

                # test.csvã‹ã‚‰ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªsequence_idã‚’å–å¾—
                test_sequences = test_df["sequence_id"].unique()
                print(f"Processing {len(test_sequences)} test sequences...")

                predictions = []
                for i, seq_id in enumerate(test_sequences):
                    if i % 100 == 0:
                        print(f"  Processing sequence {i}/{len(test_sequences)}...")

                    seq_data = test_df[test_df["sequence_id"] == seq_id]
                    seq_pl = pl.from_pandas(seq_data)

                    # subjectæƒ…å ±ã‚’å–å¾—
                    if "subject" in seq_data.columns:
                        subject_id = seq_data["subject"].iloc[0]
                        demo_data = test_demo_df[test_demo_df["subject"] == subject_id]
                    else:
                        # subjectãŒãªã„å ´åˆã¯ãƒ€ãƒŸãƒ¼ã®demographicsã‚’ä½œæˆ
                        demo_data = pd.DataFrame(
                            {"subject": [0], "age": [30], "handedness": ["R"]}
                        )

                    demo_pl = pl.from_pandas(demo_data)

                    # äºˆæ¸¬ã‚’å®Ÿè¡Œ
                    try:
                        pred = predict(seq_pl, demo_pl)
                    except Exception as e:
                        print(f"  Warning: Error predicting sequence {seq_id}: {e}")
                        pred = "Idle"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

                    predictions.append({"sequence_id": seq_id, "prediction": pred})

                # DataFrameã‚’ä½œæˆã—ã¦ä¿å­˜
                submission_df = pd.DataFrame(predictions)
                submission_df.to_parquet("submission.parquet", index=False)
                print(
                    f"âœ… Generated submission.parquet with {len(submission_df)} predictions"
                )
            else:
                print("âš ï¸ Test data not found. Creating empty submission...")
                submission_df = pd.DataFrame({"sequence_id": [], "prediction": []})
                submission_df.to_parquet("submission.parquet", index=False)
                print("âœ“ Empty submission.parquet generated")

    else:
        # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã®å‡¦ç†
        print("Local environment - skipping submission generation")
        print("To use for Kaggle submission:")
        print("1. Copy this entire script to a Kaggle notebook")
        print("2. Run all cells")
        print("3. submission.parquet will be generated automatically")
