# 問題点

- 「`✓ TensorFlow/Keras available` → `✓ All imports loaded successfully` → `Loading training data...`」の**まとまりが丸ごと 2 回**出ています。
  これは **DataLoader ワーカープロセスがあなたのノートブック（=モジュール）をインポートし直し、トップレベルの学習処理が再実行**された典型パターンです。
- `--- Torch Fold 1/5 ---` の直後にまた同じまとまりが始まっている点も、**DataLoader 起動タイミングで再実行された**ことを裏付けます。
- 20 分無音は**学習ループの途中で標準出力に何も出していない**ためです（`tqdm` の進捗はインタラクティブ TTY 向けなので、Kaggle/コンソール収集では沈黙に見えがち）。

---

## まず最小の修正（副作用少／効果大）

下記 3 点の変更で、「二重実行の排除」と「途中経過の可視化」をすぐ改善できます。

### 1) トップレベル実行を `if __name__ == "__main__":` でガード

学習・推論・サーバ起動など「実行系」のブロックを **関数 `main()` に入れて、末尾で `if __name__ == "__main__": main()` から呼ぶ**ようにします。
（**ワーカープロセスでは `__name__` が `"__mp_main__"`** になるため、トップレベルの再実行が止まります。）

```python
# ==== 追加：エントリポイント ====
def main():
    # ここに「=== REPLACE: Training block ... ===」以降の実行ブロック
    # （RUN_ANY_TRAINING 判定から、推論サーバ起動・ローカルゲートウェイ実行まで）
    # を丸ごと移動してください。

if __name__ == "__main__":
    main()
```

> 実装メモ
>
> - 既存の「定義群（クラス・関数）」はそのままトップレベルに置き、**「実行してしまう部分」だけ** `main()` に移します。
> - ノートブックでも有効です（メインプロセスでは実行され、ワーカーでは実行されません）。

---

### 2) Kaggle/ノートブックでは DataLoader を単一プロセスに

まず不具合切り分けのために **既定を `num_workers=0`** にします（必要ならあとで戻します）。また、**`persistent_workers=False`** にします。

```python
# DLConfig
class DLConfig:
    ...
    NUM_WORKERS = int(os.getenv("DL_NUM_WORKERS", "0"))   # 既定を 0 に
    ...

# DataLoader 生成部
dl_tr = torch.utils.data.DataLoader(
    ds_tr,
    batch_size=DLConfig.BATCH_SIZE,
    shuffle=True,
    num_workers=DLConfig.NUM_WORKERS,
    collate_fn=collate_batch,
    pin_memory=True,
    persistent_workers=False,                    # ← 明示的に False
    prefetch_factor=2 if DLConfig.NUM_WORKERS > 0 else None,
)

dl_va = torch.utils.data.DataLoader(
    ds_va,
    batch_size=DLConfig.BATCH_SIZE,
    shuffle=False,
    num_workers=DLConfig.NUM_WORKERS,
    collate_fn=collate_batch,
    pin_memory=True,
    persistent_workers=False,                    # ← 明示的に False
    prefetch_factor=2 if DLConfig.NUM_WORKERS > 0 else None,
)
```

> メモ：Linux では通常 `fork` ですが、Kaggle/Colab のノートブック環境では状況により `spawn` が使われることがあり、その際にトップレベル再実行が発生します。`num_workers=0` にしてまず挙動を安定させるのが安全です（速度最適化はその後で）。

---

### 3) 学習中の詳細ログを標準出力に定期出力（無音時間をなくす）

`set_postfix` だけでなく **明示的に `print(..., flush=True)`** します。
`LogConfig` に追加の間隔を設け、**バッチごと／N ステップごとの標準出力**を出します。また **特徴量抽出ループ**も細かく進捗を出します。

```python
# 追加：ログ間隔を設定
class LogConfig:
    LOG_EVERY_STEPS = int(os.getenv("LOG_EVERY_STEPS", "50"))   # 既存（tqdm側）
    PRINT_EVERY_STEPS = int(os.getenv("PRINT_EVERY_STEPS", "100"))  # NEW: 標準出力
    FEATURE_LOG_EVERY = int(os.getenv("FEATURE_LOG_EVERY", "200"))  # NEW: 特徴量抽出
    SAVE_JSONL = bool(int(os.getenv("SAVE_JSONL", "1")))
    OUT_DIR = os.path.join(Config.OUTPUT_PATH, "logs")
    os.makedirs(OUT_DIR, exist_ok=True)
```

**(a) Torch 学習ループ内：**

```python
USE_TQDM = bool(int(os.getenv("USE_TQDM", "0")))  # 既定: tqdmを使わない（無音対策）
iterator = tqdm(dl_tr, total=len(dl_tr), desc=f"[Torch] fold {fold} epoch {epoch+1}", leave=False) if USE_TQDM else dl_tr

for xb, mb, yb in iterator:
    ...
    nstep += 1

    # 既存の set_postfix はそのままでもOK（tqdm使用時）
    if USE_TQDM and (nstep % LogConfig.LOG_EVERY_STEPS == 0):
        lr = scheduler.get_last_lr()[0]
        iterator.set_postfix(loss=f"{running/nstep:.4f}", lr=f"{lr:.2e}")

    # NEW: 標準出力にも確実に吐く
    if (nstep % LogConfig.PRINT_EVERY_STEPS == 0) or (nstep == 1):
        lr = scheduler.get_last_lr()[0]
        print(
            f"[Torch] fold={fold} epoch={epoch+1}/{DLConfig.MAX_EPOCHS} "
            f"step={nstep}/{len(dl_tr)} loss={running/nstep:.4f} lr={lr:.2e} "
            f"max_mem={_gpu_mem_gb():.2f}GB",
            flush=True,
        )
```

**(b) 特徴量抽出ループ：**

```python
t0 = time.time()
for i, (sequence_id, sequence_data) in enumerate(train_sequences):
    ...
    done = i + 1
    if (done % LogConfig.FEATURE_LOG_EVERY == 0) or (done == 1) or (done == n_sequences):
        elapsed = time.time() - t0
        rate = done / max(elapsed, 1e-9)
        eta = (n_sequences - done) / max(rate, 1e-9)
        print(
            f"Processing sequence {done}/{n_sequences}  "
            f"{rate:.1f}/s  ETA {eta/60:.1f} min", flush=True
        )
```

---

## そのほか見つけた気になる点（改善提案）

1. **アンサンブル重みの妥当性チェックが Keras を無視**

   ```python
   assert EnsembleConfig.W_LGBM + EnsembleConfig.W_TORCH > 0
   ```

   だと LGBM/Torch を 0 にして Keras のみ 1.0 でも落ちます。
   **→ 修正：**

   ```python
   assert (EnsembleConfig.W_LGBM + EnsembleConfig.W_TORCH + EnsembleConfig.W_KERAS) > 0, \
          "Invalid ensemble weights (sum must be > 0)"
   ```

2. **TensorFlow の冗長な GPU プラグイン警告の抑制**（任意）
   先頭で下記を入れるとノイズが減ります（機能影響なし）。

   ```python
   os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "2")
   try:
       import absl.logging as absl_logging
       absl_logging.set_verbosity(absl_logging.ERROR)
   except Exception:
       pass
   ```

3. **Keras を使わない実行でも毎回 import している**（GPU プラグイン初期化が走りがち）
   可能なら **Keras/TensorFlow の import を遅延（必要時のみ）** にすると起動ノイズと初期化コストを減らせます。
   例：`train_keras_models` / Keras 推論ロード時に初めて import。

4. **`num_workers` を後から増やす場合の注意**
   `OneCycleLR(total_steps=MAX_EPOCHS * len(dl_tr))` を使っているので、**バッチサイズや `num_workers` 変更で `len(dl_tr)` が変わると再開時に不一致**が起こり得ます。再開（RESUME）する時は **学習条件を固定**しましょう。

5. **（任意の高速化）DL のフレーム特徴量を永続キャッシュ**
   いまは LRU メモリキャッシュ（400 件）で fold ごとに再計算が出ます。
   `DLConfig.FRAME_FEATURE_CACHE` を活用して **全シーケンスの frame_features を Parquet で一括保存 → 再利用**すると学習が安定して速くなります。

---

## 実装タスクリスト（コピペ可能な粒度）

**\[必須]**

- [ ] **エントリポイントのガード**：学習・推論実行ブロックを `main()` に入れて、末尾に

  ```python
  if __name__ == "__main__":
      main()
  ```

  を追加する（トップレベルの再実行を防止）。

- [ ] **DataLoader のワーカー停止**：`DLConfig.NUM_WORKERS` の既定を `0` に、`persistent_workers=False` にする。

  ```python
  class DLConfig:
      ...
      NUM_WORKERS = int(os.getenv("DL_NUM_WORKERS", "0"))
  ```

  DataLoader 生成時は `persistent_workers=False` を明示。

- [ ] **学習中の標準出力ログを追加**：
      `LogConfig` に `PRINT_EVERY_STEPS` を追加し、学習ループで

  ```python
  if (nstep % LogConfig.PRINT_EVERY_STEPS == 0) or (nstep == 1):
      print(f"...", flush=True)
  ```

  を挿入。`USE_TQDM=0` を既定にして無音時間を解消。

- [ ] **特徴量抽出の進捗ログを細かく**：
      ループに `FEATURE_LOG_EVERY`（既定 200）を追加し、ETA 付きで `print(..., flush=True)` する。

- [ ] **アンサンブル重みの assert 修正**：

  ```python
  assert (EnsembleConfig.W_LGBM + EnsembleConfig.W_TORCH + EnsembleConfig.W_KERAS) > 0
  ```

**\[推奨]**

- [ ] **TF のログ抑制**（冒頭に追加）

  ```python
  os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "2")
  try:
      import absl.logging as absl_logging
      absl_logging.set_verbosity(absl_logging.ERROR)
  except Exception:
      pass
  ```

- [ ] **Keras/TensorFlow の遅延 import**：Keras を実際に使うパス（学習/推論）でのみ import するよう整理。

- [ ] **（必要になったら）ワーカー再有効化**：
      まず安定化を確認後、`DL_NUM_WORKERS` を 2 などに戻す。戻す際は `if __name__ == "__main__":` ガードを残しつつ、`USE_TQDM` を 1 にするか、`PRINT_EVERY_STEPS` を小さくしてログを保つ。

---

これで、

- 学習ブロックの**二重起動は止まり**、
- 学習や特徴量抽出の**途中経過が数十秒おきに標準出力へ流れ**、
- ログの\*\*「20 分沈黙」問題が解消\*\*されます。

必要なら上記パッチを当てた小さな差分版もまとめます。
