{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMI BFRB Detection - IMU-only LightGBM Training\n",
    "\n",
    "This notebook trains the IMU-only LightGBM baseline model for BFRB detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from lightgbm import LGBMClassifier, log_evaluation, early_stopping\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print('✓ All imports loaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # Data paths for Kaggle environment\n",
    "    TRAIN_PATH = '/kaggle/input/cmi-detect-behavior-with-sensor-data/train.csv'\n",
    "    TRAIN_DEMOGRAPHICS_PATH = '/kaggle/input/cmi-detect-behavior-with-sensor-data/train_demographics.csv'\n",
    "    \n",
    "    # Training parameters\n",
    "    SEED = 42\n",
    "    N_FOLDS = 5\n",
    "    \n",
    "    # Feature columns\n",
    "    ACC_COLS = ['acc_x', 'acc_y', 'acc_z']\n",
    "    ROT_COLS = ['rot_w', 'rot_x', 'rot_y', 'rot_z']\n",
    "    \n",
    "    # LightGBM parameters\n",
    "    LGBM_PARAMS = {\n",
    "        'objective': 'multiclass',\n",
    "        'n_estimators': 1024,\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.025,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'n_jobs': -1,\n",
    "        'num_leaves': 20,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "        'subsample': 0.5,\n",
    "        'verbosity': -1,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Output path\n",
    "    OUTPUT_PATH = '/kaggle/working/'\n",
    "\n",
    "np.random.seed(Config.SEED)\n",
    "print('✓ Configuration loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gesture mapping\n",
    "GESTURE_MAPPER = {\n",
    "    'Above ear - pull hair': 0,\n",
    "    'Cheek - pinch skin': 1,\n",
    "    'Eyebrow - pull hair': 2,\n",
    "    'Eyelash - pull hair': 3,\n",
    "    'Forehead - pull hairline': 4,\n",
    "    'Forehead - scratch': 5,\n",
    "    'Neck - pinch skin': 6,\n",
    "    'Neck - scratch': 7,\n",
    "    'Drink from bottle/cup': 8,\n",
    "    'Feel around in tray and pull out an object': 9,\n",
    "    'Glasses on/off': 10,\n",
    "    'Pinch knee/leg skin': 11,\n",
    "    'Pull air toward your face': 12,\n",
    "    'Scratch knee/leg skin': 13,\n",
    "    'Text on phone': 14,\n",
    "    'Wave hello': 15,\n",
    "    'Write name in air': 16,\n",
    "    'Write name on leg': 17,\n",
    "}\n",
    "\n",
    "REVERSE_GESTURE_MAPPER = {v: k for k, v in GESTURE_MAPPER.items()}\n",
    "print(f'✓ Gesture mapping loaded ({len(GESTURE_MAPPER)} classes)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering functions\n",
    "def handle_quaternion_missing_values(rot_data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Handle missing values in quaternion data.\"\"\"\n",
    "    rot_cleaned = rot_data.copy()\n",
    "    \n",
    "    for i in range(len(rot_data)):\n",
    "        row = rot_data[i]\n",
    "        missing_count = np.isnan(row).sum()\n",
    "        \n",
    "        if missing_count == 0:\n",
    "            norm = np.linalg.norm(row)\n",
    "            if norm > 1e-8:\n",
    "                rot_cleaned[i] = row / norm\n",
    "            else:\n",
    "                rot_cleaned[i] = [1.0, 0.0, 0.0, 0.0]\n",
    "        elif missing_count == 1:\n",
    "            missing_idx = np.where(np.isnan(row))[0][0]\n",
    "            valid_values = row[~np.isnan(row)]\n",
    "            sum_squares = np.sum(valid_values**2)\n",
    "            if sum_squares <= 1.0:\n",
    "                missing_value = np.sqrt(max(0, 1.0 - sum_squares))\n",
    "                if i > 0 and not np.isnan(rot_cleaned[i-1, missing_idx]):\n",
    "                    if rot_cleaned[i-1, missing_idx] < 0:\n",
    "                        missing_value = -missing_value\n",
    "                rot_cleaned[i, missing_idx] = missing_value\n",
    "                rot_cleaned[i, ~np.isnan(row)] = valid_values\n",
    "            else:\n",
    "                rot_cleaned[i] = [1.0, 0.0, 0.0, 0.0]\n",
    "        else:\n",
    "            rot_cleaned[i] = [1.0, 0.0, 0.0, 0.0]\n",
    "    \n",
    "    return rot_cleaned\n",
    "\n",
    "def compute_world_acceleration(acc: np.ndarray, rot: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert acceleration from device to world coordinates.\"\"\"\n",
    "    try:\n",
    "        rot_scipy = rot[:, [1, 2, 3, 0]]  # Convert to scipy format\n",
    "        norms = np.linalg.norm(rot_scipy, axis=1)\n",
    "        if np.any(norms < 1e-8):\n",
    "            mask = norms < 1e-8\n",
    "            rot_scipy[mask] = [0.0, 0.0, 0.0, 1.0]\n",
    "        r = R.from_quat(rot_scipy)\n",
    "        acc_world = r.apply(acc)\n",
    "    except Exception:\n",
    "        acc_world = acc.copy()\n",
    "    return acc_world\n",
    "\n",
    "print('✓ Feature engineering functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_statistical_features(data: np.ndarray, prefix: str) -> dict:\n",
    "    \"\"\"Extract statistical features from 1D time series.\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Basic statistics\n",
    "    features[f'{prefix}_mean'] = np.mean(data)\n",
    "    features[f'{prefix}_std'] = np.std(data)\n",
    "    features[f'{prefix}_var'] = np.var(data)\n",
    "    features[f'{prefix}_min'] = np.min(data)\n",
    "    features[f'{prefix}_max'] = np.max(data)\n",
    "    features[f'{prefix}_median'] = np.median(data)\n",
    "    features[f'{prefix}_q25'] = np.percentile(data, 25)\n",
    "    features[f'{prefix}_q75'] = np.percentile(data, 75)\n",
    "    features[f'{prefix}_iqr'] = features[f'{prefix}_q75'] - features[f'{prefix}_q25']\n",
    "    features[f'{prefix}_range'] = features[f'{prefix}_max'] - features[f'{prefix}_min']\n",
    "    \n",
    "    # Boundary features\n",
    "    features[f'{prefix}_first'] = data[0] if len(data) > 0 else 0\n",
    "    features[f'{prefix}_last'] = data[-1] if len(data) > 0 else 0\n",
    "    features[f'{prefix}_delta'] = features[f'{prefix}_last'] - features[f'{prefix}_first']\n",
    "    \n",
    "    # Higher order moments\n",
    "    if len(data) > 1 and np.std(data) > 1e-8:\n",
    "        features[f'{prefix}_skew'] = pd.Series(data).skew()\n",
    "        features[f'{prefix}_kurt'] = pd.Series(data).kurtosis()\n",
    "    else:\n",
    "        features[f'{prefix}_skew'] = 0\n",
    "        features[f'{prefix}_kurt'] = 0\n",
    "    \n",
    "    # Differential features\n",
    "    if len(data) > 1:\n",
    "        diff_data = np.diff(data)\n",
    "        features[f'{prefix}_diff_mean'] = np.mean(diff_data)\n",
    "        features[f'{prefix}_diff_std'] = np.std(diff_data)\n",
    "        features[f'{prefix}_n_changes'] = np.sum(np.abs(diff_data) > np.std(data) * 0.1)\n",
    "    else:\n",
    "        features[f'{prefix}_diff_mean'] = 0\n",
    "        features[f'{prefix}_diff_std'] = 0\n",
    "        features[f'{prefix}_n_changes'] = 0\n",
    "    \n",
    "    # Segment features (3 segments)\n",
    "    seq_len = len(data)\n",
    "    if seq_len >= 9:\n",
    "        seg_size = seq_len // 3\n",
    "        for i in range(3):\n",
    "            start_idx = i * seg_size\n",
    "            end_idx = (i + 1) * seg_size if i < 2 else seq_len\n",
    "            segment = data[start_idx:end_idx]\n",
    "            features[f'{prefix}_seg{i+1}_mean'] = np.mean(segment)\n",
    "            features[f'{prefix}_seg{i+1}_std'] = np.std(segment)\n",
    "        # Segment transitions\n",
    "        features[f'{prefix}_seg1_to_seg2'] = features[f'{prefix}_seg2_mean'] - features[f'{prefix}_seg1_mean']\n",
    "        features[f'{prefix}_seg2_to_seg3'] = features[f'{prefix}_seg3_mean'] - features[f'{prefix}_seg2_mean']\n",
    "    else:\n",
    "        for i in range(3):\n",
    "            features[f'{prefix}_seg{i+1}_mean'] = features[f'{prefix}_mean']\n",
    "            features[f'{prefix}_seg{i+1}_std'] = features[f'{prefix}_std']\n",
    "        features[f'{prefix}_seg1_to_seg2'] = 0\n",
    "        features[f'{prefix}_seg2_to_seg3'] = 0\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(sequence: pl.DataFrame, demographics: pl.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Extract features from IMU sequence.\"\"\"\n",
    "    # Convert to pandas\n",
    "    seq_df = sequence.to_pandas()\n",
    "    demo_df = demographics.to_pandas()\n",
    "    \n",
    "    # Handle missing values\n",
    "    acc_data = seq_df[Config.ACC_COLS].copy()\n",
    "    acc_data = acc_data.ffill().bfill().fillna(0)\n",
    "    \n",
    "    rot_data = seq_df[Config.ROT_COLS].copy()\n",
    "    rot_data = rot_data.ffill().bfill()\n",
    "    \n",
    "    # Handle quaternion missing values\n",
    "    rot_data_clean = handle_quaternion_missing_values(rot_data.values)\n",
    "    \n",
    "    # Compute world acceleration\n",
    "    world_acc_data = compute_world_acceleration(acc_data.values, rot_data_clean)\n",
    "    \n",
    "    # Initialize features\n",
    "    features = {}\n",
    "    \n",
    "    # Sequence metadata\n",
    "    features['sequence_length'] = len(seq_df)\n",
    "    \n",
    "    # Demographics features\n",
    "    if len(demo_df) > 0:\n",
    "        demo_row = demo_df.iloc[0]\n",
    "        features['age'] = demo_row.get('age', 0)\n",
    "        features['adult_child'] = demo_row.get('adult_child', 0)\n",
    "        features['sex'] = demo_row.get('sex', 0)\n",
    "        features['handedness'] = demo_row.get('handedness', 0)\n",
    "        features['height_cm'] = demo_row.get('height_cm', 0)\n",
    "        features['shoulder_to_wrist_cm'] = demo_row.get('shoulder_to_wrist_cm', 0)\n",
    "        features['elbow_to_wrist_cm'] = demo_row.get('elbow_to_wrist_cm', 0)\n",
    "    \n",
    "    # Extract statistical features for each axis\n",
    "    for i, axis in enumerate(['x', 'y', 'z']):\n",
    "        # Device acceleration\n",
    "        features.update(extract_statistical_features(acc_data.values[:, i], f'acc_{axis}'))\n",
    "        # World acceleration\n",
    "        features.update(extract_statistical_features(world_acc_data[:, i], f'world_acc_{axis}'))\n",
    "    \n",
    "    # Rotation features\n",
    "    for i, comp in enumerate(['w', 'x', 'y', 'z']):\n",
    "        features.update(extract_statistical_features(rot_data_clean[:, i], f'rot_{comp}'))\n",
    "    \n",
    "    # Magnitude features\n",
    "    acc_magnitude = np.linalg.norm(acc_data.values, axis=1)\n",
    "    world_acc_magnitude = np.linalg.norm(world_acc_data, axis=1)\n",
    "    \n",
    "    features.update(extract_statistical_features(acc_magnitude, 'acc_magnitude'))\n",
    "    features.update(extract_statistical_features(world_acc_magnitude, 'world_acc_magnitude'))\n",
    "    \n",
    "    # Difference between device and world acceleration\n",
    "    acc_world_diff = acc_magnitude - world_acc_magnitude\n",
    "    features.update(extract_statistical_features(acc_world_diff, 'acc_world_diff'))\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    result_df = pd.DataFrame([features])\n",
    "    result_df = result_df.fillna(0)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "print('✓ Feature extraction function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print('Loading training data...')\n",
    "train_df = pl.read_csv(Config.TRAIN_PATH)\n",
    "train_demographics = pl.read_csv(Config.TRAIN_DEMOGRAPHICS_PATH)\n",
    "\n",
    "print(f'✓ Train shape: {train_df.shape}')\n",
    "print(f'✓ Demographics shape: {train_demographics.shape}')\n",
    "\n",
    "# Get IMU columns (common between train and test)\n",
    "imu_cols = ['sequence_id', 'subject', 'phase', 'gesture'] + Config.ACC_COLS + Config.ROT_COLS\n",
    "print(f'✓ Using {len(imu_cols)} IMU columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare training data\nprint('Extracting features for training sequences...')\n\ntrain_features_list = []\ntrain_labels = []\ntrain_subjects = []\n\n# Get unique sequences count\nunique_sequences = train_df['sequence_id'].unique()\nn_sequences = len(unique_sequences)\nprint(f'Total sequences to process: {n_sequences}')\n\n# Group by sequence_id\ntrain_sequences = train_df.select(pl.col(imu_cols)).group_by('sequence_id', maintain_order=True)\n\nfor i, (sequence_id, sequence_data) in enumerate(train_sequences):\n    if i % 1000 == 0:\n        print(f'Processing sequence {i+1}/{n_sequences}')\n    \n    # Get sequence ID\n    seq_id_val = sequence_id[0] if isinstance(sequence_id, tuple) else sequence_id\n    \n    # Get demographics\n    subject_id = sequence_data['subject'][0]\n    subject_demographics = train_demographics.filter(pl.col('subject') == subject_id)\n    \n    # Extract features\n    features = extract_features(sequence_data, subject_demographics)\n    train_features_list.append(features)\n    \n    # Get label\n    gesture = sequence_data['gesture'][0]\n    label = GESTURE_MAPPER[gesture]\n    train_labels.append(label)\n    train_subjects.append(subject_id)\n\n# Combine features\nX_train = pd.concat(train_features_list, ignore_index=True)\ny_train = np.array(train_labels)\nsubjects = np.array(train_subjects)\n\nprint(f'✓ Features extracted: {X_train.shape}')\nprint(f'✓ Number of classes: {len(np.unique(y_train))}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with cross-validation\n",
    "print('Training LightGBM models with cross-validation...')\n",
    "\n",
    "cv = StratifiedGroupKFold(n_splits=Config.N_FOLDS, shuffle=True, random_state=Config.SEED)\n",
    "models = []\n",
    "oof_predictions = np.zeros(len(y_train))\n",
    "cv_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train, subjects)):\n",
    "    print(f'\\n--- Fold {fold + 1}/{Config.N_FOLDS} ---')\n",
    "    \n",
    "    # Split data\n",
    "    X_fold_train = X_train.iloc[train_idx]\n",
    "    X_fold_val = X_train.iloc[val_idx]\n",
    "    y_fold_train = y_train[train_idx]\n",
    "    y_fold_val = y_train[val_idx]\n",
    "    \n",
    "    print(f'Train size: {len(X_fold_train)}, Val size: {len(X_fold_val)}')\n",
    "    \n",
    "    # Train model\n",
    "    model = LGBMClassifier(**Config.LGBM_PARAMS)\n",
    "    \n",
    "    model.fit(\n",
    "        X_fold_train, y_fold_train,\n",
    "        eval_set=[(X_fold_val, y_fold_val)],\n",
    "        eval_names=['valid'],\n",
    "        eval_metric='multi_logloss',\n",
    "        callbacks=[log_evaluation(period=50), early_stopping(stopping_rounds=100, verbose=True)]\n",
    "    )\n",
    "    \n",
    "    # Store model\n",
    "    models.append(model)\n",
    "    \n",
    "    # Predictions\n",
    "    val_preds = model.predict(X_fold_val)\n",
    "    oof_predictions[val_idx] = val_preds\n",
    "    \n",
    "    # Calculate score\n",
    "    binary_f1 = f1_score(\n",
    "        np.where(y_fold_val <= 7, 1, 0),\n",
    "        np.where(val_preds <= 7, 1, 0),\n",
    "        zero_division=0.0\n",
    "    )\n",
    "    \n",
    "    macro_f1 = f1_score(\n",
    "        np.where(y_fold_val <= 7, y_fold_val, 99),\n",
    "        np.where(val_preds <= 7, val_preds, 99),\n",
    "        average='macro',\n",
    "        zero_division=0.0\n",
    "    )\n",
    "    \n",
    "    score = 0.5 * (binary_f1 + macro_f1)\n",
    "    cv_scores.append(score)\n",
    "    \n",
    "    print(f'Fold {fold + 1} Score: {score:.4f} (Binary F1: {binary_f1:.4f}, Macro F1: {macro_f1:.4f})')\n",
    "\n",
    "print(f'\\n✓ Cross-validation complete!')\n",
    "print(f'Overall CV Score: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and metadata\n",
    "print('Saving models...')\n",
    "\n",
    "# Prepare model data\n",
    "model_data = {\n",
    "    'models': models,\n",
    "    'feature_names': list(X_train.columns),\n",
    "    'gesture_mapper': GESTURE_MAPPER,\n",
    "    'reverse_gesture_mapper': REVERSE_GESTURE_MAPPER,\n",
    "    'cv_scores': cv_scores,\n",
    "    'mean_cv_score': np.mean(cv_scores),\n",
    "    'config': {\n",
    "        'n_folds': Config.N_FOLDS,\n",
    "        'seed': Config.SEED,\n",
    "        'lgbm_params': Config.LGBM_PARAMS\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "model_path = os.path.join(Config.OUTPUT_PATH, 'imu_lgbm_model.pkl')\n",
    "joblib.dump(model_data, model_path)\n",
    "\n",
    "print(f'✓ Models saved to {model_path}')\n",
    "print(f'✓ File size: {os.path.getsize(model_path) / 1024 / 1024:.2f} MB')\n",
    "\n",
    "# Also save feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': np.mean([model.feature_importances_ for model in models], axis=0)\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print('\\nTop 20 Most Important Features:')\n",
    "print(feature_importance.head(20))\n",
    "\n",
    "feature_importance.to_csv(os.path.join(Config.OUTPUT_PATH, 'feature_importance.csv'), index=False)\n",
    "print('\\n✓ Training complete!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}