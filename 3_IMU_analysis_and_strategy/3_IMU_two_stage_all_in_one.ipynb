{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ====================================================================================================\n# CMI BFRB Detection - Two-Stage Classification System (All-in-One)\n# Score Target: 0.730+ (Binary F1: 0.94+, Macro F1: 0.52+)\n# ====================================================================================================\n\nimport os\nimport sys\nimport json\nimport pickle\nimport joblib\nimport warnings\nimport gc\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Tuple, Dict, List, Optional, Any\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom scipy import stats, signal\nfrom scipy.spatial.transform import Rotation as R\nfrom scipy.fft import fft, fftfreq\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import f1_score, confusion_matrix, classification_report\nimport lightgbm as lgb\nimport xgboost as xgb\n\n# Try to import SMOTE (optional for Kaggle)\ntry:\n    from imblearn.over_sampling import SMOTE\n    from imblearn.combine import SMOTETomek\n    SMOTE_AVAILABLE = True\nexcept ImportError:\n    SMOTE_AVAILABLE = False\n    print(\"SMOTE not available, will use class weights instead\")\n\nwarnings.filterwarnings('ignore')\n\nprint('✓ All imports loaded successfully')\n\n# ====================================================================================================\n# CONFIGURATION\n# ====================================================================================================\n\nCONFIG = {\n    'data_path': 'cmi-detect-behavior-with-sensor-data/',\n    'n_folds': 5,\n    'random_state': 42,\n    'sample_rate': 20,  # Hz\n    'use_smote': SMOTE_AVAILABLE,\n    'two_stage': True,  # Enable two-stage classification\n    \n    # Stage 1: Binary classification (BFRB vs Non-BFRB)\n    'binary_lgbm': {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'boosting_type': 'gbdt',\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'feature_fraction': 0.8,\n        'bagging_fraction': 0.8,\n        'bagging_freq': 5,\n        'n_estimators': 800,\n        'max_depth': 8,\n        'min_child_samples': 20,\n        'reg_alpha': 0.1,\n        'reg_lambda': 0.1,\n        'random_state': 42,\n        'n_jobs': -1,\n        'verbosity': -1\n    },\n    \n    # Stage 2A: BFRB multi-class (8 classes)\n    'bfrb_lgbm': {\n        'objective': 'multiclass',\n        'num_class': 8,\n        'metric': 'multi_logloss',\n        'boosting_type': 'gbdt',\n        'num_leaves': 25,\n        'learning_rate': 0.03,\n        'feature_fraction': 0.7,\n        'bagging_fraction': 0.7,\n        'bagging_freq': 5,\n        'n_estimators': 1000,\n        'max_depth': 6,\n        'min_child_samples': 30,\n        'reg_alpha': 0.2,\n        'reg_lambda': 0.2,\n        'class_weight': 'balanced',\n        'random_state': 42,\n        'n_jobs': -1,\n        'verbosity': -1\n    },\n    \n    # Stage 2B: Non-BFRB multi-class (10 classes)\n    'non_bfrb_lgbm': {\n        'objective': 'multiclass',\n        'num_class': 10,\n        'metric': 'multi_logloss',\n        'boosting_type': 'gbdt',\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'feature_fraction': 0.8,\n        'bagging_fraction': 0.8,\n        'bagging_freq': 5,\n        'n_estimators': 600,\n        'max_depth': 7,\n        'min_child_samples': 20,\n        'random_state': 42,\n        'n_jobs': -1,\n        'verbosity': -1\n    }\n}\n\n# Gesture mapping\nGESTURE_MAPPER = {\n    'Above ear - pull hair': 0, 'Cheek - pinch skin': 1, 'Eyebrow - pull hair': 2,\n    'Eyelash - pull hair': 3, 'Forehead - pull hairline': 4, 'Forehead - scratch': 5,\n    'Neck - pinch skin': 6, 'Neck - scratch': 7,\n    'Drink from bottle/cup': 8, 'Feel around in tray and pull out an object': 9,\n    'Glasses on/off': 10, 'Pinch knee/leg skin': 11, 'Pull air toward your face': 12,\n    'Scratch knee/leg skin': 13, 'Text on phone': 14, 'Wave hello': 15,\n    'Write name in air': 16, 'Write name on leg': 17\n}\nREVERSE_GESTURE_MAPPER = {v: k for k, v in GESTURE_MAPPER.items()}\n\nprint(f'✓ Configuration loaded ({len(GESTURE_MAPPER)} gesture classes)')\n\n# ====================================================================================================\n# WORLD ACCELERATION TRANSFORMATION\n# ====================================================================================================\n\ndef compute_world_acceleration(acc: np.ndarray, rot: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Convert device coordinates to world coordinates using quaternions.\"\"\"\n    # Convert quaternion format (w,x,y,z) to scipy format (x,y,z,w)\n    rot_scipy = rot[:, [1, 2, 3, 0]]\n    \n    # Create rotation object and apply to acceleration\n    r = R.from_quat(rot_scipy)\n    world_acc = r.apply(acc)\n    \n    # Estimate gravity (low-pass filter)\n    b, a = signal.butter(3, 0.3, 'low', fs=CONFIG['sample_rate'])\n    gravity = np.zeros_like(world_acc)\n    for i in range(3):\n        gravity[:, i] = signal.filtfilt(b, a, world_acc[:, i])\n    \n    # Linear acceleration = total - gravity\n    linear_acc = world_acc - gravity\n    \n    return world_acc, linear_acc\n\n# ====================================================================================================\n# FEATURE EXTRACTION\n# ====================================================================================================\n\ndef extract_statistical_features(data: np.ndarray, prefix: str) -> Dict[str, float]:\n    \"\"\"Extract comprehensive statistical features from time series data.\"\"\"\n    features = {}\n    \n    # Basic statistics\n    features[f'{prefix}_mean'] = np.mean(data)\n    features[f'{prefix}_std'] = np.std(data)\n    features[f'{prefix}_var'] = np.var(data)\n    features[f'{prefix}_min'] = np.min(data)\n    features[f'{prefix}_max'] = np.max(data)\n    features[f'{prefix}_range'] = features[f'{prefix}_max'] - features[f'{prefix}_min']\n    \n    # Percentiles\n    for p in [10, 25, 50, 75, 90]:\n        features[f'{prefix}_p{p}'] = np.percentile(data, p)\n    features[f'{prefix}_iqr'] = features[f'{prefix}_p75'] - features[f'{prefix}_p25']\n    \n    # Higher moments\n    features[f'{prefix}_skew'] = stats.skew(data)\n    features[f'{prefix}_kurtosis'] = stats.kurtosis(data)\n    \n    # Peak features\n    peaks, _ = signal.find_peaks(data)\n    features[f'{prefix}_n_peaks'] = len(peaks)\n    features[f'{prefix}_peak_density'] = len(peaks) / len(data) if len(data) > 0 else 0\n    \n    # Temporal features\n    features[f'{prefix}_first'] = data[0] if len(data) > 0 else 0\n    features[f'{prefix}_last'] = data[-1] if len(data) > 0 else 0\n    features[f'{prefix}_delta'] = features[f'{prefix}_last'] - features[f'{prefix}_first']\n    \n    # Zero crossing rate\n    zero_crossings = np.where(np.diff(np.sign(data - np.mean(data))))[0]\n    features[f'{prefix}_zero_crossing_rate'] = len(zero_crossings) / len(data) if len(data) > 0 else 0\n    \n    # Energy\n    features[f'{prefix}_energy'] = np.sum(data ** 2) / len(data) if len(data) > 0 else 0\n    \n    # Segment features (divide into 3 parts)\n    if len(data) >= 9:\n        seg_size = len(data) // 3\n        for i in range(3):\n            start = i * seg_size\n            end = (i + 1) * seg_size if i < 2 else len(data)\n            seg = data[start:end]\n            features[f'{prefix}_seg{i+1}_mean'] = np.mean(seg)\n            features[f'{prefix}_seg{i+1}_std'] = np.std(seg)\n            features[f'{prefix}_seg{i+1}_max'] = np.max(seg)\n    \n    return features\n\ndef extract_frequency_features(data: np.ndarray, prefix: str, sample_rate: int = 20) -> Dict[str, float]:\n    \"\"\"Extract frequency domain features using FFT and spectral analysis.\"\"\"\n    features = {}\n    \n    # FFT\n    fft_vals = np.abs(fft(data))\n    fft_freq = fftfreq(len(data), 1/sample_rate)\n    \n    # Only positive frequencies\n    pos_mask = fft_freq > 0\n    fft_vals = fft_vals[pos_mask]\n    fft_freq = fft_freq[pos_mask]\n    \n    if len(fft_vals) > 0:\n        # Dominant frequency\n        features[f'{prefix}_dominant_freq'] = fft_freq[np.argmax(fft_vals)]\n        features[f'{prefix}_dominant_freq_magnitude'] = np.max(fft_vals)\n        \n        # Spectral features\n        features[f'{prefix}_spectral_energy'] = np.sum(fft_vals ** 2)\n        features[f'{prefix}_spectral_entropy'] = stats.entropy(fft_vals / np.sum(fft_vals) if np.sum(fft_vals) > 0 else fft_vals)\n        \n        # Frequency bands (0-2Hz, 2-5Hz, 5-10Hz)\n        bands = [(0, 2), (2, 5), (5, 10)]\n        for i, (low, high) in enumerate(bands):\n            band_mask = (fft_freq >= low) & (fft_freq < high)\n            if np.any(band_mask):\n                features[f'{prefix}_band{i+1}_energy'] = np.sum(fft_vals[band_mask] ** 2)\n                features[f'{prefix}_band{i+1}_ratio'] = features[f'{prefix}_band{i+1}_energy'] / features[f'{prefix}_spectral_energy'] if features[f'{prefix}_spectral_energy'] > 0 else 0\n    \n    # Power spectral density\n    try:\n        freqs, psd = signal.welch(data, fs=sample_rate, nperseg=min(256, len(data)))\n        features[f'{prefix}_psd_max'] = np.max(psd)\n        features[f'{prefix}_psd_mean'] = np.mean(psd)\n        features[f'{prefix}_psd_std'] = np.std(psd)\n    except:\n        features[f'{prefix}_psd_max'] = 0\n        features[f'{prefix}_psd_mean'] = 0\n        features[f'{prefix}_psd_std'] = 0\n    \n    return features\n\ndef extract_cross_correlation_features(data1: np.ndarray, data2: np.ndarray, prefix: str) -> Dict[str, float]:\n    \"\"\"Extract correlation features between two signals.\"\"\"\n    features = {}\n    \n    # Pearson correlation\n    features[f'{prefix}_corr'] = np.corrcoef(data1, data2)[0, 1] if len(data1) > 1 else 0\n    \n    # Cross-correlation\n    cross_corr = np.correlate(data1 - np.mean(data1), data2 - np.mean(data2), mode='same')\n    features[f'{prefix}_cross_corr_max'] = np.max(cross_corr) if len(cross_corr) > 0 else 0\n    features[f'{prefix}_cross_corr_argmax'] = np.argmax(cross_corr) if len(cross_corr) > 0 else 0\n    \n    return features\n\ndef extract_features_from_sequence(seq_df: pd.DataFrame, demo_df: pd.DataFrame = None) -> pd.DataFrame:\n    \"\"\"Extract all features from a sequence.\"\"\"\n    features = {}\n    \n    # Sequence metadata\n    features['sequence_length'] = len(seq_df)\n    features['duration_seconds'] = len(seq_df) / CONFIG['sample_rate']\n    \n    # Demographics\n    if demo_df is not None and len(demo_df) > 0:\n        demo = demo_df.iloc[0]\n        for col in ['age', 'adult_child', 'sex', 'handedness', 'height_cm', \n                   'shoulder_to_wrist_cm', 'elbow_to_wrist_cm']:\n            if col in demo.index:\n                features[col] = demo[col]\n    \n    # Check if IMU columns exist\n    acc_cols = ['acc_x', 'acc_y', 'acc_z']\n    rot_cols = ['rot_w', 'rot_x', 'rot_y', 'rot_z']\n    \n    if all(col in seq_df.columns for col in acc_cols + rot_cols):\n        # Get IMU data\n        acc = seq_df[acc_cols].fillna(0).values\n        \n        # Handle rotation data more carefully\n        rot_df = seq_df[rot_cols].copy()\n        rot_df = rot_df.ffill().bfill()\n        \n        # Fill remaining NaNs with default quaternion [1, 0, 0, 0]\n        default_quat = {'rot_w': 1, 'rot_x': 0, 'rot_y': 0, 'rot_z': 0}\n        for col, default_val in default_quat.items():\n            rot_df[col] = rot_df[col].fillna(default_val)\n        \n        rot = rot_df.values\n        \n        # World acceleration transformation\n        try:\n            world_acc, linear_acc = compute_world_acceleration(acc, rot)\n            \n            # Extract features for world acceleration\n            for i, axis in enumerate(['x', 'y', 'z']):\n                features.update(extract_statistical_features(world_acc[:, i], f'world_acc_{axis}'))\n                features.update(extract_frequency_features(world_acc[:, i], f'world_acc_{axis}'))\n            \n            # Extract features for linear acceleration\n            for i, axis in enumerate(['x', 'y', 'z']):\n                features.update(extract_statistical_features(linear_acc[:, i], f'linear_acc_{axis}'))\n                features.update(extract_frequency_features(linear_acc[:, i], f'linear_freq_{axis}'))\n            \n            # Magnitude features\n            world_mag = np.linalg.norm(world_acc, axis=1)\n            linear_mag = np.linalg.norm(linear_acc, axis=1)\n            features.update(extract_statistical_features(world_mag, 'world_mag'))\n            features.update(extract_statistical_features(linear_mag, 'linear_mag'))\n            features.update(extract_frequency_features(world_mag, 'world_mag_freq'))\n            features.update(extract_frequency_features(linear_mag, 'linear_mag_freq'))\n            \n        except Exception as e:\n            print(f\"World acceleration error: {e}\")\n            # Fallback to device coordinates\n            for i, axis in enumerate(['x', 'y', 'z']):\n                features.update(extract_statistical_features(acc[:, i], f'acc_{axis}'))\n                features.update(extract_frequency_features(acc[:, i], f'acc_{axis}'))\n        \n        # Original acceleration features (device coordinates)\n        for col in acc_cols:\n            data = seq_df[col].fillna(0).values\n            features.update(extract_statistical_features(data, f'device_{col}'))\n        \n        # Rotation features\n        for i, col in enumerate(rot_cols):\n            data = rot[:, i]\n            features.update(extract_statistical_features(data, col))\n        \n        # Cross-correlation features\n        for i, j in [(0, 1), (0, 2), (1, 2)]:\n            features.update(extract_cross_correlation_features(\n                acc[:, i], acc[:, j], f'acc_corr_{i}{j}'\n            ))\n    \n    return pd.DataFrame([features])\n\nprint('✓ Feature extraction functions defined')\n\n# ====================================================================================================\n# TWO-STAGE CLASSIFIER\n# ====================================================================================================\n\nclass TwoStageClassifier:\n    \"\"\"Two-stage classification: Binary (BFRB detection) -> Multi-class (specific gesture).\"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n        self.binary_models = []  # Stage 1: BFRB vs Non-BFRB\n        self.bfrb_models = []    # Stage 2A: BFRB 8-class\n        self.non_bfrb_models = [] # Stage 2B: Non-BFRB 10-class\n        self.feature_columns = None\n        self.scaler = StandardScaler()\n        \n    def fit(self, X: np.ndarray, y: np.ndarray, groups: np.ndarray = None):\n        \"\"\"Train two-stage classifier with cross-validation.\"\"\"\n        \n        # Store feature columns\n        if isinstance(X, pd.DataFrame):\n            self.feature_columns = X.columns.tolist()\n            X = X.values\n        \n        # Scale features\n        X = self.scaler.fit_transform(X)\n        \n        # Stage 1: Binary labels (BFRB: 0-7, Non-BFRB: 8-17)\n        y_binary = (y < 8).astype(int)\n        \n        # Cross-validation\n        skf = StratifiedGroupKFold(n_splits=self.config['n_folds'], shuffle=True, \n                                   random_state=self.config['random_state'])\n        \n        cv_scores = {'binary_f1': [], 'macro_f1': [], 'combined': []}\n        \n        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y, groups)):\n            print(f\"\\nFold {fold + 1}/{self.config['n_folds']}\")\n            \n            X_train, X_val = X[train_idx], X[val_idx]\n            y_train, y_val = y[train_idx], y[val_idx]\n            y_binary_train, y_binary_val = y_binary[train_idx], y_binary[val_idx]\n            \n            # ========== Stage 1: Binary Classification ==========\n            print(\"  Training Stage 1: Binary classifier...\")\n            binary_model = lgb.LGBMClassifier(**self.config['binary_lgbm'])\n            binary_model.fit(X_train, y_binary_train, \n                           eval_set=[(X_val, y_binary_val)],\n                           eval_metric='binary_logloss',\n                           callbacks=[lgb.log_evaluation(period=100), \n                                    lgb.early_stopping(stopping_rounds=50)])\n            self.binary_models.append(binary_model)\n            \n            # Binary predictions\n            binary_pred = binary_model.predict(X_val)\n            binary_proba = binary_model.predict_proba(X_val)\n            \n            # ========== Stage 2A: BFRB Multi-class ==========\n            print(\"  Training Stage 2A: BFRB classifier...\")\n            bfrb_mask_train = y_train < 8\n            bfrb_mask_val = y_val < 8\n            \n            if np.sum(bfrb_mask_train) > 0:\n                X_bfrb_train = X_train[bfrb_mask_train]\n                y_bfrb_train = y_train[bfrb_mask_train]\n                \n                # Apply SMOTE for class balancing\n                if self.config['use_smote'] and len(np.unique(y_bfrb_train)) > 1:\n                    try:\n                        smote = SMOTE(random_state=self.config['random_state'], k_neighbors=3)\n                        X_bfrb_train, y_bfrb_train = smote.fit_resample(X_bfrb_train, y_bfrb_train)\n                        print(f\"    SMOTE applied: {len(X_bfrb_train)} samples\")\n                    except:\n                        print(\"    SMOTE failed, using original data\")\n                \n                bfrb_model = lgb.LGBMClassifier(**self.config['bfrb_lgbm'])\n                bfrb_model.fit(X_bfrb_train, y_bfrb_train)\n                self.bfrb_models.append(bfrb_model)\n            \n            # ========== Stage 2B: Non-BFRB Multi-class ==========\n            print(\"  Training Stage 2B: Non-BFRB classifier...\")\n            non_bfrb_mask_train = y_train >= 8\n            non_bfrb_mask_val = y_val >= 8\n            \n            if np.sum(non_bfrb_mask_train) > 0:\n                X_non_bfrb_train = X_train[non_bfrb_mask_train]\n                y_non_bfrb_train = y_train[non_bfrb_mask_train] - 8  # Shift labels to 0-9\n                \n                non_bfrb_model = lgb.LGBMClassifier(**self.config['non_bfrb_lgbm'])\n                non_bfrb_model.fit(X_non_bfrb_train, y_non_bfrb_train)\n                self.non_bfrb_models.append(non_bfrb_model)\n            \n            # ========== Combined Predictions ==========\n            y_pred_combined = self._predict_combined(X_val, fold)\n            \n            # Calculate metrics\n            binary_f1 = f1_score(y_binary_val, binary_pred)\n            \n            # Macro F1 for BFRB classes only\n            bfrb_true = y_val[y_val < 8]\n            bfrb_pred = y_pred_combined[y_val < 8]\n            if len(bfrb_true) > 0:\n                macro_f1 = f1_score(bfrb_true, bfrb_pred, average='macro')\n            else:\n                macro_f1 = 0\n            \n            combined_score = (binary_f1 + macro_f1) / 2\n            \n            cv_scores['binary_f1'].append(binary_f1)\n            cv_scores['macro_f1'].append(macro_f1)\n            cv_scores['combined'].append(combined_score)\n            \n            print(f\"  Binary F1: {binary_f1:.4f}, Macro F1: {macro_f1:.4f}, Combined: {combined_score:.4f}\")\n        \n        # Print CV results\n        print(\"\\n\" + \"=\"*50)\n        print(\"Cross-validation Results:\")\n        print(f\"Binary F1: {np.mean(cv_scores['binary_f1']):.4f} ± {np.std(cv_scores['binary_f1']):.4f}\")\n        print(f\"Macro F1:  {np.mean(cv_scores['macro_f1']):.4f} ± {np.std(cv_scores['macro_f1']):.4f}\")\n        print(f\"Combined:  {np.mean(cv_scores['combined']):.4f} ± {np.std(cv_scores['combined']):.4f}\")\n        \n        return cv_scores\n    \n    def _predict_combined(self, X: np.ndarray, fold: int) -> np.ndarray:\n        \"\"\"Combine predictions from two-stage models.\"\"\"\n        n_samples = len(X)\n        y_pred = np.zeros(n_samples, dtype=int)\n        \n        # Stage 1: Binary prediction\n        binary_proba = self.binary_models[fold].predict_proba(X)\n        is_bfrb = binary_proba[:, 1] > 0.5\n        \n        # Stage 2: Conditional prediction\n        for i in range(n_samples):\n            if is_bfrb[i]:  # BFRB\n                if fold < len(self.bfrb_models):\n                    y_pred[i] = self.bfrb_models[fold].predict(X[i:i+1])[0]\n                else:\n                    y_pred[i] = 0  # Default BFRB class\n            else:  # Non-BFRB\n                if fold < len(self.non_bfrb_models):\n                    y_pred[i] = self.non_bfrb_models[fold].predict(X[i:i+1])[0] + 8\n                else:\n                    y_pred[i] = 14  # Default Non-BFRB class (Text on phone)\n        \n        return y_pred\n    \n    def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Predict using ensemble of models.\"\"\"\n        if isinstance(X, pd.DataFrame):\n            X = X[self.feature_columns].values\n        \n        X = self.scaler.transform(X)\n        n_samples = len(X)\n        \n        # Ensemble predictions\n        all_predictions = []\n        for fold in range(len(self.binary_models)):\n            pred = self._predict_combined(X, fold)\n            all_predictions.append(pred)\n        \n        # Majority voting\n        all_predictions = np.array(all_predictions)\n        final_predictions = np.zeros(n_samples, dtype=int)\n        \n        for i in range(n_samples):\n            final_predictions[i] = np.bincount(all_predictions[:, i]).argmax()\n        \n        return final_predictions\n    \n    def save(self, path: str):\n        \"\"\"Save model to file.\"\"\"\n        model_data = {\n            'binary_models': self.binary_models,\n            'bfrb_models': self.bfrb_models,\n            'non_bfrb_models': self.non_bfrb_models,\n            'feature_columns': self.feature_columns,\n            'scaler': self.scaler,\n            'config': self.config\n        }\n        # Try both pickle and joblib\n        try:\n            with open(path, 'wb') as f:\n                pickle.dump(model_data, f)\n        except:\n            joblib.dump(model_data, path)\n    \n    @classmethod\n    def load(cls, path: str):\n        \"\"\"Load model from file.\"\"\"\n        try:\n            with open(path, 'rb') as f:\n                model_data = pickle.load(f)\n        except:\n            model_data = joblib.load(path)\n        \n        classifier = cls(model_data['config'])\n        classifier.binary_models = model_data['binary_models']\n        classifier.bfrb_models = model_data['bfrb_models']\n        classifier.non_bfrb_models = model_data['non_bfrb_models']\n        classifier.feature_columns = model_data['feature_columns']\n        classifier.scaler = model_data['scaler']\n        \n        return classifier\n\nprint('✓ Two-stage classifier defined')\n\n# ====================================================================================================\n# DATA LOADING AND PREPROCESSING\n# ====================================================================================================\n\ndef load_and_prepare_data():\n    \"\"\"Load training data and prepare for model training.\"\"\"\n    print(\"Loading data...\")\n    \n    # Check if running in Kaggle\n    is_kaggle = os.path.exists('/kaggle/input')\n    \n    # Load data with appropriate paths\n    if is_kaggle:\n        train_df = pl.read_csv(CONFIG['data_path'] + 'train.csv')\n        demo_df = pl.read_csv(CONFIG['data_path'] + 'train_demographics.csv')\n    else:\n        # Local environment - try both possible names\n        train_df = pl.read_csv(CONFIG['data_path'] + 'train.csv')\n        try:\n            demo_df = pl.read_csv(CONFIG['data_path'] + 'train_demographics.csv')\n        except:\n            demo_df = pl.read_csv(CONFIG['data_path'] + 'demographics.csv')\n    \n    # Convert to pandas for easier manipulation\n    train_df = train_df.to_pandas()\n    demo_df = demo_df.to_pandas()\n    \n    # Get unique sequences\n    unique_sequences = train_df['sequence_id'].unique()\n    print(f\"Total sequences: {len(unique_sequences)}\")\n    \n    # Filter for IMU columns\n    imu_cols = ['acc_x', 'acc_y', 'acc_z', 'rot_w', 'rot_x', 'rot_y', 'rot_z']\n    \n    # Get sequences with IMU data\n    imu_sequence_ids = unique_sequences\n    \n    print(f\"Found {len(imu_sequence_ids)} IMU sequences\")\n    \n    # Process sequences\n    features_list = []\n    labels = []\n    groups = []\n    \n    for i, seq_id in enumerate(imu_sequence_ids):\n        if i % 1000 == 0:\n            print(f\"Processing sequence {i+1}/{len(imu_sequence_ids)}\")\n        \n        seq_data = train_df[train_df['sequence_id'] == seq_id]\n        subject_id = seq_data['subject'].iloc[0]\n        gesture = seq_data['gesture'].iloc[0]\n        \n        # Get demographics\n        subject_demo = demo_df[demo_df['subject'] == subject_id]\n        \n        # Extract features\n        features = extract_features_from_sequence(seq_data, subject_demo)\n        features_list.append(features)\n        labels.append(GESTURE_MAPPER[gesture])\n        groups.append(subject_id)\n    \n    # Combine features\n    X = pd.concat(features_list, ignore_index=True)\n    y = np.array(labels)\n    groups = np.array(groups)\n    \n    print(f\"Feature matrix shape: {X.shape}\")\n    print(f\"Class distribution:\")\n    for i in range(18):\n        count = np.sum(y == i)\n        if count > 0:\n            print(f\"  {REVERSE_GESTURE_MAPPER[i]}: {count}\")\n    \n    return X, y, groups\n\n# ====================================================================================================\n# TRAINING PIPELINE\n# ====================================================================================================\n\ndef train_model():\n    \"\"\"Main training pipeline.\"\"\"\n    print(\"=\"*70)\n    print(\"CMI BFRB Detection - Two-Stage Classification Training\")\n    print(\"=\"*70)\n    \n    # Load data\n    X, y, groups = load_and_prepare_data()\n    \n    # Train two-stage classifier\n    print(\"\\nTraining Two-Stage Classifier...\")\n    classifier = TwoStageClassifier(CONFIG)\n    cv_scores = classifier.fit(X, y, groups)\n    \n    # Save model\n    model_path = 'two_stage_model.pkl'\n    classifier.save(model_path)\n    print(f\"\\nModel saved to {model_path}\")\n    \n    # Save results\n    results = {\n        'cv_scores': cv_scores,\n        'mean_binary_f1': float(np.mean(cv_scores['binary_f1'])),\n        'mean_macro_f1': float(np.mean(cv_scores['macro_f1'])),\n        'mean_combined': float(np.mean(cv_scores['combined'])),\n        'timestamp': datetime.now().isoformat()\n    }\n    \n    with open('training_results.json', 'w') as f:\n        json.dump(results, f, indent=2, default=lambda x: float(x) if isinstance(x, np.floating) else x)\n    \n    return classifier\n\n# ====================================================================================================\n# INFERENCE AND SUBMISSION\n# ====================================================================================================\n\ndef predict_for_submission(sequence: pl.DataFrame, demographics: pl.DataFrame, model) -> str:\n    \"\"\"Prediction function for Kaggle submission.\"\"\"\n    try:\n        # Convert to pandas\n        seq_df = sequence.to_pandas() if isinstance(sequence, pl.DataFrame) else sequence\n        demo_df = demographics.to_pandas() if isinstance(demographics, pl.DataFrame) else demographics\n        \n        # Extract features\n        features = extract_features_from_sequence(seq_df, demo_df)\n        \n        # Make prediction\n        pred = model.predict(features)[0]\n        \n        return REVERSE_GESTURE_MAPPER[pred]\n        \n    except Exception as e:\n        print(f\"Prediction error: {e}\")\n        return 'Text on phone'  # Default prediction\n\n# ====================================================================================================\n# MAIN EXECUTION\n# ====================================================================================================\n\nif __name__ == '__main__':\n    # Check if running in Kaggle environment\n    is_kaggle = os.path.exists('/kaggle/input')\n    \n    if is_kaggle:\n        print(\"Running in Kaggle environment\")\n        # Update paths for Kaggle\n        CONFIG['data_path'] = '/kaggle/input/cmi-detect-behavior-with-sensor-data/'\n        \n        # Try multiple possible model paths\n        model_paths = [\n            '/kaggle/input/cmi-two-stage-models/two_stage_model.pkl',\n            '/kaggle/working/two_stage_model.pkl',\n            'two_stage_model.pkl'\n        ]\n        \n        model_loaded = False\n        for model_path in model_paths:\n            if os.path.exists(model_path):\n                print(f\"Loading model from {model_path}...\")\n                model = TwoStageClassifier.load(model_path)\n                model_loaded = True\n                break\n        \n        if not model_loaded:\n            # Train new model if not found\n            print(\"No pre-trained model found. Training new model...\")\n            model = train_model()\n            \n            # Save for inference\n            model_path = '/kaggle/working/two_stage_model.pkl'\n            model.save(model_path)\n            print(f\"Model saved to {model_path}\")\n        \n        # Set up prediction function\n        def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n            return predict_for_submission(sequence, demographics, model)\n        \n        # Test prediction function\n        print(\"\\nTesting prediction function...\")\n        test_seq = pl.DataFrame({\n            'acc_x': np.random.randn(100),\n            'acc_y': np.random.randn(100),\n            'acc_z': np.random.randn(100),\n            'rot_w': np.random.randn(100),\n            'rot_x': np.random.randn(100),\n            'rot_y': np.random.randn(100),\n            'rot_z': np.random.randn(100)\n        })\n        test_demo = pl.DataFrame({\n            'age': [25],\n            'adult_child': [1],\n            'sex': [0],\n            'handedness': [1]\n        })\n        test_result = predict(test_seq, test_demo)\n        print(f\"Test result: {test_result}\")\n        assert isinstance(test_result, str) and test_result in GESTURE_MAPPER, \"Invalid prediction\"\n        print(\"✓ Test passed!\")\n        \n        # Initialize inference server\n        sys.path.append('/kaggle/input/cmi-detect-behavior-with-sensor-data')\n        try:\n            import kaggle_evaluation.cmi_inference_server\n            \n            print(\"\\nInitializing CMI inference server...\")\n            inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n            print(\"✓ Inference server initialized\")\n            \n            # Run inference\n            print(\"\\nStarting inference...\")\n            if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n                print(\"Competition environment - serving predictions...\")\n                inference_server.serve()\n            else:\n                print(\"Local testing mode...\")\n                try:\n                    inference_server.run_local_gateway(\n                        data_paths=(\n                            CONFIG['data_path'] + 'test.csv',\n                            CONFIG['data_path'] + 'test_demographics.csv',\n                        )\n                    )\n                    print(\"\\n✓ Inference complete!\")\n                    print(\"✓ submission.parquet has been generated\")\n                    \n                    # Check if submission file was created\n                    if os.path.exists('submission.parquet'):\n                        submission_df = pd.read_parquet('submission.parquet')\n                        print(f\"\\nSubmission shape: {submission_df.shape}\")\n                        print(f\"Submission columns: {submission_df.columns.tolist()}\")\n                        print(f\"\\nFirst 5 predictions:\")\n                        print(submission_df.head())\n                except Exception as e:\n                    print(f\"Inference error (may be normal in notebook): {e}\")\n        except ImportError as e:\n            print(f\"Could not import CMI inference server: {e}\")\n            print(\"This is normal if running locally without the CMI package.\")\n    else:\n        # Local training and testing\n        print(\"Running in local environment\")\n        \n        # Check if we should train or just run inference\n        if os.path.exists('two_stage_model.pkl'):\n            print(\"Found existing model. Loading...\")\n            model = TwoStageClassifier.load('two_stage_model.pkl')\n            print(\"✓ Model loaded successfully\")\n        else:\n            print(\"Training new model...\")\n            model = train_model()\n            print(\"✓ Training completed!\")\n        \n        # Test inference\n        print(\"\\nTesting inference...\")\n        test_seq_df = pd.DataFrame({\n            'acc_x': np.random.randn(100),\n            'acc_y': np.random.randn(100),\n            'acc_z': np.random.randn(100),\n            'rot_w': np.ones(100),\n            'rot_x': np.zeros(100),\n            'rot_y': np.zeros(100),\n            'rot_z': np.zeros(100)\n        })\n        test_demo_df = pd.DataFrame({\n            'age': [30],\n            'adult_child': [1],\n            'sex': [0],\n            'handedness': [1],\n            'height_cm': [170],\n            'shoulder_to_wrist_cm': [50],\n            'elbow_to_wrist_cm': [30]\n        })\n        \n        test_features = extract_features_from_sequence(test_seq_df, test_demo_df)\n        test_pred = model.predict(test_features)[0]\n        print(f\"Test prediction: {REVERSE_GESTURE_MAPPER[test_pred]}\")\n        \n        print(\"\\n\" + \"=\"*70)\n        print(\"To use this model for Kaggle submission:\")\n        print(\"1. Upload this notebook to Kaggle\")\n        print(\"2. Run it to train the model (or upload pre-trained model as dataset)\")\n        print(\"3. The notebook will automatically run inference and generate submission.parquet\")\n        print(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-cmi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}