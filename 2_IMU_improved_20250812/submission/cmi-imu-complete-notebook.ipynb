{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CMI BFRB Detection - IMU Improved Model - Complete Training and Inference Pipeline\\n",
    "# Single-cell implementation for Kaggle competition\\n",
    "# Version: 2.0.0\\n",
    "# Date: 2025-01-12\\n",
    "\\n",
    "import os\\n",
    "import sys\\n",
    "import json\\n",
    "import pickle\\n",
    "import joblib\\n",
    "import warnings\\n",
    "from datetime import datetime\\n",
    "from pathlib import Path\\n",
    "from typing import Dict, List, Optional, Tuple\\n",
    "import numpy as np\\n",
    "import pandas as pd\\n",
    "import polars as pl\\n",
    "from scipy import stats\\n",
    "from scipy.signal import find_peaks, welch\\n",
    "from sklearn.model_selection import StratifiedGroupKFold\\n",
    "from sklearn.preprocessing import LabelEncoder\\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\\n",
    "import lightgbm as lgb\\n",
    "import xgboost as xgb\\n",
    "\\n",
    "warnings.filterwarnings('ignore')\\n",
    "\\n",
    "# ======================== Configuration ========================\\n",
    "\\n",
    "# Paths\\n",
    "IS_KAGGLE = '/kaggle' in os.getcwd()\\n",
    "if IS_KAGGLE:\\n",
    "    DATA_PATH = '/kaggle/input/cmi-detect-behavior-with-sensor-data/'\\n",
    "    MODEL_PATH = '/kaggle/working/models/'\\n",
    "    OUTPUT_PATH = '/kaggle/working/'\\n",
    "else:\\n",
    "    DATA_PATH = '../cmi-detect-behavior-with-sensor-data/'\\n",
    "    MODEL_PATH = './models/'\\n",
    "    OUTPUT_PATH = './'\\n",
    "\\n",
    "# Create directories\\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\\n",
    "\\n",
    "# General settings\\n",
    "SEED = 42\\n",
    "N_FOLDS = 5\\n",
    "DEBUG = False  # Set to True for quick testing\\n",
    "\\n",
    "# Gesture mapping\\n",
    "GESTURE_MAPPER = {\\n",
    "    'Above ear - pull hair': 0, 'Cheek - pinch skin': 1, 'Eyebrow - pull hair': 2,\\n",
    "    'Eyelash - pull hair': 3, 'Forehead - pull hairline': 4, 'Forehead - scratch': 5,\\n",
    "    'Neck - pinch skin': 6, 'Neck - scratch': 7, 'Drink from bottle/cup': 8,\\n",
    "    'Feel around in tray and pull out an object': 9, 'Glasses on/off': 10,\\n",
    "    'Pinch knee/leg skin': 11, 'Pull air toward your face': 12,\\n",
    "    'Scratch knee/leg skin': 13, 'Text on phone': 14, 'Wave hello': 15,\\n",
    "    'Write name in air': 16, 'Write name on leg': 17,\\n",
    "}\\n",
    "REVERSE_GESTURE_MAPPER = {v: k for k, v in GESTURE_MAPPER.items()}\\n",
    "\\n",
    "# BFRB behaviors (indices 0-7)\\n",
    "BFRB_INDICES = list(range(8))\\n",
    "\\n",
    "# Model parameters\\n",
    "LGBM_PARAMS = {\\n",
    "    'objective': 'multiclass',\\n",
    "    'num_class': 18,\\n",
    "    'n_estimators': 500 if not DEBUG else 50,\\n",
    "    'max_depth': 8,\\n",
    "    'learning_rate': 0.03,\\n",
    "    'num_leaves': 31,\\n",
    "    'colsample_bytree': 0.6,\\n",
    "    'subsample': 0.7,\\n",
    "    'reg_alpha': 0.1,\\n",
    "    'reg_lambda': 0.1,\\n",
    "    'random_state': SEED,\\n",
    "    'n_jobs': -1,\\n",
    "    'verbosity': -1,\\n",
    "    'early_stopping_rounds': 150,\\n",
    "    'min_child_samples': 20,\\n",
    "    'min_split_gain': 0.001\\n",
    "}\\n",
    "\\n",
    "XGB_PARAMS = {\\n",
    "    'objective': 'multi:softprob',\\n",
    "    'num_class': 18,\\n",
    "    'n_estimators': 500 if not DEBUG else 50,\\n",
    "    'max_depth': 8,\\n",
    "    'learning_rate': 0.03,\\n",
    "    'colsample_bytree': 0.6,\\n",
    "    'subsample': 0.7,\\n",
    "    'reg_alpha': 0.1,\\n",
    "    'reg_lambda': 0.1,\\n",
    "    'random_state': SEED,\\n",
    "    'n_jobs': -1,\\n",
    "    'verbosity': 0,\\n",
    "    'early_stopping_rounds': 150,\\n",
    "    'min_child_weight': 5,\\n",
    "    'gamma': 0.1,\\n",
    "    'tree_method': 'hist'\\n",
    "}\\n",
    "\\n",
    "# ======================== Feature Engineering ========================\\n",
    "\\n",
    "def handle_quaternion_missing_values(rot_w, rot_x, rot_y, rot_z):\\n",
    "    \\\"\\\"\\\"Handle missing values in quaternion data\\\"\\\"\\\"\\n",
    "    mask = ~(np.isnan(rot_w) | np.isnan(rot_x) | np.isnan(rot_y) | np.isnan(rot_z))\\n",
    "    if np.sum(mask) == 0:\\n",
    "        return np.ones(len(rot_w)), np.zeros(len(rot_w)), np.zeros(len(rot_w)), np.zeros(len(rot_w))\\n",
    "    \\n",
    "    # Forward fill then backward fill\\n",
    "    df_temp = pd.DataFrame({'w': rot_w, 'x': rot_x, 'y': rot_y, 'z': rot_z})\\n",
    "    df_temp = df_temp.fillna(method='ffill').fillna(method='bfill')\\n",
    "    \\n",
    "    # If still NaN, use identity quaternion\\n",
    "    df_temp = df_temp.fillna({'w': 1.0, 'x': 0.0, 'y': 0.0, 'z': 0.0})\\n",
    "    \\n",
    "    return df_temp['w'].values, df_temp['x'].values, df_temp['y'].values, df_temp['z'].values\\n",
    "\\n",
    "def compute_world_acceleration(acc_x, acc_y, acc_z, rot_w, rot_x, rot_y, rot_z):\\n",
    "    \\\"\\\"\\\"Convert local acceleration to world coordinates using quaternion rotation\\\"\\\"\\\"\\n",
    "    # Handle missing values\\n",
    "    rot_w, rot_x, rot_y, rot_z = handle_quaternion_missing_values(rot_w, rot_x, rot_y, rot_z)\\n",
    "    \\n",
    "    # Normalize quaternions\\n",
    "    norm = np.sqrt(rot_w**2 + rot_x**2 + rot_y**2 + rot_z**2) + 1e-10\\n",
    "    rot_w, rot_x, rot_y, rot_z = rot_w/norm, rot_x/norm, rot_y/norm, rot_z/norm\\n",
    "    \\n",
    "    # Rotation matrix components\\n",
    "    wx, wy, wz = rot_w*rot_x, rot_w*rot_y, rot_w*rot_z\\n",
    "    xx, xy, xz = rot_x*rot_x, rot_x*rot_y, rot_x*rot_z\\n",
    "    yy, yz, zz = rot_y*rot_y, rot_y*rot_z, rot_z*rot_z\\n",
    "    \\n",
    "    # Transform to world coordinates\\n",
    "    world_x = (1 - 2*(yy + zz)) * acc_x + 2*(xy - wz) * acc_y + 2*(xz + wy) * acc_z\\n",
    "    world_y = 2*(xy + wz) * acc_x + (1 - 2*(xx + zz)) * acc_y + 2*(yz - wx) * acc_z\\n",
    "    world_z = 2*(xz - wy) * acc_x + 2*(yz + wx) * acc_y + (1 - 2*(xx + yy)) * acc_z\\n",
    "    \\n",
    "    return world_x, world_y, world_z\\n",
    "\\n",
    "def extract_statistical_features(data: np.ndarray, prefix: str) -> Dict[str, float]:\\n",
    "    \\\"\\\"\\\"Extract statistical features from time series data\\\"\\\"\\\"\\n",
    "    features = {}\\n",
    "    \\n",
    "    # Basic statistics\\n",
    "    features[f'{prefix}_mean'] = np.mean(data)\\n",
    "    features[f'{prefix}_std'] = np.std(data)\\n",
    "    features[f'{prefix}_min'] = np.min(data)\\n",
    "    features[f'{prefix}_max'] = np.max(data)\\n",
    "    features[f'{prefix}_median'] = np.median(data)\\n",
    "    features[f'{prefix}_q25'] = np.percentile(data, 25)\\n",
    "    features[f'{prefix}_q75'] = np.percentile(data, 75)\\n",
    "    features[f'{prefix}_iqr'] = features[f'{prefix}_q75'] - features[f'{prefix}_q25']\\n",
    "    features[f'{prefix}_range'] = features[f'{prefix}_max'] - features[f'{prefix}_min']\\n",
    "    features[f'{prefix}_cv'] = np.std(data) / (np.mean(data) + 1e-10)\\n",
    "    \\n",
    "    # Higher moments\\n",
    "    if len(data) > 1:\\n",
    "        features[f'{prefix}_skew'] = stats.skew(data)\\n",
    "        features[f'{prefix}_kurt'] = stats.kurtosis(data)\\n",
    "    else:\\n",
    "        features[f'{prefix}_skew'] = 0\\n",
    "        features[f'{prefix}_kurt'] = 0\\n",
    "    \\n",
    "    # Time series features\\n",
    "    if len(data) > 1:\\n",
    "        features[f'{prefix}_first'] = data[0]\\n",
    "        features[f'{prefix}_last'] = data[-1]\\n",
    "        features[f'{prefix}_delta'] = data[-1] - data[0]\\n",
    "        \\n",
    "        # Difference statistics\\n",
    "        diff_data = np.diff(data)\\n",
    "        features[f'{prefix}_diff_mean'] = np.mean(diff_data)\\n",
    "        features[f'{prefix}_diff_std'] = np.std(diff_data)\\n",
    "        features[f'{prefix}_diff_max'] = np.max(np.abs(diff_data))\\n",
    "        \\n",
    "        # Trend\\n",
    "        time_indices = np.arange(len(data))\\n",
    "        corr_coef = np.corrcoef(time_indices, data)[0, 1]\\n",
    "        features[f'{prefix}_trend'] = corr_coef if not np.isnan(corr_coef) else 0\\n",
    "    \\n",
    "    return features\\n",
    "\\n",
    "def extract_frequency_features(data: np.ndarray, prefix: str, sampling_rate: float = 20) -> Dict[str, float]:\\n",
    "    \\\"\\\"\\\"Extract frequency domain features\\\"\\\"\\\"\\n",
    "    features = {}\\n",
    "    \\n",
    "    if len(data) < 4:\\n",
    "        # Return default values for short sequences\\n",
    "        for feat in ['fft_max_freq', 'fft_max_power', 'spectral_centroid', \\n",
    "                    'spectral_rolloff', 'spectral_entropy', 'zero_crossing_rate']:\\n",
    "            features[f'{prefix}_{feat}'] = 0\\n",
    "        return features\\n",
    "    \\n",
    "    # FFT features\\n",
    "    fft = np.fft.fft(data)\\n",
    "    freqs = np.fft.fftfreq(len(data), 1/sampling_rate)\\n",
    "    positive_freqs = freqs[:len(freqs)//2]\\n",
    "    power_spectrum = np.abs(fft[:len(fft)//2])**2\\n",
    "    \\n",
    "    if len(power_spectrum) > 0:\\n",
    "        max_power_idx = np.argmax(power_spectrum)\\n",
    "        features[f'{prefix}_fft_max_freq'] = positive_freqs[max_power_idx]\\n",
    "        features[f'{prefix}_fft_max_power'] = power_spectrum[max_power_idx]\\n",
    "        \\n",
    "        # Spectral features\\n",
    "        total_power = np.sum(power_spectrum) + 1e-10\\n",
    "        normalized_spectrum = power_spectrum / total_power\\n",
    "        \\n",
    "        # Spectral centroid\\n",
    "        spectral_centroid = np.sum(positive_freqs * normalized_spectrum)\\n",
    "        features[f'{prefix}_spectral_centroid'] = spectral_centroid\\n",
    "        \\n",
    "        # Spectral rolloff\\n",
    "        cumsum = np.cumsum(normalized_spectrum)\\n",
    "        rolloff_idx = np.where(cumsum >= 0.85)[0]\\n",
    "        if len(rolloff_idx) > 0:\\n",
    "            features[f'{prefix}_spectral_rolloff'] = positive_freqs[rolloff_idx[0]]\\n",
    "        else:\\n",
    "            features[f'{prefix}_spectral_rolloff'] = positive_freqs[-1]\\n",
    "        \\n",
    "        # Spectral entropy\\n",
    "        spectral_entropy = -np.sum(normalized_spectrum * np.log2(normalized_spectrum + 1e-10))\\n",
    "        features[f'{prefix}_spectral_entropy'] = spectral_entropy\\n",
    "    \\n",
    "    # Zero crossing rate\\n",
    "    zero_crossings = np.sum(np.diff(np.sign(data)) != 0)\\n",
    "    features[f'{prefix}_zero_crossing_rate'] = zero_crossings / len(data)\\n",
    "    \\n",
    "    return features\\n",
    "\\n",
    "def extract_segment_features(data: np.ndarray, prefix: str, n_segments: int = 3) -> Dict[str, float]:\\n",
    "    \\\"\\\"\\\"Extract features from segments of the time series\\\"\\\"\\\"\\n",
    "    features = {}\\n",
    "    \\n",
    "    if len(data) < n_segments * 3:\\n",
    "        # Too short for segmentation\\n",
    "        for i in range(n_segments):\\n",
    "            features[f'{prefix}_seg{i+1}_mean'] = np.mean(data)\\n",
    "            features[f'{prefix}_seg{i+1}_std'] = np.std(data)\\n",
    "        return features\\n",
    "    \\n",
    "    segment_size = len(data) // n_segments\\n",
    "    for i in range(n_segments):\\n",
    "        start = i * segment_size\\n",
    "        end = (i + 1) * segment_size if i < n_segments - 1 else len(data)\\n",
    "        segment = data[start:end]\\n",
    "        \\n",
    "        features[f'{prefix}_seg{i+1}_mean'] = np.mean(segment)\\n",
    "        features[f'{prefix}_seg{i+1}_std'] = np.std(segment)\\n",
    "        features[f'{prefix}_seg{i+1}_max'] = np.max(segment)\\n",
    "        features[f'{prefix}_seg{i+1}_min'] = np.min(segment)\\n",
    "    \\n",
    "    return features\\n",
    "\\n",
    "def extract_comprehensive_features(seq_df: pd.DataFrame, demo_df: pd.DataFrame = None) -> pd.DataFrame:\\n",
    "    \\\"\\\"\\\"Extract all features from a sequence\\\"\\\"\\\"\\n",
    "    features = {}\\n",
    "    \\n",
    "    # Sequence length\\n",
    "    features['sequence_length'] = len(seq_df)\\n",
    "    \\n",
    "    # Demographics with defaults\\n",
    "    if demo_df is not None and len(demo_df) > 0:\\n",
    "        demo = demo_df.iloc[0]\\n",
    "        features['age'] = demo.get('age', 30)\\n",
    "        features['adult_child'] = demo.get('adult_child', 1)\\n",
    "        features['sex'] = demo.get('sex', 0)\\n",
    "        features['handedness'] = demo.get('handedness', 1)\\n",
    "        features['height_cm'] = demo.get('height_cm', 170)\\n",
    "        features['shoulder_to_wrist_cm'] = demo.get('shoulder_to_wrist_cm', 50)\\n",
    "        features['elbow_to_wrist_cm'] = demo.get('elbow_to_wrist_cm', 30)\\n",
    "    else:\\n",
    "        features.update({\\n",
    "            'age': 30, 'adult_child': 1, 'sex': 0, 'handedness': 1,\\n",
    "            'height_cm': 170, 'shoulder_to_wrist_cm': 50, 'elbow_to_wrist_cm': 30\\n",
    "        })\\n",
    "    \\n",
    "    # Extract IMU features\\n",
    "    imu_cols = ['acc_x', 'acc_y', 'acc_z', 'rot_w', 'rot_x', 'rot_y', 'rot_z']\\n",
    "    \\n",
    "    for col in imu_cols:\\n",
    "        if col in seq_df.columns:\\n",
    "            data = seq_df[col].fillna(0).values\\n",
    "            \\n",
    "            # Statistical features\\n",
    "            features.update(extract_statistical_features(data, col))\\n",
    "            \\n",
    "            # Frequency features\\n",
    "            features.update(extract_frequency_features(data, col))\\n",
    "            \\n",
    "            # Segment features\\n",
    "            features.update(extract_segment_features(data, col))\\n",
    "    \\n",
    "    # World acceleration features\\n",
    "    if all(col in seq_df.columns for col in ['acc_x', 'acc_y', 'acc_z', 'rot_w', 'rot_x', 'rot_y', 'rot_z']):\\n",
    "        world_x, world_y, world_z = compute_world_acceleration(\\n",
    "            seq_df['acc_x'].fillna(0).values,\\n",
    "            seq_df['acc_y'].fillna(0).values,\\n",
    "            seq_df['acc_z'].fillna(0).values,\\n",
    "            seq_df['rot_w'].fillna(1).values,\\n",
    "            seq_df['rot_x'].fillna(0).values,\\n",
    "            seq_df['rot_y'].fillna(0).values,\\n",
    "            seq_df['rot_z'].fillna(0).values\\n",
    "        )\\n",
    "        \\n",
    "        # Extract features for world coordinates\\n",
    "        for data, prefix in [(world_x, 'world_acc_x'), (world_y, 'world_acc_y'), (world_z, 'world_acc_z')]:\\n",
    "            features.update(extract_statistical_features(data, prefix))\\n",
    "        \\n",
    "        # World acceleration magnitude\\n",
    "        world_mag = np.sqrt(world_x**2 + world_y**2 + world_z**2)\\n",
    "        features.update(extract_statistical_features(world_mag, 'world_acc_mag'))\\n",
    "    \\n",
    "    # Acceleration magnitude\\n",
    "    if all(col in seq_df.columns for col in ['acc_x', 'acc_y', 'acc_z']):\\n",
    "        acc_mag = np.sqrt(\\n",
    "            seq_df['acc_x'].fillna(0)**2 + \\n",
    "            seq_df['acc_y'].fillna(0)**2 + \\n",
    "            seq_df['acc_z'].fillna(0)**2\\n",
    "        )\\n",
    "        features.update(extract_statistical_features(acc_mag, 'acc_magnitude'))\\n",
    "    \\n",
    "    # Rotation energy\\n",
    "    if all(col in seq_df.columns for col in ['rot_x', 'rot_y', 'rot_z']):\\n",
    "        rot_energy = seq_df['rot_x'].fillna(0)**2 + seq_df['rot_y'].fillna(0)**2 + seq_df['rot_z'].fillna(0)**2\\n",
    "        features.update(extract_statistical_features(rot_energy, 'rotation_energy'))\\n",
    "    \\n",
    "    # Jerk (acceleration derivative)\\n",
    "    if all(col in seq_df.columns for col in ['acc_x', 'acc_y', 'acc_z']) and len(seq_df) > 1:\\n",
    "        for col in ['acc_x', 'acc_y', 'acc_z']:\\n",
    "            jerk = np.diff(seq_df[col].fillna(0).values)\\n",
    "            features[f'{col}_jerk_mean'] = np.mean(jerk)\\n",
    "            features[f'{col}_jerk_std'] = np.std(jerk)\\n",
    "            features[f'{col}_jerk_max'] = np.max(np.abs(jerk))\\n",
    "    \\n",
    "    return pd.DataFrame([features])\\n",
    "\\n",
    "# ======================== Model Training ========================\\n",
    "\\n",
    "def competition_metric(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[float, float, float]:\\n",
    "    \\\"\\\"\\\"Calculate competition metric: (Binary F1 + Macro F1) / 2\\\"\\\"\\\"\\n",
    "    # Binary F1: BFRB vs non-BFRB\\n",
    "    binary_f1 = f1_score(\\n",
    "        np.where(y_true <= 7, 1, 0),\\n",
    "        np.where(y_pred <= 7, 1, 0),\\n",
    "        zero_division=0.0,\\n",
    "    )\\n",
    "    \\n",
    "    # Macro F1: within BFRB behaviors\\n",
    "    macro_f1 = f1_score(\\n",
    "        np.where(y_true <= 7, y_true, 99),\\n",
    "        np.where(y_pred <= 7, y_pred, 99),\\n",
    "        average='macro',\\n",
    "        zero_division=0.0,\\n",
    "    )\\n",
    "    \\n",
    "    final_score = 0.5 * (binary_f1 + macro_f1)\\n",
    "    return final_score, binary_f1, macro_f1\\n",
    "\\n",
    "def train_models(X_train: pd.DataFrame, y_train: np.ndarray, subjects: np.ndarray) -> Dict:\\n",
    "    \\\"\\\"\\\"Train LightGBM and XGBoost models with cross-validation\\\"\\\"\\\"\\n",
    "    print('\\\\n========== Model Training ==========')\\n",
    "    \\n",
    "    # Initialize models storage\\n",
    "    lgb_models = []\\n",
    "    xgb_models = []\\n",
    "    oof_predictions_lgb = np.zeros((len(X_train), 18))\\n",
    "    oof_predictions_xgb = np.zeros((len(X_train), 18))\\n",
    "    feature_importance = pd.DataFrame()\\n",
    "    \\n",
    "    # Cross-validation\\n",
    "    sgkf = StratifiedGroupKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\\n",
    "    \\n",
    "    for fold, (train_idx, val_idx) in enumerate(sgkf.split(X_train, y_train, subjects)):\\n",
    "        print(f'\\\\n--- Fold {fold + 1}/{N_FOLDS} ---')\\n",
    "        \\n",
    "        X_fold_train = X_train.iloc[train_idx]\\n",
    "        X_fold_val = X_train.iloc[val_idx]\\n",
    "        y_fold_train = y_train[train_idx]\\n",
    "        y_fold_val = y_train[val_idx]\\n",
    "        \\n",
    "        # Train LightGBM\\n",
    "        print('Training LightGBM...')\\n",
    "        lgb_model = lgb.LGBMClassifier(**LGBM_PARAMS)\\n",
    "        lgb_model.fit(\\n",
    "            X_fold_train, y_fold_train,\\n",
    "            eval_set=[(X_fold_val, y_fold_val)],\\n",
    "            callbacks=[lgb.early_stopping(150), lgb.log_evaluation(0)]\\n",
    "        )\\n",
    "        lgb_models.append(lgb_model)\\n",
    "        \\n",
    "        # Train XGBoost\\n",
    "        print('Training XGBoost...')\\n",
    "        xgb_model = xgb.XGBClassifier(**XGB_PARAMS, enable_categorical=False)\\n",
    "        xgb_model.fit(\\n",
    "            X_fold_train, y_fold_train,\\n",
    "            eval_set=[(X_fold_val, y_fold_val)],\\n",
    "            verbose=False\\n",
    "        )\\n",
    "        xgb_models.append(xgb_model)\\n",
    "        \\n",
    "        # OOF predictions\\n",
    "        oof_predictions_lgb[val_idx] = lgb_model.predict_proba(X_fold_val)\\n",
    "        oof_predictions_xgb[val_idx] = xgb_model.predict_proba(X_fold_val)\\n",
    "        \\n",
    "        # Feature importance\\n",
    "        importance = pd.DataFrame({\\n",
    "            'feature': X_train.columns,\\n",
    "            'importance': lgb_model.feature_importances_,\\n",
    "            'fold': fold\\n",
    "        })\\n",
    "        feature_importance = pd.concat([feature_importance, importance])\\n",
    "        \\n",
    "        # Evaluate fold\\n",
    "        val_pred_lgb = np.argmax(oof_predictions_lgb[val_idx], axis=1)\\n",
    "        val_pred_xgb = np.argmax(oof_predictions_xgb[val_idx], axis=1)\\n",
    "        val_pred_ensemble = np.argmax(0.6 * oof_predictions_lgb[val_idx] + 0.4 * oof_predictions_xgb[val_idx], axis=1)\\n",
    "        \\n",
    "        score, binary_f1, macro_f1 = competition_metric(y_fold_val, val_pred_ensemble)\\n",
    "        print(f'Fold {fold + 1} - Score: {score:.4f}, Binary F1: {binary_f1:.4f}, Macro F1: {macro_f1:.4f}')\\n",
    "    \\n",
    "    # Final ensemble predictions\\n",
    "    oof_predictions_ensemble = 0.6 * oof_predictions_lgb + 0.4 * oof_predictions_xgb\\n",
    "    final_predictions = np.argmax(oof_predictions_ensemble, axis=1)\\n",
    "    \\n",
    "    # Overall metrics\\n",
    "    final_score, binary_f1, macro_f1 = competition_metric(y_train, final_predictions)\\n",
    "    print(f'\\\\n========== Overall Results ==========')\\n",
    "    print(f'Competition Score: {final_score:.4f}')\\n",
    "    print(f'Binary F1: {binary_f1:.4f}')\\n",
    "    print(f'Macro F1: {macro_f1:.4f}')\\n",
    "    print(f'Accuracy: {accuracy_score(y_train, final_predictions):.4f}')\\n",
    "    \\n",
    "    # Top features\\n",
    "    feature_importance_mean = feature_importance.groupby('feature')['importance'].mean().sort_values(ascending=False)\\n",
    "    print(f'\\\\nTop 10 Features:')\\n",
    "    for i, (feat, imp) in enumerate(feature_importance_mean.head(10).items(), 1):\\n",
    "        print(f'{i:2d}. {feat}: {imp:.2f}')\\n",
    "    \\n",
    "    return {\\n",
    "        'lgb_models': lgb_models,\\n",
    "        'xgb_models': xgb_models,\\n",
    "        'feature_columns': list(X_train.columns),\\n",
    "        'feature_importance': feature_importance_mean.to_dict(),\\n",
    "        'oof_predictions': oof_predictions_ensemble,\\n",
    "        'metrics': {\\n",
    "            'competition_score': final_score,\\n",
    "            'binary_f1': binary_f1,\\n",
    "            'macro_f1': macro_f1,\\n",
    "            'accuracy': accuracy_score(y_train, final_predictions)\\n",
    "        }\\n",
    "    }\\n",
    "\\n",
    "# ======================== Inference ========================\\n",
    "\\n",
    "def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\\n",
    "    \\\"\\\"\\\"Prediction function for Kaggle inference server\\\"\\\"\\\"\\n",
    "    try:\\n",
    "        # Convert to pandas\\n",
    "        seq_df = sequence.to_pandas() if isinstance(sequence, pl.DataFrame) else sequence\\n",
    "        demo_df = demographics.to_pandas() if isinstance(demographics, pl.DataFrame) else demographics\\n",
    "        \\n",
    "        # Extract features\\n",
    "        features = extract_comprehensive_features(seq_df, demo_df)\\n",
    "        \\n",
    "        # Ensure all features present\\n",
    "        for col in feature_cols:\\n",
    "            if col not in features.columns:\\n",
    "                features[col] = 0\\n",
    "        \\n",
    "        X_pred = features[feature_cols]\\n",
    "        \\n",
    "        # Get predictions from all models\\n",
    "        lgb_preds = np.zeros((1, 18))\\n",
    "        xgb_preds = np.zeros((1, 18))\\n",
    "        \\n",
    "        for lgb_model in lgb_models:\\n",
    "            lgb_preds += lgb_model.predict_proba(X_pred) / len(lgb_models)\\n",
    "        \\n",
    "        for xgb_model in xgb_models:\\n",
    "            xgb_preds += xgb_model.predict_proba(X_pred) / len(xgb_models)\\n",
    "        \\n",
    "        # Ensemble\\n",
    "        ensemble_pred = 0.6 * lgb_preds + 0.4 * xgb_preds\\n",
    "        \\n",
    "        # Apply post-processing (boost BFRB behaviors)\\n",
    "        if np.max(ensemble_pred[0, :8]) > 0.35:  # If any BFRB behavior has high confidence\\n",
    "            ensemble_pred[0, :8] *= 1.25  # Boost BFRB probabilities\\n",
    "        \\n",
    "        final_pred = np.argmax(ensemble_pred[0])\\n",
    "        \\n",
    "        return REVERSE_GESTURE_MAPPER.get(final_pred, 'Text on phone')\\n",
    "        \\n",
    "    except Exception as e:\\n",
    "        print(f'Prediction error: {e}')\\n",
    "        return 'Text on phone'  # Default fallback\\n",
    "\\n",
    "# ======================== Main Execution ========================\\n",
    "\\n",
    "print('CMI BFRB Detection - IMU Improved Model')\\n",
    "print('=' * 50)\\n",
    "\\n",
    "# Check if we're in training or inference mode\\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\\n",
    "    # ========== Inference Mode ==========\\n",
    "    print('Running in INFERENCE mode')\\n",
    "    \\n",
    "    # Load pre-trained models\\n",
    "    print('Loading models...')\\n",
    "    model_file = os.path.join(MODEL_PATH, 'model_data.pkl')\\n",
    "    \\n",
    "    if os.path.exists(model_file):\\n",
    "        with open(model_file, 'rb') as f:\\n",
    "            model_data = pickle.load(f)\\n",
    "        \\n",
    "        lgb_models = model_data['lgb_models']\\n",
    "        xgb_models = model_data['xgb_models']\\n",
    "        feature_cols = model_data['feature_columns']\\n",
    "        print(f'Loaded {len(lgb_models)} LightGBM and {len(xgb_models)} XGBoost models')\\n",
    "    else:\\n",
    "        print('ERROR: Model file not found!')\\n",
    "        lgb_models = []\\n",
    "        xgb_models = []\\n",
    "        feature_cols = []\\n",
    "    \\n",
    "    # Initialize inference server\\n",
    "    print('Initializing inference server...')\\n",
    "    sys.path.append('/kaggle/input/cmi-detect-behavior-with-sensor-data')\\n",
    "    import kaggle_evaluation.cmi_inference_server\\n",
    "    \\n",
    "    inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\\n",
    "    print('Serving predictions...')\\n",
    "    inference_server.serve()\\n",
    "    \\n",
    "else:\\n",
    "    # ========== Training Mode ==========\\n",
    "    print('Running in TRAINING mode')\\n",
    "    \\n",
    "    # Load training data\\n",
    "    print('\\\\nLoading data...')\\n",
    "    train_df = pl.read_csv(os.path.join(DATA_PATH, 'train.csv'))\\n",
    "    train_demographics = pl.read_csv(os.path.join(DATA_PATH, 'train_demographics.csv'))\\n",
    "    \\n",
    "    # Filter for 'Performs gesture' phase only\\n",
    "    train_filtered = train_df.filter(pl.col('behavior') == 'Performs gesture')\\n",
    "    print(f'Total sequences: {train_filtered[\\\"sequence_id\\\"].n_unique()}')\\n",
    "    \\n",
    "    # Extract features for all sequences\\n",
    "    print('\\\\nExtracting features...')\\n",
    "    features_list = []\\n",
    "    labels = []\\n",
    "    subjects = []\\n",
    "    \\n",
    "    sequences = list(train_filtered.group_by('sequence_id', maintain_order=True))\\n",
    "    \\n",
    "    # Use subset for debugging\\n",
    "    if DEBUG:\\n",
    "        sequences = sequences[:500]\\n",
    "        print(f'DEBUG MODE: Using only {len(sequences)} sequences')\\n",
    "    \\n",
    "    for idx, (seq_id, seq_data) in enumerate(sequences):\\n",
    "        if idx % 500 == 0:\\n",
    "            print(f'Processing: {idx}/{len(sequences)} sequences')\\n",
    "        \\n",
    "        # Get sequence info\\n",
    "        sequence_id = seq_id[0] if isinstance(seq_id, tuple) else seq_id\\n",
    "        subject_id = seq_data['subject'][0]\\n",
    "        gesture = seq_data['gesture'][0]\\n",
    "        \\n",
    "        # Get demographics\\n",
    "        subject_demographics = train_demographics.filter(pl.col('subject') == subject_id)\\n",
    "        \\n",
    "        # Convert to pandas and extract features\\n",
    "        seq_df = seq_data.to_pandas()\\n",
    "        demo_df = subject_demographics.to_pandas() if not subject_demographics.is_empty() else pd.DataFrame()\\n",
    "        \\n",
    "        features = extract_comprehensive_features(seq_df, demo_df)\\n",
    "        features_list.append(features)\\n",
    "        labels.append(GESTURE_MAPPER[gesture])\\n",
    "        subjects.append(subject_id)\\n",
    "    \\n",
    "    # Combine features\\n",
    "    X_train = pd.concat(features_list, ignore_index=True)\\n",
    "    y_train = np.array(labels)\\n",
    "    subjects = np.array(subjects)\\n",
    "    \\n",
    "    print(f'\\\\nFeature matrix shape: {X_train.shape}')\\n",
    "    print(f'Labels shape: {y_train.shape}')\\n",
    "    print(f'Unique subjects: {len(np.unique(subjects))}')\\n",
    "    \\n",
    "    # Train models\\n",
    "    model_results = train_models(X_train, y_train, subjects)\\n",
    "    \\n",
    "    # Save models and metadata\\n",
    "    print('\\\\nSaving models...')\\n",
    "    model_data = {\\n",
    "        'lgb_models': model_results['lgb_models'],\\n",
    "        'xgb_models': model_results['xgb_models'],\\n",
    "        'feature_columns': model_results['feature_columns'],\\n",
    "        'feature_importance': model_results['feature_importance'],\\n",
    "        'metrics': model_results['metrics']\\n",
    "    }\\n",
    "    \\n",
    "    with open(os.path.join(MODEL_PATH, 'model_data.pkl'), 'wb') as f:\\n",
    "        pickle.dump(model_data, f)\\n",
    "    \\n",
    "    # Save training results\\n",
    "    with open(os.path.join(MODEL_PATH, 'training_results.json'), 'w') as f:\\n",
    "        json.dump(model_results['metrics'], f, indent=2)\\n",
    "    \\n",
    "    print(f'\\\\nModels saved to {MODEL_PATH}')\\n",
    "    print('\\\\n========== Training Complete ==========')\\n",
    "    \\n",
    "    # Test inference function\\n",
    "    print('\\\\nTesting inference function...')\\n",
    "    lgb_models = model_results['lgb_models']\\n",
    "    xgb_models = model_results['xgb_models']\\n",
    "    feature_cols = model_results['feature_columns']\\n",
    "    \\n",
    "    test_seq = pl.DataFrame({\\n",
    "        'acc_x': np.random.randn(100),\\n",
    "        'acc_y': np.random.randn(100),\\n",
    "        'acc_z': np.random.randn(100),\\n",
    "        'rot_w': np.random.randn(100),\\n",
    "        'rot_x': np.random.randn(100),\\n",
    "        'rot_y': np.random.randn(100),\\n",
    "        'rot_z': np.random.randn(100)\\n",
    "    })\\n",
    "    test_demo = pl.DataFrame({\\n",
    "        'age': [25],\\n",
    "        'adult_child': [1],\\n",
    "        'sex': [0],\\n",
    "        'handedness': [1]\\n",
    "    })\\n",
    "    \\n",
    "    result = predict(test_seq, test_demo)\\n",
    "    print(f'Test prediction: {result}')\\n",
    "    assert isinstance(result, str) and result in GESTURE_MAPPER, 'Invalid prediction!'\\n",
    "    print('Inference test passed!')\\n",
    "\\n",
    "print('\\\\n========== Process Complete ==========')\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.10.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}